Speculative devices for photo display	In this paper, we describe three purposefully provocative, digital photo display technologies designed for home settings. The three devices have been built to provoke questions around how digital photographs might be seen and interacted with in novel ways. They are also intended for speculation about the expressive resources afforded by digital technologies for displaying photos. It is hoped interactions with the devices will help researchers and designers reflect on new design possibilities. The devices are also being deployed as part of ongoing home-oriented field research.
Collaborating around collections: informing the continued development of photoware	This paper explores the embodied interactional ways in which people naturally collaborate around and share collections of photographs. We employ ethnographic studies of paper-based photograph use to consider requirements for distributed collaboration around digital photographs. Distributed sharing is currently limited to the 'passing on' of photographs to others, by email, webpages, or mobile phones. To move beyond this, a fundamental challenge for photoware consists of developing support for the practical achievement of sharing 'at a distance'. Specifically, this entails augmenting the natural production of accounts or 'photo-talk' to support the distributed achievement of sharing.
Paperproof: a paper-digital proof-editing system	We present PaperProof, a paper-digital proof-editing application that allows users to edit digital documents by means of gesture-based mark-up of their printed versions. This enables users to switch seamlessly back and forth between paper and digital instances of a document throughout the document lifecycle, working with whichever medium is preferred for a given task. Importantly, by maintaining a logical mapping between the printed and digital instances, editing operations on paper can later be integrated into the digital document even if other users have edited the digital version in parallel. The system is based on Anoto digital pen and paper technology and is implemented using the iPaper framework for interactive paper.
Implications for a gesture design tool	Interest in pen-based user interfaces is growing rapidly. Onepotentially useful feature of pen-based user interfaces isgestures, that is, a mark or stroke that causes a command toexecute. Unfortunately, it is difficult to design gestures that areeasy 1) for computers to recognize and 2) for humans to learn andremember. To investigate these problems, we built a prototype tooltypical fo those used for designing gesture sets. An experiment wasthen performed to gain insight into the gesture design process andto evaluate this style of tool. The experiment confirmed thatgesture design is very difficult and suggested several ways inwhich current tools can be improved. The most important improvementis to make the tools more active and provide more guidance fordesigners. This paper describes the gesture design tool, theexperiment, and its results.
End user software engineering: chi'2008 special interest group meeting	End users create software whenever they write, for instance, educational simulations, spreadsheets, or dynamic e-business web applications. Researchers are working to bring the benefits of rigorous software engineering methodologies to these end users to try to make their software more reliable. Unfortunately, errors are pervasive in end-user software, and the resulting impact is sometimes enormous. This special interest group meeting has two purposes: to incorporate attendees' and feedback into an emerging survey of the state of this interesting new sub-area, and generally to bring together the community of researchers who are addressing this topic, with the companies that are creating end-user programming tools.
Debugging reinvented: asking and answering why and why not questions about program behavior	When software developers want to understand the reason for a program's behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of why did and why didn't questions derived from the program's code and execution. The tool then finds one or more possible explanations for the output in question, using a combination of static and dynamic slicing, precise call graphs, and new algorithms for determining potential sources of values and explanations for why a line of code was not reached. Evaluations of the tool on one task showed that novice programmers with the Whyline were twice as fast as expert programmers without it. The tool has the potential to simplify debugging in many software development contexts.
The next step: from end-user programming to end-user software engineering	Is it possible to bring the benefits of rigorous software engineering methodologies to end users? End users create software when they use spreadsheet systems, web authoring tools and graphical languages, when they write educational simulations, spreadsheets, and dynamic e-business web applications. Unfortunately, however, errors are pervasive in end-user software, and the resulting impact is sometimes enormous. A growing number of researchers and developers are working on ways to make the software created by end users more reliable. This workshop brings together researchers who are addressing this topic with industry representatives who are deploying end-user programming applications, to facilitate sharing of real-world problems and solutions.
Program comprehension as fact finding	Little is known about how developers think about design during code modification tasks or how experienced developers' design knowledge helps them work more effectively. We performed a lab study in which thirteen developers worked for 3 hours under-standing the design of a 54 KLOC open source application. Par-ticipants had from 0 to 10.5 years of industry experience and were grouped into three "experts" and ten "novices." We observed that participants spent their time seeking, learning, critiquing, explain-ing, proposing, and implementing facts about the code such as "getFoldLevel has effects". These facts served numerous roles, such as suggesting changes, constraining changes, and predicting the amount of additional investigation necessary to make a change. Differences between experts and novices included that the experts explained the root cause of the design problem and made changes to address it, while novice changes addressed only the symptoms. Experts did not read more methods but also did not visit some methods novices wasted time understanding. Experts talked about code in terms of abstractions such as "caching" while novices more often described code statement by statement. Ex-perts were able to implement a change faster than novices. Experts perceived problems novices did not and were able to explain facts novices could not. These findings have interesting implications for future tools.
The first workshop on end-user software engineering	WEUSE is a workshop dedicated to the problems faced by end-user programmers, and research that can address those problems.
Testing vs. code inspection vs. what else?: male and female end users' debugging strategies	Little is known about the strategies end-user programmers use in debugging their programs, and even less is known about gender differences that may exist in these strategies. Without this type of information, designers of end-user programming systems cannot know the "target" at which to aim, if they are to support male and female end-user programmers. We present a study investigating this issue. We asked end-user programmers to debug spreadsheets and to describe their debugging strategies. Using mixed methods, we analyzed their strategies and looked for relationships among participants' strategy choices, gender, and debugging success. Our results indicate that males and females debug in quite different ways, that opportunities for improving support for end-user debugging strategies for both genders are abundant, and that tools currently available to end-user debuggers may be especially deficient in supporting debugging strategies used by females.
Punctuation as implicit annotations for chinese word segmentation	We present a Chinese word segmentation model learned from punctuation marks which are perfect word delimiters. The learning is aided by a manually segmented corpus. Our method is considerably more effective than previous methods in unknown word recognition. This is a step toward addressing one of the toughest problems in Chinese word segmentation.
Contextual dependencies in unsupervised word segmentation	Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech. We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively. The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation. We also show that previous probabilistic models rely crucially on sub-optimal search procedures.
Accessor variety criteria for Chinese word extraction	We are interested in the problem of word extraction from Chinese text collections. We define a word to be a meaningful string composed of several Chinese characters. For example, 'percent', and, 'more and more', are not recognized as traditional Chinese words from the viewpoint of some people. However, in our work, they are words because they are very widely used and have specific meanings. We start with the viewpoint that a word is a distinguished linguistic entity that can be used in many different language environments. We consider the characters that are directly before a string (predecessors) and the characters that are directly after a string (successors) as important factors for determining the independence of the string. We call such characters accessors of the string, consider the number of distinct predecessors and successors of a string in a large corpus (TREC 5 and TREC 6 documents), and use them as the measurement of the context independency of a string from the rest of the sentences in the document. Our experiments confirm our hypothesis and show that this simple rule gives quite good results for Chinese word extraction and is comparable to, and for long words outperforms, other iterative methods.
Shallow parsing with conditional random fields	Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.
Chinese segmentation and new word detection using conditional random fields	Chinese word segmentation is a difficult, important and widely-studied sequence modeling problem. This paper demonstrates the ability of linear-chain conditional random fields (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the integration of domain knowledge in the form of multiple lexicons of characters and words. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets used in a recent comprehensive Chinese word segmentation competition. State-of-the-art performance is obtained.
Interaction criticism: a proposal and framework for a new discipline of hci	Though interaction designers critique interfaces as a regular part of their research and practice, the field of HCI lacks a proper discipline of interaction criticism. By interaction criticism we mean rigorous, evidence-based interpretive analysis that explicates relationships among elements of an interface and the meanings, affects, moods, and intuitions they produce in the people that interact with them; the immediate goal of this analysis is the generation of innovative design insights. We summarize existing work offering promising directions in interaction criticism to build a case for a proper discipline. We then propose a framework for the discipline, relating each of its parts to recent HCI research.
Affect: from information to interaction	While affective computing explicitly challenges the primacy of rationality in cognitivist accounts of human activity, at a deeper level it relies on and reproduces the same information-processing model of cognition. In affective computing, affect is often seen as another kind of information - discrete units or states internal to an individual that can be transmitted in a loss-free manner from people to computational systems and back. Drawing on cultural, social, and interactional critiques of cognition which have arisen in HCI, we introduce and explore an alternative model of emotion as interaction: dynamic, culturally mediated, and socially constructed and experienced. This model leads to new goals for the design and evaluation of affective systems - instead of sensing and transmitting emotion, systems should support human users in understanding, interpreting, and experiencing emotion in its full complexity and ambiguity.
Revisiting usability's three key principles	The foundations of much HCI research and practice were elaborated over 20 years ago as three key principles by Gould and Lewis [7]: early focus on users and tasks; empirical measurement; and iterative design. Close reading of this seminal paper and subsequent versions indicates that these principles evolved, and that success in establishing them within software development involved a heady mix of power and destiny. As HCI's fourth decade approaches, we re-examine the origins and status of Gould and Lewis' principles, and argue that is time to move on, not least because the role of the principles in reported case studies is unconvincing. Few, if any, examples of successful application of the first or second principles are offered, and examples of the third tell us little about the nature of successful iteration. More credible, better grounded and more appropriate principles are needed. We need not so much to start again, but to start for the first time, and argue from first principles for apt principles for designing.
Reflective design	As computing moves into every aspect of our daily lives, the values and assumptions that underlie our technical practices may unwittingly be propagated throughout our culture. Drawing on existing critical approaches in computing, we argue that reflection on unconscious values embedded in computing and the practices that it supports can and should be a core principle of technology design. Building on a growing body of work in critical computing, reflective design combines analysis of the ways in which technologies reflect and perpetuate unconscious cultural assumptions, with design, building, and evaluation of new computing devices that reflect alternative possibilities. We illustrate this approach through two design case studies.
Early olpc experiences in a rural uruguayan school	In this paper, we discuss children's and teachers' experiences in a small rural town in Uruguay where every child in elementary school has received a laptop from the OLPC Foundation. In conducting activities in classrooms, observing children, and speaking with their teachers we found that the laptops have had a positive impact so far, with children accessing information resources that were previously unavailable, creating content for the world to see, collaborating and learning from each other, and increasing their interest in reading and writing. We also noted several challenges that need to be addressed, some directly related to human-computer interaction including problems with input devices, basic interactions, and the conceptual design and localization of user interfaces.
Social dynamics of early stage co-design in developing regions	Technology arguably has the potential to play a key role in improving the lives of people in developing regions. However, these communities are not well understood and designers must thoroughly investigate possibilities for technological innovations in these contexts. We describe findings from two field studies in India and one in Uganda where we explore technological solutions in the domains of communication, microfinance and education. Two common underlying themes emerge from these studies: (1) local stakeholders can contribute cultural information relevant to design such as needs and practices through interaction with technology artifacts and (2) unique social network structures embedded within communities are crucial to the acceptance and potential adoption of technology. We end with a synthesis of the three experiences that draws some practical lessons for ICT designers to elicit meaningful feedback and participation from local stakeholders in developing regions communities.
Comicboarding: using comics as proxies for participatory design with children	Comicboarding is a participatory design method that uses specially created comic books to generate engaging, productive brainstorming sessions with children. By leveraging known plot formats, interaction styles, and characters in comics, researchers can elicit ideas even from children who are not accustomed to brainstorming, such as those from schools were rote learning is the norm. We conducted an experiment using two variants of the comicboarding methodology with 17 children in China, where traditional participatory design may fail in the face of local cultural practices. The results suggest that comicboarding holds promise for co-design with children.
Multiple mice for retention tasks in disadvantaged schools	This study evaluates single-mouse and multiple-mice configurations for computer-aided learning in schools where access to computers is limited due to resource constraints. Multimouse, a single display groupware solution, developed to allow multiple mice to be used simultaneously on a single PC, is compared with single-user-single-mouse and multiple-user-single-mouse scenarios. Multimouse itself is trialed with two unique interaction designs -- one where competitive interaction among students is encouraged, and another where more collaborative interaction is expected. Experiments were conducted with 238 schoolchildren from underprivileged households in rural India on an English vocabulary retention task. On the whole, Multimouse configurations (five users each) were found to be at par with single-user scenarios in terms of actual words learned by students. This suggests that the value of a PC can be inexpensively multiplied by employing a multi-input shared-use design. Gender effects were found, where boys show significant differences in learning depending on interaction modality, whereas girls learned at similar rates across configurations. In addition, a comparison of the two Multimouse modes -- collaborative and competitive -- showed the striking difference in learning outcomes and user behavior that is possible due to even slight variations in interaction designs for multiple-mice.
Slurp: tangibility spatiality and an eyedropper	The value of tangibility for ubiquitous computing is in its simplicity-when faced with the question of how to grasp a digital object, why not just pick it up? But this is problematic; digital media is powerful due to its extreme mutability and is therefore resistant to the constraints of static physical form. We present Slurp, a tangible interface for locative media interactions in a ubiquitous computing environment. Based on the affordances of an eyedropper, Slurp provides haptic and visual feedback while extracting and injecting pointers to digital media between physical objects and displays.
On tangible user interfaces, humans and spatiality	Like the prehistoric twig and stone, tangible user interfaces (TUIs) are objects manipulated by humans. Tangible user interface success will depend on how well they exploit spatiality, the intuitive spatial skills humans have with the objects they use. In this paper, we carefully examine the relationship between humans and physical objects, and related previous research. From this examination, we distill a set of observations and turn these into heuristics for incorporation of spatiality into TUI application design, a cornerstone for their success. Following this line of thought, we identify “spatial TUIs,” the subset of TUIs that mediate interaction with shape, space and structure. We then examine several existing spatial TUIs using our heuristics.
The TAC paradigm: specifying tangible user interfaces	This paper introduces a paradigm for describing and specifying tangible user interfaces (TUIs). The proposed Token and Constraints (TAC) paradigm captures the core components of TUIs while addressing many of the conceptual challenges unique to building these interfaces. The paradigm enables the description of a broad range of TUIs by providing a common set of constructs. Thus, the TAC paradigm lays the foundation for a high-level description language and a software toolkit for TUIs. We evaluate the proposed paradigm by testing its ability to specify a wide variety of existing TUIs.
Optimal parameters for efficient crossing-based dialog boxes	We present an empirical analysis of crossing-based dialog boxes. First, we study the spatial constraints required for efficient crossing-based interactions in the case of a simple multi-parameter dialog box. Through a series of 3 tasks, we establish the minimal value of the landing margin, the takeoff margin, and the column width. We also offer an estimation of the role of stroke shape on user performance. After studying the reasons for errors during our experiment, we propose a relaxed crossing semantic that combines aspects of pointing and crossing-based interfaces. To test our design, we compare a naïve dialog box implementation with our new implementation, as well as a standard point-and-click dialog box. Our results reveal that there is not a significant difference between the naïve crossing implementation and the standard point-and-click interface and that the new crossing semantic is faster than both the naïve crossing implementation and the point-and-click interface, despite a higher error rate. Together these two experiments establish that crossing-based dialog boxes can be as spatially efficient and faster than their point-and-click counterpart. Our new semantic provides the first step towards a smooth transition from point-and-click interfaces to crossing-based interfaces.
Refining Fitts' law models for bivariate pointing	We investigate bivariate pointing in light of the recent progress in the modeling of univariate pointing. Unlike previous studies, we focus on the effect of target shape (width and height ratio) on pointing performance, particularly when such a ratio is between 1 and 2. Results showed unequal impact of amplitude and directional constraints, with the former dominating the latter. Investigating models based on the notion of weighted Lp norm, we found that our empirical findings were best captured by an Euclidean model with one free weight. This model significantly outperforms the best model to date.
Interactive Tools for Virtual X-Ray Vision in Mobile Augmented Reality	This paper presents a set of interactive tools designed to give users Virtual X-Ray vision. These tools address a common problem in depicting occluded infrastructure: either too much information is displayed, confusing users, or too little information is displayed, depriving users of important depth cues. Four tools are presented: The Tunnel Tool and Room Selector Tool directly augment the user's view of the environment, allowing them to explore the scene in direct, first person view. The Room in Miniature Tool allows the user to select and interact with a room from a third person perspective, allowing users to view the contents of the room from points of view that would normally be difficult or impossible to achieve. The Room Slicer Tool aids users in exploring volumetric data displayed within the Room in Miniature tool. Used together, the tools presented in this paper can be used to achieve the virtual x-ray vision effect. We test our prototype system in a far-field mobile augmented reality setup, visualizing the interiors of a small set of buildings on the UCSB campus.
An annotated situation-awareness aid for augmented reality	We present a situation-awareness aid for augmented reality systems based on an annotated "world in miniature." Our aid is designed to provide users with an overview of their environment that allows them to select and inquire about the objects that it contains. Two key capabilities are discussed that are intended to address the needs of mobile users. The aid's position, scale, and orientation are controlled by a novel approach that allows the user to inspect the aid without the need for manual interaction. As the user alternates their attention between the physical world and virtual aid, popup annotations associated with selected objects can move freely between the objects' representations in the two models.
Enhancing online personal connections through the synchronized sharing of online video	Going to movies in a group and inviting friends over to watch TV are common social activities. This social engagement both improves the viewing experience and helps us stay close with our friends and family. To bring this feeling of co-presence to the Internet, we developed a set of prototypes that enable people to feel more connected by watching web video together in sync. We present the preliminary results of a quantitative usage study and show initial evidence that simultaneous video sharing online can help people feel closer and more connected to their friends and family.
Zync: the design of synchronized video sharing	In this sketch, we present a design research approach which led to the development of Zync, a synchronized video player that provides a social viewing experience for online videos. This approach utilizes an iterative design process, focuses on research pragmatics over semantics, and examines the landscape of existing tools and technologies to identify the best venue for deployment. In lieu of creating an entirely new collaboration tool, we chose to build Zync as a plug-in module for a popular instant messaging (IM) client to help foster conversations where they normally occur. Zync augments the IM experience by enabling the inclusion of online videos within these conversations. Based on Zync usage data, we identified three classes of people who share videos via IM, obtained insight into what people want to watch, and created a framework to understand how people behave and hold conversations in synchronicity with temporal media.
The television will be revolutionized: effects of PVRs and filesharing on television watching	This paper investigates television-watching practices amongst early adopters of personal hard-disk video recorders (PVRs such as TiVotm) and Internet downloading of shows. Through in-depth interviews with early adopters, we describe how the rhythms of television watching change when decoupled from broadcast TV. For both the PVR users and downloaders TV watching has become less of a passive process, with viewers instead actively gathered shows from the schedules or online, and watching shows from their stored collection. From these results we discuss the 'video media lifecycle', and three new design concepts for supporting TV watching.
Evaluating automatically generated location-based stories for tourists	Tourism provides over six percent of the world's gross domestic product. As a result, there have been many efforts to use technology to improve the tourist's experience via mobile tour guide systems. One key bottleneck in such location-based systems is content development; existing systems either provide trivial information at a global scale or present quality narratives but at an extremely local scale. The primary reason for this dichotomy is that, although good narrative content is more educationally effective (and more entertaining) than a stream of simple, disconnected facts, it is time-intensive and expensive to develop. However, the WikEar system uses narrative theory-informed data mining methodologies in an effort to produce high-quality narrative content for any location on Earth. It allows tourists to interact with these narratives using their camera-enabled cell phones and an innovative interface designed around a magic lens and paper map metaphor. In this paper, we describe a first evaluation of these narratives and the WikEar interface, which reported promising, but not conclusive, results. We also present ideas for future work that will use this feedback to improve the narratives.
Improving interaction with virtual globes through spatial thinking: helping users ask "why?"	Virtual globes have progressed from little-known technology to broadly popular software in a mere few years. We investigated this phenomenon through a survey and discovered that, while virtual globes are en vogue, their use is restricted to a small set of tasks so simple that they do not involve any spatial thinking. Spatial thinking requires that users ask "what is where" and "why"; the most common virtual globe tasks only include the "what". Based on the results of this survey, we have developed a multi-touch virtual globe derived from an adapted virtual globe paradigm designed to widen the potential uses of the technology by helping its users to inquire about both the "what is where" and "why" of spatial distribution. We do not seek to provide users with full GIS (geographic information system) functionality, but rather we aim to facilitate the asking and answering of simple "why" questions about general topics that appeal to a wide virtual globe user base.
REXplorer: a mobile, pervasive spell-casting game for tourists	REXplorer is a mobile, pervasive spell-casting game designed for tourists of Regensburg, Germany. The game uses location sensing to create player encounters with spirits (historical figures) that are associated with historical buildings in an urban setting. A novel mobile interaction mechanism of "casting a spell" (making a gesture by waving a mobile phone through the air) allows the player to awaken and communicate with a spirit to continue playing the game. The game is designed to make learning history fun for young (and young at heart) tourists and influence their path through the city.
Hd touch: multi-touch and object sensing on a high definition lcd tv	In this paper, we describe our first prototype in implementing a robust and low-cost multi-touch and tangible system using a High Definition LCD monitor. Since our prototype utilizes an LCD, we discuss and compare the advantages that HD LCD monitors provide over projectors. Secondly, we give an overview of the sensing data the system can detect, and what interaction techniques it enables. Since our approach is scalable, we anticipate being able to implement multi-touch and tangible interactions on large high-definition LCD monitors in the future.
ThinSight: versatile multi-touch sensing for thin form-factor displays	ThinSight is a novel optical sensing system, fully integrated into a thin form factor display, capable of detecting multi-ple fingers placed on or near the display surface. We describe this new hardware in detail, and demonstrate how it can be embedded behind a regular LCD, allowing sensing without degradation of display capability. With our approach, fingertips and hands are clearly identifiable through the display. The approach of optical sensing also opens up the exciting possibility for detecting other physical objects and visual markers through the display, and some initial experiments are described. We also discuss other novel capabilities of our system: interaction at a distance using IR pointing devices, and IR-based communication with other electronic devices through the display. A major advantage of ThinSight over existing camera and projector based optical systems is its compact, thin form-factor making such systems even more deployable. We therefore envisage using ThinSight to capture rich sensor data through the display which can be processed using computer vision techniques to enable both multi-touch and tangible interaction.
International ethnographic observation of social networking sites	Current research on social networking largely covers US providers. To investigate broader trends, we examine cross-cultural differences in the usage patterns of social networking services with observation and ethnographic interviews in multiple cultures. This appears to be the first systematic investigation of social networking behavior across multiple cultures. We report here on the first four locations with observation and interviews of 36 respondents, 8-10 in each of the US, France, China, and South Korea. The results show three dimensions of cultural difference for typical social networking behaviors: the users' goals, typical pattern of self expression, and common interaction behaviors. These differences exemplify a developmental path of interest in social networking and the gradual integration of social networking behavior into more general communications behaviors. Future work in other cultures and with additional methods will evaluate the hypotheses presented here.
Imagined communities: awareness, information sharing, and privacy on the facebook	Online social networks such as Friendster, MySpace, or the Facebook have experienced exponential growth in membership in recent years. These networks offer attractive means for interaction and communication, but also raise privacy and security concerns. In this study we survey a representative sample of the members of the Facebook (a social network for colleges and high schools) at a US academic institution, and compare the survey data to information retrieved from the network itself. We look for underlying demographic or behavioral differences between the communities of the network's members and non-members; we analyze the impact of privacy concerns on members' behavior; we compare members' stated attitudes with actual behavior; and we document the changes in behavior subsequent to privacy-related information exposure. We find that an individual's privacy concerns are only a weak predictor of his membership to the network. Also privacy concerned individuals join the network and reveal great amounts of personal information. Some manage their privacy concerns by trusting their ability to control the information they provide and the external access to it. However, we also find evidence of members' misconceptions about the online community's actual size and composition, and about the visibility of members' profiles.
Mobile science learning for the blind	Science learning for blind people is limited. For a variety of reasons there is a very low emphasis on science learning for such users, especially those from deprived communities. We have designed, implemented and evaluated the usability of AudioNature, an audio-based interface implemented for pocketPC devices to assist with science learning in users with visual impairments. The usability and the cognitive impact of the device were evaluated. Users accepted the interface, enjoyed the interaction with AudioNature, felt motivated, and learned science. Preliminary results provided evidence that points towards gains in problem solving skills and showed that game-based learning activities facilitate the user's interaction with the software.
Children and emerging wireless technologies: investigating the potential for spatial practice	In this paper, we describe design work with 36 children aged 9 and 10 in Bristol, United Kingdom. The design work was conducted using emerging mobile and wireless technology which has the potential to impact on the problematic issue of children's access to, use of, and safety within the wider urban environment. A series of workshops are described in which children were encouraged to think about their use of an outdoor space before their introduction to the technology. The children designed and created "soundscapes" in the outdoor environment. The future potential impact of the technology on children's spatial practice is discussed and the concept of children "tagging" environmental hazards is raised.
Opportunities to support parents in managing their children's health	Parents always desire to take good care of their children and manage their numerous responsibilities. One of parents' main responsibilities is to manage their children's health. Through their actions of caring for their children, parents want to know they're doing the best job to ensure their children's well being. Unfortunately, much of the time this responsibility is a challenge-particularly for busy, dual-income parents-because it involves the collection, organization, retrieval, and transfer of information between many people in many different contexts. In our user research with dual-income parents they shared their experiences of forgetting to give medication, and of both not having an easy way of recording information and not having the information they needed when communicating with childcare and healthcare providers. Smart home technology appears to offer a promise to easy this situation for parents; however, the HCI community has only investigated healthcare in the home with a focus on elders. To better understand this opportunity area we conducted a user-centered design project looking at the management of children's by their parents.
Smart home care network using sensor fusion and distributed vision-based reasoning	A wireless sensor network employing multiple sensing and event detection modalities and distributed processing is proposed for smart home monitoring applications. Image sensing and vision-based reasoning are employed to verify and further analyze events reported by other sensors. The system has been developed to address the growing application domain in caregiving to the elderly and persons in need of monitored living, who care to live independently while enjoying the assurance of timely access to caregivers when needed. An example of sensed events is the accidental fall of the person under care. A wireless badge node acts as a bridge between the user and the network. The badge node provides user-centric event sensing functions such as detecting falls, and also provides a voice communication channel between the user and the caregiving center when the system detects an alert and dials the center. The voice connection is carried over an IEEE 802.15.4 radio link between the user badge and another node in the network that acts as a modem. Using signal strength measurements, the network nodes keep track of the approximate location of the user in the monitoring environment.The network also includes wall-mounted image sensor nodes, which are triggered upon detection of a fall to analyze their field-of-view and provide the caregiving center with further information about the user 's status. A description of the developed network and several examples of the vision-based reasoning algorithm are presented in the paper.
Select-and-point: a novel interface for multi-device connection and control based on simple hand gestures	Select-and-Point provides us with a new interface and intuitive interaction style in our daily computer use. With simple selection and pointing hand gestures, users can eliminate cumbersome processes in managing connections and controls between multiple devices as well as in sharing information/data. We implemented a Select-and-Point system in an intelligent meeting room and performed a preliminary user study in this environment. The results show that Select-and-Point is easily accepted by users and it can significantly improve users' interaction with various devices in a ubiquitous computing environment.
Table-centric interactive spaces for real-time collaboration	Tables have historically played a key role in many real-time collaborative environments, often referred to as "war rooms". Today, these environments have been transformed by computational technology into spaces with large vertical displays surrounded by numerous desktop computers. However, despite significant research activity in the area of tabletop computing, very little is known about how to best integrate a digital tabletop into these multi-surface environments. In this paper, we identify various design requirements for the implementation of a system intended to support such an environment. We then present a set of designs that demonstrate how an interactive tabletop can be used in a real-time operations center to facilitate collaborative situation-assessment and decision-making.
Information spaces -- building meeting rooms in virtual environments	Virtual worlds are typically designed to recreate the familiar physical world, both in the design of the spaces and the ways that people interact within them. In this paper we describe an alternate approach that uses the computational capabilities unique to the virtual world to augment social interaction and personal experience. We propose a specific design for supporting medium sized group meetings using avatar's positions in the space to represent their feelings about the discussion and discuss our preliminary testing results.
Meeting central: making distributed meetings more effective	The Meeting Central prototype is a suite of collaboration tools designed to support distributed meetings. The tools' minimalist design provides only those features that have the most impact on distributed meeting effectiveness. The collaboration suite is built on top of a distributed, extensible, and scalable framework.
The chat circles series: explorations in designing abstract graphical communication interfaces	We have been creating a series of graphical chat programs designed to foster social interaction and expressive communication. We started with a spare, minimalist interface and in subsequent programs have modified its fundamental features: background space, individual representation, movement implementation, communication channels, and history depiction. The resulting family of graphical chat programs share many interface features but differ significantly in their feel and function. This paper examines the variations among the interfaces and discusses their implications for social interaction.
User recommendation for collaborative and personalised digital archives	Every day, a huge amount of newly created information is electronically published in digital libraries. Complementary to the usual vision, we envisage a digital library not only as an information source where users may submit queries to satisfy their daily information need, but also as a collaborative working and meeting space of people sharing common interests.
A Personalized Collaborative Digital Library Environment	We envisage a Digital Library not only as an information resource where users may submit queries to satisfy their information needs, but also as a collaborative working and meeting space. We present a personalized collaborative Digital Library environment, where users may organise the information space according to their own subjective view, become aware of each other, exchange information and knowledge with each other, build communities and get recommendations based on preference patterns of other users.
Scaling Parallel Programs for Multiprocessors: Methodology and Examples	Models for the constraints under which an application should be scaled, including constant problem-size scaling, memory-constrained scaling, and time-constrained scaling, are reviewed. A realistic method is described that scales all relevant parameters under considerations imposed by the application domain. This method leads to different conclusions about the effectiveness and design of large multiprocessors than the naive practice of scaling only the data set size. The primary example application is a simulation of galaxies using the Barnes-Hut hierarchical N-body method.
The directory-based cache coherence protocol for the DASH multiprocessor	DASH is a scalable shared-memory multiprocessor currently being developed at Stanford's Computer Systems Laboratory. The architecture consists of powerful processing nodes, each with a portion of the shared-memory, connected to a scalable interconnection network. A key feature of DASH is its distributed directory-based cache coherence protocol. Unlike traditional snoopy coherence protocols, the DASH protocol does not rely on broadcast; instead it uses point-to-point messages sent between the processors and memories to keep caches consistent. Furthermore, the DASH system does not contain any single serialization or control point. While these features provide the basis for scalability, they also force a reevaluation of many fundamental issues involved in the design of a protocol. These include the issues of correctness, performance and protocol complexity. In this paper, we present the design of the DASH coherence protocol and discuss how it addresses the above issues. We also discuss our strategy for verifying the correctness of the protocol and briefly compare our protocol to the IEEE Scalable Coherent Interface protocol.
Sensor coverage in wireless ad hoc sensor networks	We present our solutions to the general sensor coverage problem in wireless ad hoc sensor networks, where the sensors are heterogeneous in terms of sensing, communication and motion capabilities. We propose an integrated framework to accommodate such heterogeneity, in which sensors are logically organised into different tiers. We then address, for each given set of heterogeneous sensors, how the coverage problem can be solved. We first present our Sensing Neighbour Acquiring Protocol (SNAP) which renders correct neighbour information necessary for coverage algorithms. Then we provide our insights on coverage and the necessary and sufficient conditions for the coverage degree determination in a target area. Such theoretical results lead to our general and efficient coverage determination algorithm. Further, we prove that by using SNAP, coverage implies network connectivity. Without assuming uniform sensing ranges, our protocol, algorithm and theoretical results are generally applicable in a wide range of sensor network applications.
Integrated coverage and connectivity configuration in wireless sensor networks	An effective approach for energy conservation in wireless sensor networks is scheduling sleep intervals for extraneous nodes, while the remaining nodes stay active to provide continuous service. For the sensor network to operate successfully, the active nodes must maintain both sensing coverage and network connectivity. Furthermore, the network must be able to configure itself to any feasible degrees of coverage and connectivity in order to support different applications and environments with diverse requirements. This paper presents the design and analysis of novel protocols that can dynamically configure a network to achieve guaranteed degrees of coverage and connectivity. This work differs from existing connectivity or coverage maintenance protocols in several key ways: 1) We present a Coverage Configuration Protocol (CCP) that can provide different degrees of coverage requested by applications. This flexibility allows the network to self-configure for a wide range of applications and (possibly dynamic) environments. 2) We provide a geometric analysis of the relationship between coverage and connectivity. This analysis yields key insights for treating coverage and connectivity in a unified framework: this is in sharp contrast to several existing approaches that address the two problems in isolation. 3) Finally, we integrate CCP with SPAN to provide both coverage and connectivity guarantees. We demonstrate the capability of our protocols to provide guaranteed coverage and connectivity configurations, through both geometric analysis and extensive simulations.
Exposure in wireless Ad-Hoc sensor networks	Wireless ad-hoc sensor networks will provide one of the missing connections between the Internet and the physical world. One of the fundamental problems in sensor networks is the calculation of coverage. Exposure is directly related to coverage in that it is a measure of how well an object, moving on an arbitrary path, can be observed by the sensor network over a period of time.In addition to the informal definition, we formally define exposure and study its properties. We have developed an efficient and effective algorithm for exposure calculation in sensor networks, specifically for finding minimal exposure paths. The minimal exposure path provides valuable information about the worst case exposure-based coverage in sensor networks. The algorithm works for any given distribution of sensors, sensor and intensity models, and characteristics of the network. It provides an unbounded level of accuracy as a function of run time and storage. We provide an extensive collection of experimental results and study the scaling behavior of exposure and the proposed algorithm for its calculation.
Geography-informed energy conservation for Ad Hoc routing	We introduce a geographical adaptive fidelity (GAF) algorithm that reduces energy consumption in ad hoc wireless networks. GAF conserves energy by identifying nodes that are equivalent from a routing perspective and then turning off unnecessary nodes, keeping a constant level of routing fidelity. GAF moderates this policy using application- and system-level information; nodes that source or sink data remain on and intermediate nodes monitor and balance energy use. GAF is independent of the underlying ad hoc routing protocol; we simulate GAF over unmodified AODV and DSR. Analysis and simulation studies of GAF show that it can consume 40% to 60% less energy than an unmodified ad hoc routing protocol. Moreover, simulations of GAP suggest that network lifetime increases proportionally to node density; in one example, a four-fold increase in node density leads to network lifetime increase for 3 to 6 times (depending on the mobility pattern). More generally, GAF is an example of adaptive fidelity, a technique proposed for extending the lifetime of self-configuring systems by exploiting redundancy to conserve energy while maintaining application fidelity.
Span: An energy-efficient coordination algorithm for topology maintenance in Ad Hoc wireless networks	This paper presents Span, a power saving technique for multi-hop ad hoc wireless networks that reduces energy consumption without significantly diminishing the capacity or connectivity of the network. Span builds on the observation that when a region of a shared-channel wireless network bag a sufficient density of nodes, only a small number of them need be on at any time to forward traffic for active connections.Span is a distributed, randomized algorithm where nodes make local decisions on whether to sleep, or to join a forwarding backbone as a coordinator. Each node bases its decision on an estimate of how many of its neighbors will benefit from it being awake, and the amount of energy available to it. We give a randomized algorithm where coordinators rotate with time, demonstrating how localized node decisions lead to a connected, capacity-preserving global topology.Improvement in system lifetime due to Span increases as the ratio of idle-to-sleep energy consumption increases, and increases as the density of the network increases. For example, our simulations show that with a practical energy model, system lifetime of an 802.11 network in power saving mode with Span is a factor of two better than without. Span integrates nicely with 802.11—when run in conjunction with the 802.11 power saving mode, Span improves communication latency, capacity, and system lifetime.
Dynamic fine-grained localization in Ad-Hoc networks of sensors	The recent advances in radio and em beddedsystem technologies have enabled the proliferation of wireless microsensor networks. Such wirelessly connected sensors are released in many diverse environments to perform various monitoring tasks. In many such tasks, location awareness is inherently one of the most essential system parameters. It is not only needed to report the origins of events, but also to assist group querying of sensors, routing, and to answer questions on the network coverage. In this paper we present a novel approach to the localization of sensors in an ad-hoc network. We describe a system called AHLoS (Ad-Hoc Localization System) that enables sensor nodes to discover their locations using a set distributed iterative algorithms. The operation of AHLoS is demonstrated with an accuracy of a few centimeters using our prototype testbed while scalability and performance are studied through simulation.
On the (un)reliability of eavesdropping	We investigate the reliability of current generation eavesdropping tools and show that obtaining 'high fidelity' transcripts is harder than previously assumed. Even in situations highly favourable to the eavesdropper, simple unilateral countermeasures are shown to be sufficient to prevent all tested systems from reliably reconstructing communicated messages. Less than a third of the tested systems report irregularities, and 45% incorrectly interpret covertext chosen by the sending party. Unlike cryptography or steganography, the techniques introduced require no cooperation by the communicating parties and, in some case, can be employed entirely by a third party not involved in the communication at all.
Tor: the second-generation onion router	We present Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design by adding perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for location-hidden services via rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency. We briefly describe our experiences with an international network of more than 30 nodes. We close with a list of open problems in anonymous communication.
Active Mapping: Resisting NIDS Evasion without Altering Traffic	A critical problem faced by a Network Intrusion DetectionSystem (NIDS) is that of ambiguity.TheNIDScannot always determine what traffic reaches a givenhost nor how that host will interpret the traffic, and attackersmay exploit this ambiguity to avoid detection orcause misleading alarms. We present a lightweight solution,Active Mapping, which eliminates TCP/IP-basedambiguity in a NIDS' analysis with minimal runtimecost. Active Mapping efficiently builds profiles of thenetwork topology and the TCP/IP policies of hosts onthe network; a NIDS may then use the host profiles todisambiguate the interpretation of the network traffic ona per-host basis. Active Mapping avoids the semanticand performance problems of traffic normalization,inwhich traffic streams are modified to remove ambiguities.We have developed a prototype implementation ofActive Mapping and modified a NIDS to use the ActiveMapping-generated profile database in our tests. Wefound wide variation across operating systems' TCP/IPstack policies in real-world tests (about 6,700 hosts), underscoringthe need for this sort of disambiguation.
Efficient multicast stream authentication for the fully adversarial network model	We consider the stream authentication problem when an adversary has the ability to drop, reorder or inject data in the network. We propose a coding approach for multicast stream authentication using the list-decoding property of Reed-Solomon codes. We divide the data to be authenticated into a stream of packets and associate a single trapdoor hash collision for every λn packets where λ and n are predesignated parameters. Our scheme, which is also joinable at the boundary of any n-packet block, can be viewed as an extension of Lysyanskaya, Tamassia and Triandopoulos's technique in which λ = 1. We show that by choosing λ and n appropriately, our scheme outperforms theirs in time spent for processing data at the sender and receiver. Our approach relies on the dispersion process as SAIDA and eSAIDA. Assuming that we use RSA for signing and SHA-256 for hashing, we give an approximation of the proportion of extra packets per block which could be processed via our technique with respect to the previous scheme. As example when we process λ = 1000 blocks of 2650 64-byte-packets, the gain of our scheme with respect to Lysyanskaya et al.'s is about 30%.
Efficient multicast stream authentication using erasure codes	We describe a novel method for authenticating multicast packets that is robust against packet loss. Our focus is to minimize the size of the communication overhead required to authenticate the packets. Our approach is to encode the hash values and the signatures with Rabin's Information Dispersal Algorithm (IDA) to construct an authentication scheme that amortizes a single signature operation over multiple packets. This strategy is especially efficient in terms of space overhead, because just the essential elements needed for authentication (i.e., one hash per packet and one signature per group of packets) are used in conjunction with an erasure code that is space optimal. Using asymptotic techniques, we derive the authentication probability of our scheme using two different bursty loss models. A lower bound of the authentication probability is also derived for one of the loss models. To evaluate the performance of our scheme, we compare our technique with four other previously proposed schemes using empirical results.
Learning relations and networks in web-based communities	Learning entails exchange of information, resources, methods and practices; it entails conversation, and the co-creation of joint practice; it requires getting to know colleagues, their talents and their work practices, and joining a community of scholars, co workers and co-learners. In short, learning is predicated on interaction between individuals, interactions that build into communities that share common knowledge and practice. But what kinds of interactions are important for learning, and how do these help build and form a learning community? And how is this different or the same when accomplished in a web-based environment? Drawing on literature on learning, community and computer-mediated communication, and on results of a number of studies of learning networks conducted by the author, this paper addresses learning communities from a social network perspective, including what relations are evident in learning and learning communities, how media affect online tie formation, and what benefits can result from successfully maintained learning networks.
Using social psychology to motivate contributions to online communities	Under-contribution is a problem for many online communities. Social psychology theories of social loafing and goal-setting can provide mid-level design principles to address this problem. We tested the design principles in two field experiments. In one, members of an online movie recommender community were reminded of the uniqueness of their contributions and the benefits that follow from them. In the second, they were given a range of individual or group goals for contribution. As predicted by theory, individuals contributed when they were reminded of their uniqueness and when they were given specific and challenging goals, but other predictions were not borne out. The paper ends with suggestions and challenges for mining social science theories as well as implications for design.
Rethinking multi-channel protocols in wireless sensor networks	The availability of multiple frequency channels on modern radios has provided a way to improve networking performance. Nevertheless, current multi-channel protocols lack the architectural consistency and flexibility to support a diverse set of applications. In this paper we argue that it is necessary to integrate channel switching to the emerging wireless sensor network architecture and propose a way to decompose the problem into two reusable components: the channel allocation component that is integrated with network layer protocols and a shared channel synchronization component at the MAC layer. Furthermore, we outline how existing multi-channel protocols can be re-factored to comply with the proposed architecture and present ViR, an initial implementation of the channel synchronization component. Finally, using realistic applications synthesized from existing protocols, we show how ViR reduces conflicts among protocols and reduces packet losses.
Collection tree protocol	This paper presents and evaluates two principles for wireless routing protocols. The first is datapath validation: data traffic quickly discovers and fixes routing inconsistencies. The second is adaptive beaconing: extending the Trickle algorithm to routing control traffic reduces route repair latency and sends fewer beacons. We evaluate datapath validation and adaptive beaconing in CTP Noe, a sensor network tree collection protocol. We use 12 different testbeds ranging in size from 20--310 nodes, comprising seven platforms, and six different link layers, on both interference-free and interference-prone channels. In all cases, CTP Noe delivers 90% of packets. Many experiments achieve 99.9%. Compared to standard beaconing, CTP Noe sends 73% fewer beacons while reducing topology repair latency by 99.8%. Finally, when using low-power link layers, CTP Noe has duty cycles of 3% while supporting aggregate loads of 30 packets/minute.
CoPhy: a scalable, portable, and interactive index advisor for large workloads	Index tuning, i.e., selecting the indexes appropriate for a workload, is a crucial problem in database system tuning. In this paper, we solve index tuning for large problem instances that are common in practice, e.g., thousands of queries in the workload, thousands of candidate indexes and several hard and soft constraints. Our work is the first to reveal that the index tuning problem has a well structured space of solutions, and this space can be explored efficiently with well known techniques from linear optimization. Experimental results demonstrate that our approach outperforms state-of-the-art commercial and research techniques by a significant margin (up to an order of magnitude).
Constrained physical design tuning	Existing solutions to the automated physical design problem in database systems attempt to minimize execution costs of input workloads for a given a storage constraint. In this paper, we argue that this model is not flexible enough to address several real-world situations. To overcome this limitation, we introduce a constraint language that is simple yet powerful enough to express many important scenarios. We build upon an existing transformation-based framework to effectively incorporate constraints in the search space. We then show experimentally that we are able to handle a rich class of constraints and that our proposed technique scales gracefully.
Analyzing plan diagrams of database query optimizers	A "plan diagram" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality; that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models; that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost; and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.
DB2 design advisor: integrated automatic physical database design	The DB2 Design Advisor in IBM® DB2® Universal DatabaseTM (DB2 UDB) Version 8.2 for Linux®, UNIX® and Windows® is a tool that, for a given workload, automatically recommends physical design features that are any subset of indexes, materialized query tables (also called materialized views), shared-nothing database partitionings, and multidimensional clustering of tables. Our work is the very first industrial-strength tool that covers the design of as many as four different features, a significant advance to existing tools, which support no more than just indexes and materialized views. Building such a tool is challenging, because of not only the large search space introduced by the interactions among features, but also the extensibility needed by the tool to support additional features in the future. We adopt a novel "hybrid" approach in the Design Advisor that allows us to take important interdependencies into account as well as to encapsulate design features as separate components to lower the reengineering cost. The Design Advisor also features a built-in module that automatically reduces the given workload, and therefore provides great scalability for the tool. Our experimental results demonstrate that our tool can quickly provide good physical design recommendations that satisfy users' requirements.
Index Selection for Databases: A Hardness Study and a Principled Heuristic Solution	We study the index selection problem: Given a workload consisting of SQL statements on a database, and a user-specified storage constraint, recommend a set of indexes that have the maximum benefit for the given workload. We present a formal statement for this problem and show that it is computationally "hard驴 to solve or even approximate it. We develop a new algorithm for the problem which is based on treating the problem as a knapsack problem. The novelty of our approach lies in an LP (linear programming) based method that assigns benefits to individual indexes. For a slightly modified algorithm, that does more work, we prove that we can give instance specific guarantees about the quality of our solution. We conduct an extensive experimental evaluation of this new heuristic and compare it with previous solutions. Our results demonstrate that our solution is more scalable while achieving comparable quality.
Configuration-parametric query optimization for physical design tuning	Automated physical design tuning for database systems has recently become an active area of research and development. Existing tuning tools explore the space of feasible solutions by repeatedly optimizing queries in the input workload for several candidate configurations. This general approach, while scalable, often results in tuning sessions waiting for results from the query optimizer over 90% of the time. In this paper we introduce a novel approach, called Configuration-Parametric Query Optimization, that drastically improves the performance of current tuning tools. By issuing a single optimization call per query, we are able to generate a compact representation of the optimization space that can then produce very efficiently execution plans for the input query under arbitrary configurations. Our experiments show that our technique speeds-up query optimization by 30x to over 450x with virtually no loss in quality, and effectively eliminates the optimization bottleneck in existing tuning tools. Our techniques open the door for new, more sophisticated optimization strategies by eliminating the main bottleneck of current tuning tools.
Tuffy: scaling up statistical inference in Markov logic networks using an RDBMS	Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their widespread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets.
Speeding up inference in Markov logic networks by preprocessing to reduce the size of the resulting grounded network	Statistical-relational reasoning has received much attention due to its ability to robustly model complex relationships. A key challenge is tractable inference, especially in domains involving many objects, due to the combinatorics involved. One can accelerate inference by using approximation techniques, "lazy" algorithms, etc. We consider Markov Logic Networks (MLNs), which involve counting how often logical formulae are satisfied. We propose a preprocessing algorithm that can substantially reduce the effective size of MLNs by rapidly counting how often the evidence satisfies each formula, regardless of the truth values of the query literals. This is a general preprocessing method that loses no information and can be used for any MLN inference algorithm. We evaluate our algorithm empirically in three real-world domains, greatly reducing the work needed during subsequent inference. Such reduction might even allow exact inference to be performed when sampling methods would be otherwise necessary.
Collective semantic role labelling with Markov logic	This paper presents our system for the Open Track of the CoNLL 2008 Shared Task (Surdeanu et al., 2008) in Joint Dependency Parsing and Semantic Role Labelling. We use Markov Logic to define a joint SRL model and achieve a semantic F-score of 74.59%, the second best in the Open Track.
Event queries on correlated probabilistic streams	A major problem in detecting events in streams of data is that the data can be imprecise (e.g. RFID data). However, current state-ofthe-art event detection systems such as Cayuga [14], SASE [46] or SnoopIB[1], assume the data is precise. Noise in the data can be captured using techniques such as hidden Markov models. Inference on these models creates streams of probabilistic events which cannot be directly queried by existing systems. To address this challenge we propose Lahar1, an event processing system for probabilistic event streams. By exploiting the probabilistic nature of the data, Lahar yields a much higher recall and precision than deterministic techniques operating over only the most probable tuples. By using a novel static analysis and novel algorithms, Lahar processes data orders of magnitude more efficiently than a naïve approach based on sampling. In this paper, we present Lahar's static analysis and core algorithms. We demonstrate the quality and performance of our approach through experiments with our prototype implementation and comparisons with alternate methods.
MCDB: a monte carlo approach to managing uncertain data	To deal with data uncertainty, existing probabilistic database systems augment tuples with attribute-level or tuple-level probability values, which are loaded into the database along with the data itself. This approach can severely limit the system's ability to gracefully handle complex or unforeseen types of uncertainty, and does not permit the uncertainty model to be dynamically parameterized according to the current state of the database. We introduce MCDB, a system for managing uncertain data that is based on a Monte Carlo approach. MCDB represents uncertainty via "VG functions," which are used to pseudorandomly generate realized values for uncertain attributes. VG functions can be parameterized on the results of SQL queries over "parameter tables" that are stored in the database, facilitating what-if analyses. By storing parameters, and not probabilities, and by estimating, rather than exactly computing, the probability distribution over possible query answers, MCDB avoids many of the limitations of prior systems. For example, MCDB can easily handle arbitrary joint probability distributions over discrete or continuous attributes, arbitrarily complex SQL queries, and arbitrary functionals of the query-result distribution such as means, variances, and quantiles. To achieve good performance, MCDB uses novel query processing techniques, executing a query plan exactly once, but over "tuple bundles" instead of ordinary tuples. Experiments indicate that our enhanced functionality can be obtained with acceptable overheads relative to traditional systems.
Efficient query evaluation on probabilistic databases	We describe a system that supports arbitrarily complex SQL queries on probabilistic databases. The query semantics is based on a probabilistic model and the results are ranked, much like in Information Retrieval. Our main focus is efficient query evaluation, a problem that has not received attention in the past. We describe an optimization algorithm that can compute efficiently most queries. We show, however, that the data complexity of some queries is #P-complete, which implies that these queries do not admit any efficient evaluation methods. For these queries we describe both an approximation algorithm and a Monte-Carlo simulation algorithm.
Bottom-up learning of Markov logic network structure	Markov logic networks (MLNs) are a statistical relational model that consists of weighted firstorder clauses and generalizes first-order logic and Markov networks. The current state-of-the-art algorithm for learning MLN structure follows a top-down paradigm where many potential candidate structures are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data. Even though this existing algorithm outperforms an impressive array of benchmarks, its greedy search is susceptible to local maxima or plateaus. We present a novel algorithm for learning MLN structure that follows a more bottom-up approach to address this problem. Our algorithm uses a "propositional" Markov network learning method to construct "template" networks that guide the construction of candidate clauses. Our algorithm significantly improves accuracy and learning time over the existing topdown approach in three real-world domains.
Towards efficient sampling: exploiting random walk strategies	From a computational perspective, there is a close connection between various probabilistic reasoning tasks and the problem of counting or sampling satisfying assignments of a propositional theory. We consider the question of whether state-of-the-art satisfiability procedures, based on random walk strategies, can be used to sample uniformly or nearuniformly from the space of satisfying assignments. We first show that random walk SAT procedures often do reach the full set of solutions of complex logical theories. Moreover, by interleaving random walk steps with Metropolis transitions, we also show how the sampling becomes near-uniform.
A general method for reducing the complexity of relational inference and its application to MCMC	Many real-world problems are characterized by complex relational structure, which can be succinctly represented in first-order logic. However, many relational inference algorithms proceed by first fully instantiating the first-order theory and then working at the propositional level. The applicability of such approaches is severely limited by the exponential time and memory cost of propositionalization. Singla and Domingos (2006) addressed this by developing a "lazy" version of the WalkSAT algorithm, which grounds atoms and clauses only as needed. In this paper we generalize their ideas to a much broader class of algorithms, including other types of SAT solvers and probabilistic inference methods like MCMC. Lazy inference is potentially applicable whenever variables and functions have default values (i.e., a value that is much more frequent than the others). In relational domains, the default is false for atoms and true for clauses. We illustrate our framework by applying it to MC-SAT, a state-of-the-art MCMC algorithm. Experiments on a number of real-world domains show that lazy inference reduces both space and time by several orders of magnitude, making probabilistic relational inference applicable in previously infeasible domains.
Sound and efficient inference with probabilistic and deterministic dependencies	Reasoning with both probabilistic and deterministic dependencies is important for many real-world problems, and in particular for the emerging field of statistical relational learning. However, probabilistic inference methods like MCMC or belief propagation tend to give poor results when deterministic or near-deterministic dependencies are present, and logical ones like satisfiability testing are inapplicable to probabilistic ones. In this paper we propose MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. MC-SAT is based on Markov logic, which defines Markov networks using weighted clauses in first-order logic. From the point of view of MCMC, MC-SAT is a slice sampler with an auxiliary variable per clause, and with a satisfiability-based method for sampling the original variables given the auxiliary ones. From the point of view of satisfiability, MCSAT wraps a procedure around the SampleSAT uniform sampler that enables it to sample from highly non-uniform distributions over satisfying assignments. Experiments on entity resolution and collective classification problems show that MC-SAT greatly outperforms Gibbs sampling and simulated tempering over a broad range of problem sizes and degrees of determinism.
Tuffy: scaling up statistical inference in Markov logic networks using an RDBMS	Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their widespread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets.
Automatically refining the wikipedia infobox ontology	The combined efforts of human volunteers have recently extracted numerous facts from Wikipedia, storing them as machine-harvestable object-attribute-value triples in Wikipedia infoboxes. Machine learning systems, such as Kylin, use these infoboxes as training data, accurately extracting even more semantic knowledge from natural language text. But in order to realize the full power of this information, it must be situated in a cleanly-structured ontology. This paper introduces KOG, an autonomous system for refining Wikipedia's infobox-class ontology towards this end. We cast the problem of ontology refinement as a machine learning problem and solve it using both SVMs and a more powerful joint-inference approach expressed in Markov Logic Networks. We present experiments demonstrating the superiority of the joint-inference approach and evaluating other aspects of our system. Using these techniques, we build a rich ontology, integrating Wikipedia's infobox-class schemata with WordNet. We demonstrate how the resulting ontology may be used to enhance Wikipedia with improved query processing and other features.
Lifted first-order belief propagation	Unifying first-order logic and probability is a long-standing goal of AI, and in recent years many representations combining aspects of the two have been proposed. However, inference in them is generally still at the level of propositional logic, creating all ground atoms and formulas and applying standard probabilistic inference methods to the resulting network. Ideally, inference should be lifted as in first-order logic, handling whole sets of indistinguishable objects together, in time independent of their cardinality. Poole (2003) and Braz et al. (2005, 2006) developed a lifted version of the variable elimination algorithm, but it is extremely complex, generally does not scale to realistic domains, and has only been applied to very small artificial problems. In this paper we propose the first lifted version of a scalable probabilistic inference algorithm, belief propagation (loopy or not). Our approach is based on first constructing a lifted network, where each node represents a set of ground atoms that all pass the same messages during belief propagation. We then run belief propagation on this network. We prove the correctness and optimality of our algorithm. Experiments show that it can greatly reduce the cost of inference.
Automatic optimization for MapReduce programs	The MapReduce distributed programming framework has become popular, despite evidence that current implementations are inefficient, requiring far more hardware than a traditional relational databases to complete similar tasks. MapReduce jobs are amenable to many traditional database query optimizations (B+Trees for selections, column-store-style techniques for projections, etc), but existing systems do not apply them, substantially because free-form user code obscures the true data operation being performed. For example, a selection in SQL is easily detected, but a selection in a MapReduce program is embedded in Java code along with lots of other program logic. We could ask the programmer to provide explicit hints about the program's data semantics, but one of MapReduce's attractions is precisely that it does not ask the user for such information. This paper covers Manimal, which automatically analyzes MapReduce programs and applies appropriate data-aware optimizations, thereby requiring no additional help at all from the programmer. We show that Manimal successfully detects optimization opportunities across a range of data operations, and that it yields speedups of up to 1,121% on previously-written MapReduce programs.
C-store: a column-oriented DBMS	This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.We present preliminary performance data on a subset of TPC-H and show that the system we are building, C-Store, is substantially faster than popular commercial products. Hence, the architecture looks very encouraging.
Optimizing joins in a map-reduce environment	Implementations of map-reduce are being used to perform many operations on very large data. We examine strategies for joining several relations in the map-reduce environment. Our new approach begins by identifying the "map-key," the set of attributes that identify the Reduce process to which a Map process must send a particular tuple. Each attribute of the map-key gets a "share," which is the number of buckets into which its values are hashed, to form a component of the identifier of a Reduce process. Relations have their tuples replicated in limited fashion, the degree of replication depending on the shares for those map-key attributes that are missing from their schema. We study the problem of optimizing the shares, given a fixed number of Reduce processes. An algorithm for detecting and fixing problems where an attribute is "mistakenly" included in the map-key is given. Then, we consider two important special cases: chain joins and star joins. In each case we are able to determine the map-key and determine the shares that yield the least replication. While the method we propose is not always superior to the conventional way of using map-reduce to implement joins, there are some important cases involving large-scale data where our method wins, including: (1) analytic queries in which a very large fact table is joined with smaller dimension tables, and (2) queries involving paths through graphs with high out-degree, such as the Web or a social network.
HaLoop: efficient iterative data processing on large clusters	The growing demand for large-scale data mining and data analysis applications has led both industry and academia to design new types of highly scalable data-intensive computing platforms. MapReduce and Dryad are two popular platforms in which the dataflow takes the form of a directed acyclic graph of operators. These platforms lack built-in support for iterative programs, which arise naturally in many applications including data mining, web ranking, graph analysis, model fitting, and so on. This paper presents HaLoop, a modified version of the Hadoop MapReduce framework that is designed to serve these applications. HaLoop not only extends MapReduce with programming support for iterative applications, it also dramatically improves their efficiency by making the task scheduler loop-aware and by adding various caching mechanisms. We evaluated HaLoop on real queries and real datasets. Compared with Hadoop, on average, HaLoop reduces query runtimes by 1.85, and shuffles only 4% of the data between mappers and reducers.
MapReduce: simplified data processing on large clusters	MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.
Sandnet: network traffic analysis of malicious software	Dynamic analysis of malware is widely used to obtain a better understanding of unknown software. While existing systems mainly focus on host-level activities of malware and limit the analysis period to a few minutes, we concentrate on the network behavior of malware over longer periods. We provide a comprehensive overview of typical malware network behavior by discussing the results that we obtained during the analysis of more than 100,000 malware samples. The resulting network behavior was dissected in our new analysis environment called Sandnet that complements existing systems by focusing on network traffic analysis. Our in-depth analysis of the two protocols that are most popular among malware authors, DNS and HTTP, helps to understand and characterize the usage of these prevalent protocols.
The nepenthes platform: an efficient approach to collect malware	Up to now, there is little empirically backed quantitative and qualitative knowledge about self-replicating malware publicly available. This hampers research in these topics because many counter-strategies against malware, e.g., network- and host-based intrusion detection systems, need hard empirical data to take full effect. We present the nepenthes platform, a framework for large-scale collection of information on self-replicating malware in the wild. The basic principle of nepenthes is to emulate only the vulnerable parts of a service. This leads to an efficient and effective solution that offers many advantages compared to other honeypot-based solutions. Furthermore, nepenthes offers a flexible deployment solution, leading to even better scalability. Using the nepenthes platform we and several other organizations were able to greatly broaden the empirical basis of data available about self-replicating malware and provide thousands of samples of previously unknown malware to vendors of host-based IDS/anti-virus systems. This greatly improves the detection rate of this kind of threat.
Rishi: identify bot contaminated hosts by IRC nickname evaluation	In this paper, we describe a simple, yet effective method to detect bot-infected machines within a given network that relies on detection of the communication channel between bot and Command & Control server (C&C server). The presented techniques are mainly based on passively monitoring network traffic for unusual or suspicious IRC nicknames, IRC servers, and uncommon server ports. By using n-gram analysis and a scoring system, we are able to detect bots that use uncommon communication channels, which are commonly not detected by classical intrusion detection systems. Upon detection, it is possible to determine the IP address of the C&C server, as well as, the channels a bot joined and the additional parameters which were set. The software Rishi implements the mentioned features and is able to automatically generate warning emails to report infected machines to an administrator. Within the 10 GBit network of RWTH Aachen university, we detected 82 bot-infected machines within two weeks, some of them using communication channels not picked up by other intrusion detection systems.
Toward a standard benchmark for computer security research: the worldwide intelligence network environment (WINE)	Unlike benchmarks that focus on performance or reliability evaluations, a benchmark for computer security must necessarily include sensitive code and data. Because these artifacts could damage systems or reveal personally identifiable information about the users affected by cyber attacks, publicly disseminating such a benchmark raises several scientific, ethical and legal challenges. We propose the Worldwide Intelligence Network Environment (WINE), a security-benchmarking approach based on rigorous experimental methods. WINE includes representative field data, collected worldwide from 240,000 sensors, for new empirical studies, and it will enable the validation of research on all the phases in the lifecycle of security threats. We tackle the key challenges for security benchmarking by designing a platform for repeatable experimentation on the WINE data sets and by collecting the metadata required for understanding the results. In this paper, we review the unique characteristics of the WINE data, we discuss why rigorous benchmarking will provide fresh insights on the security arms race and we propose a research agenda for this area.
MapReduce: simplified data processing on large clusters	MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.
An experimentation workbench for replayable networking research	The networked and distributed systems research communities have an increasing need for "replayable" research, but our current experimentation resources fall short of satisfying this need. Replayable activities are those that can be re-executed, either as-is or in modified form, yielding new results that can be compared to previous ones. Replayability requires complete records of experiment processes and data, of course, but it also requires facilities that allow those processes to actually be examined, repeated, modified, and reused. We are now evolving Emulab, our popular network testbed management system, to be the basis of a new experimentation workbench in support of realistic, largescale, replayable research. We have implemented a new model of testbed-based experiments that allows people to move forward and backward through their experimentation processes. Integrated tools help researchers manage their activities (both planned and unplanned), software artifacts, data, and analyses. We present the workbench, describe its implementation, and report how it has been used by early adopters. Our initial case studies highlight both the utility of the current workbench and additional usability challenges that must be addressed.
Off-Line Fair Payment Protocols Using Convertible Signatures	An exchange or payment protocol is considered fair if neither of the two parties exchanging items or payment at any time during the protocol has a significant advantage over the other entity. Fairness is an important property for electronic commerce. This paper identifies a design framework based on existing fair protocols which use offline trusted third parties, but with convertible signatures as the underlying mechanism. We show that in principle any convertible signature scheme can be used to design a fair payment protocol. A specific protocol is detailed based on RSA undeniable signatures which is more efficient than other similar fair payment schemes. Furthermore, in this protocol the final signature obtained is always an ordinary RSA signature.
Convertible Undeniable Signatures	We introduce a new concept called convertible undeniable signature schemes. In these schemes, release of a single bit string by the signer turns all of his signatures, which were originally undeniable signatures, into ordinary digital signatures. We prove that the existence of such schemes is implied by the existence of digital signature schemes. Then, looking at the problem more practically, we present a very efficient convertible undeniable signature scheme. This scheme has the added benefit that signatures can also be selectively converted.
Performance and Scalability Evaluation of 'Big Memory' on Blue Gene Linux	We address memory performance issues observed in Blue Gene Linux and discuss the design and implementation of â聙聵Big Memoryâ聙聶â聙聰â聙聰an alternative, transparent memory space introduced to eliminate the memory performance issues. We evaluate the performance of Big Memory using custom memory benchmarks, NAS Parallel Benchmarks, and the Parallel Ocean Program, at a scale of up to 4,096 nodes. We find that Big Memory successfully resolves the performance issues normally encountered in Blue Gene Linux. For the ocean simulation program, we even find that Linux with Big Memory provides better scalability than does the lightweight compute node kernel designed solely for high-performance applications. Originally intended exclusively for compute node tasks, our new memory subsystem dramatically improves the performance of certain I/O node applications as well. We demonstrate this performance using the central processor of the LOw Frequency ARray radio telescope as an example.
Blue Gene/L programming and operating environment	With up to 65,536 compute nodes and a peak performance of more than 360 teraflops, the Blue Gene®/L (BG/L) supercomputer represents a new level of massively parallel systems. The system software stack for BG/L creates a programming and operating environment that harnesses the raw power of this architecture with great effectiveness. The design and implementation of this environment followed three major principles: simplicity, performance, and familiarity. By specializing the services provided by each component of the system architecture, we were able to keep each one simple and leverage the BG/L hardware features to deliver high performance to applications. We also implemented standard programming interfaces and programming languages that greatly simplified the job of porting applications to BG/L. The effectiveness of our approach has been demonstrated by the operational success of several prototype and production machines, which have already been scaled to 16,384 nodes.
Designing a highly-scalable operating system: the Blue Gene/L story	Blue Gene/L is currently the world's fastest and most scalable supercomputer. It has demonstrated essentially linear scaling all the way to 131,072 processors in several benchmarks and real applications. The operating systems for the compute and I/O nodes of Blue Gene/L, are among the components responsible for that scalability. Compute nodes are dedicated to running application processes, whereas I/O nodes are dedicated to performing system functions. The operating systems adopted for each of these nodes reflect this separation of function. Compute nodes run a lightweight operating system called the compute node kernel. I/O nodes run a port of the Linux operating system. This paper discusses the architecture and design of this solution for Blue Gene/L in the context of the hardware characteristics that led to the design decisions. It also explains and demonstrates how those decisions are instrumental in achieving the performance and scalability for which Blue Gene/L is famous.
ZOID: I/O-forwarding infrastructure for petascale architectures	The ZeptoOS project is developing an open-source alternative to the proprietary software stacks available on contemporary massively parallel architectures. The aim is to enable computer science research on these architectures, enhance community collaboration, and foster innovation. In this paper, we introduce a component of ZeptoOS called ZOID---an I/O-forwarding infrastructure for architectures such as IBM Blue Gene that decouple file and socket I/O from the compute nodes, shipping those functions to dedicated I/O nodes. Through the use of optimized network protocols and data paths, as well as a multithreaded daemon running on I/O nodes, ZOID provides greater performance than does the stock infrastructure. We present a set of benchmark results that highlight the improvements. Crucially, the flexibility of our infrastructure is a vast improvement over the stock infrastructure, allowing users to forward data using custom-designed application interfaces, through an easy-to-use plug-in mechanism. This capability is used for real-time telescope data transfers, extensively discussed in the paper. Plug-in--specific threads implement prefetching of data obtained over sockets from an input cluster and merge results from individual compute nodes before sending them out, significantly reducing required network bandwidth. This approach allows a ZOID version of the application to handle a larger number of subbands per I/O node, or even to bypass the input cluster altogether, plugging the input from remote receiver stations directly into the I/O nodes. Using the resources more efficiently can result in considerable savings.
Cohesion and collocation: using context vectors in text segmentation	Collocational word similarity is considered a source of text cohesion that is hard to measure and quantify. The work presented here explores the use of information from a training corpus in measuring word similarity and evaluates the method in the text segmentation task. An implementation, the VecTile system, produces similarity curves over texts using pre-compiled vector representations of the contextual behavior of words. The performance of this system is shown to improve over that of the purely string-based TextTiling algorithm (Hearst, 1997).
Automatic word sense discrimination	This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.
Factors that affect software systems development project outcomes: A survey of research	Determining the factors that have an influence on software systems development and deployment project outcomes has been the focus of extensive and ongoing research for more than 30 years. We provide here a survey of the research literature that has addressed this topic in the period 1996–2006, with a particular focus on empirical analyses. On the basis of this survey we present a new classification framework that represents an abstracted and synthesized view of the types of factors that have been asserted as influencing project outcomes.
Information systems development as emergent socio-technical change: a practice approach	Many information systems development (ISD) initiatives fail to deliver the expected benefits. An important percentage of these are the result of social and organizational factors, not simply technical failures. This paper explores the dynamics of these social and organizational factors to better understand the causes of success and failure. Based on data from a detailed case analysis of an ISD project, the paper depicts the ISD process as an emergent and dynamic one, characterized by continuous local adaptations. The paper ends with a proposal of a feedback-rich framework, based on a practice view of sociotechnical change that offers theoretical insights and practical heuristics to system developers and project managers.
Dimensions of power and IS implementation	This paper analyzes the failure of an initiative of the Nevada Department of Motor Vehicles (DMV) and Public Safety. It argues that an inability to understand power relationships during systems analysis, design, and implementation has serious implication on the well being of an organization and its business processes; thus, it is a key factor in IS's project failure. The argument is conducted by evaluating various dimensions of power. Lessons are drawn that return to the importance of considering organizational power issues in implementing a successful computer-based system and in realizing an information technology enabled strategic change initiative.
Negotiating ICT development and use: The case of a telemedicine system in the healthcare region of Crete	Recent research on the development and use of information and communication technology (ICT) has focused on the emergent use of technology in practice and the multiplicity of outcomes being simultaneously negotiated by different groups and individuals. In this paper, we seek to understand this emergent process by examining the interrelationship between the context(s) in which ICTs are introduced, the ways in which ICTs are enacted in practice, and the role of different technological artifacts. We pursue the value of these conceptual developments in an interpretive case study on the introduction of a telemedicine system in the healthcare region of Crete, Greece. Some key implications arising from the case study refer to the relationship between power relationships and organizational change; the relationship between existing work practices and resistance to ICT-mediated change; and the role of different artifacts in negotiations of power, as well as in processes of community formation.
A social action model of situated information systems design	The aim of this paper is to examine the nature of information systems (IS) design as situated in its organizational context. Much of the IS literature takes a fragmented perspective of the nature of IS design, examining methodological issues, social issues, or political issues in isolation from the context of the design initiative. Recent work in situated action and social cognition provides a basis for a more integrated understanding of situated IS design.Findings from a participant observation study of situated IS design are discussed, to form the basis for an integrative, social action model of IS design. Findings from the study demonstrate how innovative IS design activity is situated in its organizational context. It is argued that the form and nature of an organizational design "problem" is inseparable from its context and that design knowledge is distributed among a design team rather than shared intersubjectively. The situated nature of design requires design skills to be learned through simulated design contexts, rather than the communication of abstract models (as in many formal education programs). The situated model rejects the predefined goal-structures assumed by decompositional models of design, such as the "waterfall" model. It is suggested that design goal-definition must proceed recursively through the processes of design, which requires new approaches to the design and development of organizational information systems.
Implementation and performance evaluation of nanoMAC: a low-power MAC solution for high density wireless sensor networks	This paper describes the implementation architecture and performance analysis of nanoMAC, a CSMA/CA based Medium Access Control (MAC) protocol, which is specifically designed for high density Wireless Sensor Networks (WSNs). We empirically show that nanoMAC performs with high reliability in a variety of network traffic conditions in single and multihop scenarios. For energy efficient operation and minimising idle-overhearing, nanoMAC uses a specialised sleep algorithm. We also show results from a comparative study of nanoMAC with Berkeley's MAC (B-MAC) in terms of performance measures, as well as their coexistence in networks deployed in overlapping or nearby areas.
Energy-efficient collision-free medium access control for wireless sensor networks	The traffic-adaptive medium access protocol (TRAMA) is introduced for energy-efficient collision-free channel access in wireless sensor networks. TRAMA reduces energy consumption by ensuring that unicast, multicast, and broadcast transmissions have no collisions, and by allowing nodes to switch to a low-power, idle state whenever they are not transmitting or receiving. TRAMA assumes that time is slotted and uses a distributed election scheme based on information about the traffic at each node to determine which node can transmit at a particular time slot. TRAMA avoids the assignment of time slots to nodes with no traffic to send, and also allows nodes to determine when they can become idle and not listen to the channel using traffic information. TRAMA is shown to be fair and correct, in that no idle node is an intended receiver and no receiver suffers collisions. The performance of TRAMA is evaluated through extensive simulations using both synthetic- as well as sensor-network scenarios. The results indicate that TRAMA outperforms contention-based protocols (e.g., CSMA, 802.11 and S-MAC) as well as scheduling-based protocols (e.g., NAMA) with significant energy savings.
Medium access control with coordinated adaptive sleeping for wireless sensor networks	This paper proposes S-MAC, a medium access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with nodes remaining largely inactive for long time, but becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in several ways: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses a few novel techniques to reduce energy consumption and support self-configuration. It enables low-duty-cycle operation in a multihop network. Nodes form virtual clusters based on common sleep schedules to reduce control overhead and enable traffic-adaptive wake-up. S-MAC uses in-channel signaling to avoid overhearing unnecessary traffic. Finally, S-MAC applies message passing to reduce contention latency for applications that require in-network data processing. The paper presents measurement results of S-MAC performance on a sample sensor node, the UC Berkeley Mote, and reveals fundamental tradeoffs on energy, latency and throughput. Results show that S-MAC obtains significant energy savings compared with an 802.11-like MAC without sleeping.
A survey of hard real-time scheduling for multiprocessor systems	This survey covers hard real-time scheduling algorithms and schedulability analysis techniques for homogeneous multiprocessor systems. It reviews the key results in this field from its origins in the late 1960s to the latest research published in late 2009. The survey outlines fundamental results about multiprocessor real-time scheduling that hold independent of the scheduling algorithms employed. It provides a taxonomy of the different scheduling methods, and considers the various performance metrics that can be used for comparison purposes. A detailed review is provided covering partitioned, global, and hybrid scheduling algorithms, approaches to resource sharing, and the latest results from empirical investigations. The survey identifies open issues, key research challenges, and likely productive research directions.
Real Time Scheduling Theory: A Historical Perspective	In this 25th year anniversary paper for the IEEE Real Time Systems Symposium, we review the key results in real-time scheduling theory and the historical events that led to the establishment of the current real-time computing infrastructure. We conclude this paper by looking at the challenges ahead of us.
Scalably scheduling processes with arbitrary speedup curves	We give a scalable ((1+ε)-speed O(1)-competitive) non-clairvoyant algorithm for scheduling jobs with sublinear nondecreasing speed-up curves on multiple processors with the objective of average response time.
Ambient intelligence: A survey	In this article we survey ambient intelligence (AmI), including its applications, some of the technologies it uses, and its social and ethical implications. The applications include AmI at home, care of the elderly, healthcare, commerce, and business, recommender systems, museums and tourist scenarios, and group decision making. Among technologies, we focus on ambient data management and artificial intelligence; for example planning, learning, event-condition-action rules, temporal reasoning, and agent-oriented technologies. The survey is not intended to be exhaustive, but to convey a broad range of applications, technologies, and technical, social, and ethical challenges.
Easishop: Ambient intelligence assists everyday shopping	E-commerce has been one of the success stories of the last decade. Developments in wireless communications and mobile computing have heralded an era of mobile commerce (m-commerce). Though a number of successful applications and services have been deployed, these are almost invariably inherently static in nature. By augmenting m-commerce with intelligent and autonomous components, the significant benefits of convenience and added value may be realized for the average shopper as they wander their local shopping mall or high street. By a selective and judicious juxtaposition of Ambient Intelligent (AmI) concepts and e-commerce precepts, the foundations for truly ubiquitous commerce (u-commerce) may be constructed. In this paper, the synergy between AmI and e-commerce is explored and developed, resulting in the description of a prototypical application that demonstrates the viability of this approach.
A context-aware preference model for database querying in an ambient intelligent environment	Users' preferences have traditionally been exploited in query personalization to better serve their information needs. With the emerging ubiquitous computing technologies, users will be situated in an Ambient Intelligent (AmI) environment, where users' database access will not occur at a single location in a single context as in the traditional stationary desktop computing, but rather span a multitude of contexts like office, home, hotel, plane, etc. To deliver personalized query answering in this environment, the need for context-aware query preferences arises accordingly. In this paper, we propose a knowledge-based context-aware query preference model, which can cater for both pull and push queries. We analyze requirements and challenges that AmI poses upon such a model and discuss the interpretation of the model in the domain of relational databases. We implant the model on top of a traditional DBMS to demonstrate the applicability and feasibility of our approach.
PEIS ecologies: ambient intelligence meets autonomous robotics	A common vision in the field of autonomous robotics is to create a skilled robot companion that is able to live in our homes and perform physical tasks to help us in our everyday life. Another vision, coming from the field of ambient intelligence, is to create a network of intelligent home devices that provide us with information, communication, and entertainment. We propose to combine these two visions into the new concept of an ecology of networked Physically Embedded Intelligent Systems (PEIS). In this paper, we define this concept, and illustrate it by describing an experimental system that involves real robotic devices.
Perspectives of ambient intelligence in the home environment	Ambient Intelligence is a vision of the future information society stemming from the convergence of ubiquitous computing, ubiquitous communication and intelligent user-friendly interfaces. It offers an opportunity to realise an old dream, i.e. the smart or intelligent home. Will it fulfil the promises or is it just an illusion--offering apparently easy living while actually increasing the complexity of life? This article touches upon this question by discussing the technologies, applications and social implications of ambient intelligence in the home environment. It explores how Ambient Intelligence may change our way of life. It concludes that there are great opportunities for Ambient Intelligence to support social developments and modern lifestyles. However, in order to gain wide acceptance a delicate balance is needed: the technology should enhance the quality of life but not be seeking domination. It should be reliable and controllable but nevertheless adaptive to human habits and changing contexts.
Agent-based ambient intelligence for healthcare	Healthcare professionals, working in hospitals experience a high level of mobility due to their need for accessing patients' clinical records, medical devices distributed throughout the premises, and colleagues with whom they collaborate. In this paper, we present how autonomous agents provide capabilities of intelligence and proactivity to healthcare environments furnished with ubiquitous computing and medical devices, resulting thus, in an ambient intelligence (AmI) system. Autonomous agents enable ubiquitous technology to respond to users' particular conditions and demands. To support the building of this type of intelligent systems, we created the SALSA development framework. SALSA is a middleware designed to support the development of AmI environments based on autonomous agents. We illustrate the facilities provided by SALSA and its flexibility to iteratively implement an AmI system for a healthcare application scenario.
Plans and planning in smart homes	In this chapter, we review the use (and uses) of plans and planning in Smart Homes. Plans have several applications within Smart Homes, including: sharing task execution with the home's inhabitants, providing task guidance to inhabitants, and to identifying emergencies. These plans are not necessarily generated automatically, nor are they always represented in a human-readable form. The chapter ends with a discussion of the research issues surrounding the integration of plans and planning into Smart Homes.
Group dynamics and ubiquitous computing	From "Here and Now" to "Everywhere and Forever."
AmbienNet: an intelligent environment to support people with disabilities and elderly people	AmbienNet is an ongoing project aiming to demonstrate the viability of accessible intelligent environments to support people with disabilities and elderly people living autonomously. Based on the Ambient Intelligence paradigm, it tries to study in depth its advantages and disadvantages for people with sensory, physical or cognitive restrictions. To this end diverse supporting technologies and applications have been designed, in order to test their accessibility, usability and validity. After introducing the objectives and findings of the project, in this paper a number of preliminary results are presented and discussed.
The ciff proof procedure for abductive logic programming with constraints: Theory, implementation and experiments	We present the CIFF proof procedure for abductive logic programming with constraints, and we prove its correctness. CIFF is an extension of the IFF proof procedure for abductive logic programming, relaxing the original restrictions over variable quantification (allowedness conditions) and incorporating a constraint solver to deal with numerical constraints as in constraint logic programming. Finally, we describe the CIFF system, comparing it with state-of-the-art abductive systems and answer set solvers and showing how to use it to program some applications.
DKMS: distributed hierarchical access control for multimedia networks	Hierarchical access control for group communication providesaccess control which can assure that group members can subscribedifferent data streams or possibly multiples of them in amultimedia network. In this paper, to efficiently and effectivelyachieve this goal, we propose a Distributed Key Management Scheme(DKMS) whereby each Service Group (SG) maintains a SG server andgive detailed analysis. In the proposed scheme, the server for a SGis utilised to manage the key tree and provide the related sessionkeys for all the users in this SG. Detailed analysis is providedand we show that the communication overhead can be greatly reducedby O(n0 logd n0), wheren0 is the number of users in a SG, compared withemploying an integrated key graph to the hierarchical accesscontrol problem. At the same time, the storage overhead for allusers can be reduced by O(N), where N is the total number ofusers.
Adaptive clock synchronization in sensor networks	Recent advances in technology have made low cost, low power wireless sensors a reality. Clock synchronization is an important service in any distributed system, including sensor network systems. Applications of clock synchronization in sensor networks include data integration in sensors, sensor reading fusion, TDMA medium access scheduling, and power mode energy saving. However, for a number of reasons, standard clock synchronization protocols are unsuitable for direct application in sensor networks. In this paper, we introduce the concept of adaptive clock synchronization based on the need of the application and the resource constraint in the sensor networks. We describe a probabilistic method for clock synchronization that uses the higher precision of receiver-to-receiver synchronization, as described in Reference Broadcast Synchronization (RBS) protocol. This deterministic protocol is extended to provide a probabilistic bound on the accuracy of the clock synchronization, allowing for a tradeo between accuracy and resource requirement. Expressions to convert service specifications (maximum clock synchronization error and confidence probability) to actual protocol parameters (minimum number of messages and synchronization overhead) are derived. Further, we extend this protocol for maintaining clock synchronization in a multihop network.
Iolus: a framework for scalable secure multicasting	As multicast applications are deployed for mainstream use, the need to secure multicast communications will become critical. Multicast, however, does not fit the point-to-point model of most network security protocols which were designed with unicast communications in mind. As we will show, securing multicast (or group) communications is fundamentally different from securing unicast (or paired) communications. In turn, these differences can result in scalability problems for many typical applications.In this paper, we examine and model the differences between unicast and multicast security and then propose Iolus: a novel framework for scalable secure multicasting. Protocols based on Iolus can be used to achieve a variety of security objectives and may be used either to directly secure multicast communications or to provide a separate group key management service to other "security-aware" applications. We describe the architecture and operation of Iolus in detail and also describe our experience with a protocol based on the Iolus framework.
A survey of key management for secure group communication	Group communication can benefit from IP multicast to achieve scalable exchange of messages. However, there is a challenge of effectively controlling access to the transmitted data. IP multicast by itself does not provide any mechanisms for preventing nongroup members to have access to the group communication. Although encryption can be used to protect messages exchanged among group members, distributing the cryptographic keys becomes an issue. Researchers have proposed several different approaches to group key management. These approaches can be divided into three main classes: centralized group key management protocols, decentralized architectures and distributed key management protocols. The three classes are described here and an insight given to their features and goals. The area of group key management is then surveyed and proposed solutions are classified according to those characteristics.
Timing-sync protocol for sensor networks	Wireless ad-hoc sensor networks have emerged as an interesting and important research area in the last few years. The applications envisioned for such networks require collaborative execution of a distributed task amongst a large set of sensor nodes. This is realized by exchanging messages that are time-stamped using the local clocks on the nodes. Therefore, time synchronization becomes an indispensable piece of infrastructure in such systems. For years, protocols such as NTP have kept the clocks of networked systems in perfect synchrony. However, this new class of networks has a large density of nodes and very limited energy resource at every node; this leads to scalability requirements while limiting the resources that can be used to achieve them. A new approach to time synchronization is needed for sensor networks.In this paper, we present Timing-sync Protocol for Sensor Networks (TPSN) that aims at providing network-wide time synchronization in a sensor network. The algorithm works in two steps. In the first step, a hierarchical structure is established in the network and then a pair wise synchronization is performed along the edges of this structure to establish a global timescale throughout the network. Eventually all nodes in the network synchronize their clocks to a reference node. We implement our algorithm on Berkeley motes and show that it can synchronize a pair of neighboring motes to an average accuracy of less than 20ms. We argue that TPSN roughly gives a 2x better performance as compared to Reference Broadcast Synchronization (RBS) and verify this by implementing RBS on motes. We also show the performance of TPSN over small multihop networks of motes and use simulations to verify its accuracy over large-scale networks. We show that the synchronization accuracy does not degrade significantly with the increase in number of nodes being deployed, making TPSN completely scalable.
RECA: a ring-structured energy-efficient clustering architecture for robust communication in wireless sensor networks	Clustering methods have shown their promising effects in the conservation of energy in Wireless Sensor Networks (WSNs). In this paper, we present a Ring-structured Energy-efficient Clustering Architecture (RECA) for WSNs to prolong the network lifetime. RECA uses deterministic cluster-head management algorithm to evenly distribute the work load among the nodes within a cluster. Nodes within a cluster make local decisions on the fair-share length of their duty cycle according to their remaining energy supply and those of the rest of the nodes within the same cluster. This guarantees that all nodes deplete their energy supply at approximately the same time regardless of the initial amount of energy in their battery. Simulation study has been used to evaluate the performance of RECA. The results show that RECA is efficient in managing energy in a wide range of networks settings. The results also show the importance of even energy dissipation with regard to network lifetime. Finally, we propose mechanisms to improve the robustness of RECA and ensure that there is an upper bound on the period of time during which a cluster remains without a cluster-head.
Geography-informed energy conservation for Ad Hoc routing	We introduce a geographical adaptive fidelity (GAF) algorithm that reduces energy consumption in ad hoc wireless networks. GAF conserves energy by identifying nodes that are equivalent from a routing perspective and then turning off unnecessary nodes, keeping a constant level of routing fidelity. GAF moderates this policy using application- and system-level information; nodes that source or sink data remain on and intermediate nodes monitor and balance energy use. GAF is independent of the underlying ad hoc routing protocol; we simulate GAF over unmodified AODV and DSR. Analysis and simulation studies of GAF show that it can consume 40% to 60% less energy than an unmodified ad hoc routing protocol. Moreover, simulations of GAP suggest that network lifetime increases proportionally to node density; in one example, a four-fold increase in node density leads to network lifetime increase for 3 to 6 times (depending on the mobility pattern). More generally, GAF is an example of adaptive fidelity, a technique proposed for extending the lifetime of self-configuring systems by exploiting redundancy to conserve energy while maintaining application fidelity.
Some problems of directional sensor networks	Wireless sensor networks are often based on omni-sensing and communication models. In contrast, in this paper, we investigate sensor networks with directional sensing and communication capability. Due to the distinct characteristics and potential effects on coverage and connectivity of the network, novel analysis and solutions are demanded. Towards this end, this paper analyses deployment strategies for satisfying given coverage probability requirements with directional sensing models. Moreover, for sensors with directional communication model, we propose methods for checking and repairing the connectivity of the network. We design efficient protocols to implement our idea. A set of experiments are also performed to prove the effectiveness of our solution.
Integrated coverage and connectivity configuration in wireless sensor networks	An effective approach for energy conservation in wireless sensor networks is scheduling sleep intervals for extraneous nodes, while the remaining nodes stay active to provide continuous service. For the sensor network to operate successfully, the active nodes must maintain both sensing coverage and network connectivity. Furthermore, the network must be able to configure itself to any feasible degrees of coverage and connectivity in order to support different applications and environments with diverse requirements. This paper presents the design and analysis of novel protocols that can dynamically configure a network to achieve guaranteed degrees of coverage and connectivity. This work differs from existing connectivity or coverage maintenance protocols in several key ways: 1) We present a Coverage Configuration Protocol (CCP) that can provide different degrees of coverage requested by applications. This flexibility allows the network to self-configure for a wide range of applications and (possibly dynamic) environments. 2) We provide a geometric analysis of the relationship between coverage and connectivity. This analysis yields key insights for treating coverage and connectivity in a unified framework: this is in sharp contrast to several existing approaches that address the two problems in isolation. 3) Finally, we integrate CCP with SPAN to provide both coverage and connectivity guarantees. We demonstrate the capability of our protocols to provide guaranteed coverage and connectivity configurations, through both geometric analysis and extensive simulations.
On k-coverage in a mostly sleeping sensor network	Sensor networks are often desired to last many times longer than the active lifetime of individual sensors. This is usually achieved by putting sensors to sleep for most of their lifetime. On the other hand, surveillance kind of applications require guaranteed k-coverage of the protected region at all times. As a result, determining the appropriate number of sensors to deploy that achieves both goals simultaneously becomes a challenging problem. In this paper, we consider three kinds of deployments for a sensor network on a unit square - a √n x √n grid, random uniform (for all n points), and Poisson (with density n). In all three deployments, each sensor is active with probability p, independently from the others. Then, we claim that the critical value of the function npπr2/log(np) is 1 for the event of k-coverage of every point. We also provide an upper bound on the window of this phase transition. Although the conditions for the three deployments are similar, we obtain sharper bounds for the random deployments than the grid deployment, which occurs due to the boundary condition. In this paper, we also provide corrections to previously published results for the grid deployment model. Finally, we use simulation to show the usefulness of our analysis in real deployment scenarios.
The coverage problem in a wireless sensor network	One fundamental issue in sensor networks is the coverage problem, which reflects how well a sensor network is monitored or tracked by sensors. In this paper, we formulate this problem as a decision problem, whose goal is to determine whether every point in the service area of the sensor network is covered by at least k sensors, where k is a predefined value. The sensing ranges of sensors can be unit disks or non-unit disks. We present polynomial-time algorithms, in terms of the number of sensors, that can be easily translated to distributed protocols. The result is a generalization of some earlier results where only k=1 is assumed. Applications of the result include: (i) positioning applications, (ii) situations which require stronger environmental monitoring capability, and (iii) scenarios which impose more stringent fault-tolerant capability.
Panoptes: scalable low-power video sensor networking technologies	Video-based sensor networks can provide important visual information in a number of applications including: environmental monitoring, health care, emergency response, and video security. This paper describes the Panoptes video-based sensor networking architecture, including its design, implementation, and performance. We describe a video sensor platform that can deliver high-quality video over 802.11 networks with a power requirement of approximately 5 watts. In addition, we describe the streaming and prioritization mechanisms that we have designed to allow it to survive long-periods of disconnected operation. Finally, we describe a sample application and bitmapping algorithm that we have implemented to show the usefulness of our platform. Our experiments include an in-depth analysis of the bottlenecks within the system as well as power measurements for the various components of the system.
Area-based beaconless reliable broadcasting in sensor networks	We consider the broadcasting problem in sensor networks where the nodes have no prior knowledge of their neighbourhood. We describe several Area-based Beaconless Broadcasting Algorithms (ABBAs). In 2D, on receiving the packet (together with geographic coordinates of the sender), each node calculates the ratio P of its perimeter, along the circle of transmission radius, that is not covered by this and previous transmissions of the same packet. The node then sets or updates its timeout to be inversely proportional to P. If the perimeter becomes fully covered, the node cancels retransmissions. Otherwise, it retransmits at the end of the timeout interval. The protocol is reliable, that is, all nodes, connected to the source, are guaranteed to receive the packet, assuming an ideal MAC layer. We also describe three 3D-ABBAs, one of them being reliable. These three protocols are based on covering three projections, covering particular points on intersection circles and covering intersection points of three spheres. Our protocols are the first reliable broadcasting protocols, other than blind flooding.
Dominating Sets and Neighbor Elimination-Based Broadcasting Algorithms in Wireless Networks	In a multihop wireless network, each node has a transmission radius and is able to send a message to all of its neighbors that are located within the radius. In a broadcasting task, a source node sends the same message to all the nodes in the network. In this paper, we propose to significantly reduce or eliminate the communication overhead of a broadcasting task by applying the concept of localized dominating sets. Their maintenance does not require any communication overhead in addition to maintaining positions of neighboring nodes. Retransmissions by only internal nodes in a dominating set is sufficient for reliable broadcasting. Existing dominating sets are improved by using node degrees instead of their ids as primary keys. We also propose to eliminate neighbors that already received the message and rebroadcast only if the list of neighbors that might need the message is nonempty. A retransmission after negative acknowledgements scheme is also described. The important features of proposed algorithms are their reliability (reaching all nodes in the absence of message collisions), significant rebroadcast savings, and their localized and parameterless behavior. The reduction in communication overhead for broadcasting task is measured experimentally. Dominating sets based broadcasting, enhanced by neighbor elimination scheme and highest degree key, provides reliable broadcast with \leq 53 percent of node retransmissions (on random unit graphs with 100 nodes) for all average degrees d. Critical d is around 4, with \leq 48 percent for \leq 3,\leq 40 percent for d\geq 10, and \leq 20 percent for d\geq 25. The proposed methods are better than existing ones in all considered aspects: reliability, rebroadcast savings, and maintenance communication overhead. In particular, the cluster structure is inefficient for broadcasting because of considerable communication overhead for maintaining the structure and is also inferior in terms of rebroadcast savings.
BLR: beacon-less routing algorithm for mobile ad hoc networks	Routing of packets in mobile ad hoc networks with a large number of nodes or with high mobility is a very difficult task and current routing protocols do not really scale well with these scenarios. The beacon-less routing algorithm (BLR) presented in this paper is a routing protocol that makes use of location information to reduce routing overhead. However, unlike other position-based routing protocols, BLR does not require nodes to periodically broadcast Hello-messages (called beaconing), and thus avoids drawbacks such as extensive use of scarce battery-power, interferences with regular data transmission, and performance degradation. BLR selects a forwarding node in a distributed manner among all its neighboring nodes with having information neither about their positions nor even about their existence. Data packets are broadcasted and the protocol takes care that just one of the receiving nodes forwards the packet. Optimized forwarding is achieved by applying a concept of dynamic forwarding delay. Consequently, the node that computes the shortest forwarding delay relays the packet first. This forwarding is detected by the other nodes and suppresses them to relay the same packet any further. Analytical results and simulation experiments indicate that BLR provides efficient and robust routing in highly dynamic mobile ad hoc networks.
Optimized Broadcast Protocol for Sensor Networks	Sensor networks usually operate under very severe energy restrictions. Therefore, sensor communications should consume the minimum possible amount of energy. While broadcasting is a very energy-expensive protocol, it is also widely used as a building block for a variety of other network layer protocols. Therefore, reducing the energy consumption by optimizing broadcasting is a major improvement in sensor networking. In this paper, we propose an optimized Broadcast Protocol for Sensor networks (BPS). The major novelty of BPS is its adaptive-geometric approach that enables considerable reduction of retransmissions by maximizing each hop length. BPS adapts itself and gets the best out of existing radio conditions. In BPS, nodes do not need any neighborhood information, which leads to low communication and memory overhead. We analyze the worst-case scenario for BPS and show that the number of transmissions in such a scenario is a constant multiple of those required in the ideal case. Our simulation results show that BPS is very scalable with respect to network density. BPS is also resilient to transmission errors.
Improving network lifetime using sensors with adjustable sensing ranges	This paper addresses the target coverage problem in wireless sensor networks with adjustable sensing range. Communication and sensing consume energy, therefore efficient power management can extend network lifetime. In this paper, we consider a large number of sensors with adjustable sensing range that are randomly deployed to monitor a number of targets. Since targets are redundantly covered by multiple sensors, to conserve energy resources, sensors can be organised in sets, activated successively. In this paper, we address the Adjustable Range Set Covers (AR-SC) problem that has as its objective finding a maximum number of set covers and the ranges associated with each sensor, such that each sensor set covers all the targets. A sensor can participate in multiple sensor sets, but the sum of the energy spent in each set is constrained by the initial energy resources. In this paper, we mathematically model solutions to this problem and design heuristics that efficiently compute the sets. Simulation results are presented to verify our approaches.
Integrated coverage and connectivity configuration in wireless sensor networks	An effective approach for energy conservation in wireless sensor networks is scheduling sleep intervals for extraneous nodes, while the remaining nodes stay active to provide continuous service. For the sensor network to operate successfully, the active nodes must maintain both sensing coverage and network connectivity. Furthermore, the network must be able to configure itself to any feasible degrees of coverage and connectivity in order to support different applications and environments with diverse requirements. This paper presents the design and analysis of novel protocols that can dynamically configure a network to achieve guaranteed degrees of coverage and connectivity. This work differs from existing connectivity or coverage maintenance protocols in several key ways: 1) We present a Coverage Configuration Protocol (CCP) that can provide different degrees of coverage requested by applications. This flexibility allows the network to self-configure for a wide range of applications and (possibly dynamic) environments. 2) We provide a geometric analysis of the relationship between coverage and connectivity. This analysis yields key insights for treating coverage and connectivity in a unified framework: this is in sharp contrast to several existing approaches that address the two problems in isolation. 3) Finally, we integrate CCP with SPAN to provide both coverage and connectivity guarantees. We demonstrate the capability of our protocols to provide guaranteed coverage and connectivity configurations, through both geometric analysis and extensive simulations.
The coverage problem in a wireless sensor network	One fundamental issue in sensor networks is the coverage problem, which reflects how well a sensor network is monitored or tracked by sensors. In this paper, we formulate this problem as a decision problem, whose goal is to determine whether every point in the service area of the sensor network is covered by at least k sensors, where k is a predefined value. The sensing ranges of sensors can be unit disks or non-unit disks. We present polynomial-time algorithms, in terms of the number of sensors, that can be easily translated to distributed protocols. The result is a generalization of some earlier results where only k=1 is assumed. Applications of the result include: (i) positioning applications, (ii) situations which require stronger environmental monitoring capability, and (iii) scenarios which impose more stringent fault-tolerant capability.
Efficient anonymity schemes for clustered wireless sensor networks	In this paper, we propose two simple and efficient schemes for establishing anonymity in Clustered Wireless Sensor Networks (CWSNs). The first scheme Simple Anonymity Scheme (SAS), uses a range of pseudonyms as identifiers for a node to ensure concealment of its true identifier (ID). After deployment, neighbouring nodes share their individual pseudonyms and use them for anonymous communication. The second scheme Cryptographic Anonymity Scheme (CAS), uses a Keyed Cryptographic one way Hash Function (KCHF) for ID concealment. In CAS, neighbouring nodes securely share the information used by the KCHF to generate pseudonyms. Even when many nodes in a neighbourhood are compromised and are colluding, our schemes guarantee complete anonymity to non-compromised nodes during their mutual communication. Our schemes have reasonably low memory and computation costs. They can be embedded into any wireless sensor network routing protocol to ensure anonymity and privacy during node discovery, routing and data delivery.
Establishing pairwise keys in distributed sensor networks	Pairwise key establishment is a fundamental security service in sensor networks; it enables sensor nodes to communicate securely with each other using cryptographic techniques. However, due to the resource constraints on sensors, it is infeasible to use traditional key management techniques such as public key cryptography and key distribution center (KDC). To facilitate the study of novel pairwise key predistribution techniques, this paper presents a general framework for establishing pairwise keys between sensors on the basis of a polynomial-based key predistribution protocol [2]. This paper then presents two efficient instantiations of the general framework: a random subset assignment key predistribution scheme and a grid-based key predistribution scheme. The analysis in this paper indicates that these two schemes have a number of nice properties, including high probability (or guarantee) to establish pairwise keys, tolerance of node captures, and low communication overhead. Finally, this paper presents a technique to reduce the computation at sensors required by these schemes.
AO2P: Ad Hoc On-Demand Position-Based Private Routing Protocol	Privacy is needed in ad hoc networks. An ad hoc on-demand position-based private routing algorithm, called AO2P, is proposed for communication anonymity. Only the position of the destination is exposed in the network for route discovery. To discover routes with the limited routing information, a receiver contention scheme is designed for determining the next hop. Pseudo identifiers are used for data packet delivery after a route is established. Real identities (IDs) for the source nodes, the destination nodes, and the forwarding nodes in the end-to-end connections are kept private. Anonymity for a destination relies on the difficulty of matching a geographic position to a real node ID. This can be enforced by the use of secure position service systems. Node mobility enhances destination anonymity by making the match of a node ID with a position momentary. To further improve destination privacy, R-AO2P is proposed. In this protocol, the position of a reference point, instead of the position of the destination, is used for route discovery. Analytical models are developed for evaluating the delay in route discovery and the probability of route discovery failure. A simulator based on ns{\hbox{-}}2 is developed for evaluating network throughput. Analysis and simulation results show that, while AO2P preserves communication privacy in ad hoc networks, its routing performance is comparable with other position-based routing algorithms.
Wireless sensor networks: a survey	This paper describes the concept of sensor networks which has been made viable by the convergence of micro-electro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are also discussed.
ANODR: anonymous on demand routing with untraceable routes for mobile ad-hoc networks	In hostile environments, the enemy can launch traffic analysis against interceptable routing information embedded in routing messages and data packets. Allowing adversaries to trace network routes and infer the motion pattern of nodes at the end of those routes may pose a serious threat to covert operations. We propose ANODR, an anonymous on-demand routing protocol for mobile ad hoc networks deployed in hostile environments. We address two closely related problems: For route anonymity, ANODR prevents strong adversaries from tracing a packet flow back to its source or destination; for location privacy, ANODR ensures that adversaries cannot discover the real identities of local transmitters. The design of ANODR is based on "broadcast with trapdoor information", a novel network security concept which includes features of two existing network and security mechanisms, namely "broadcast" and "trapdoor information". We use simulations and implementation to validate the effectiveness of our design.
WCA: A Weighted Clustering Algorithm for Mobile Ad Hoc Networks	In this paper, we propose an on-demand distributed clustering algorithm for multi-hop packet radio networks. These types of networks, also known as iad hoc networks, are dynamic in nature due to the mobility of nodes. The association and dissociation of nodes to and from iclusters perturb the stability of the network topology, and hence a reconfiguration of the system is often unavoidable. However, it is vital to keep the topology stable as long as possible. The iclusterheads, form a idominant set in the network, determine the topology and its stability. The proposed weight-based distributed clustering algorithm takes into consideration the ideal degree, transmission power, mobility, and battery power of mobile nodes. The time required to identify the clusterheads depends on the diameter of the underlying graph. We try to keep the number of nodes in a cluster around a pre-defined threshold to facilitate the optimal operation of the medium access control (MAC) protocol. The non-periodic procedure for clusterhead election is invoked on-demand, and is aimed to reduce the computation and communication costs. The clusterheads, operating in “dual" power mode, connects the clusters which help in routing messages from a node to any other node. We observe a trade-off between the uniformity of the load handled by the clusterheads and the connectivity of the network. Simulation experiments are conducted to evaluate the performance of our algorithm in terms of the number of clusterheads, ireaffiliation frequency, and dominant set updates. Results show that our algorithm performs better than existing ones and is also tunable to different kinds of network conditions.
Privacy risks emerging from the adoption of innocuous wearable sensors in the mobile environment	Wearable sensors are revolutionizing healthcare and science by enabling capture of physiological, psychological, and behavioral measurements in natural environments. However, these seemingly innocuous measurements can be used to infer potentially private behaviors such as stress, conversation, smoking, drinking, illicit drug usage, and others. We conducted a study to assess how concerned people are about disclosure of a variety of behaviors and contexts that are embedded in wearable sensor data. Our results show participants are most concerned about disclosures of conversation episodes and stress - inferences that are not yet widely publicized. These concerns are mediated by temporal and physical context associated with the data and the participant's personal stake in the data. Our results provide key guidance on the extent to which people understand the potential for harm and data characteristics researchers should focus on to reduce the perceived harm from such datasets.
Exploring end user preferences for location obfuscation, location-based services, and the value of location	Long-term personal GPS data is useful for many UbiComp services such as traffic monitoring and environmental impact assessment. However, inference attacks on such traces can reveal private information including home addresses and schedules. We asked 32 participants from 12 households to collect 2 months of GPS data, and showed it to them in visualizations. We explored if they understood how their individual privacy concerns mapped onto 5 location obfuscation schemes (which they largely did), which obfuscation schemes they were most comfortable with (Mixing, Deleting data near home, and Randomizing), how they monetarily valued their location data, and if they consented to share their data publicly. 21/32 gave consent to publish their data, though most households' members shared at different levels, which indicates a lack of awareness of privacy interrelationships. Grounded in real decisions about real data, our findings highlight the potential for end-user involvement in obfuscation of their own location data.
Who gets to know what when: configuring privacy permissions in an awareness application	We report on a study (N=36) of user preferences for balancing awareness with privacy. Participants defined permissions for sharing of location, availability, calendar information and instant messaging (IM) activity within an application called mySpace. MySpace is an interactive visualization of the physical workplace that provides dynamic information about people, places and equipment. We found a significant preference for defining privacy permissions at the group level. While "family" received high levels of awareness sharing, interestingly, "team" was granted comparable levels during business hours at work. Surprisingly, presenting participants with a detailed list of all pieces of personal context to which the system had access, did not result in more conservative privacy settings. Although location was the most sensitive aspect of awareness, participants were comfortable disclosing room-level location information to their team members at work. Our findings suggest utilizing grouping mechanisms to balance privacy control with configuration burden, and argue for increased system transparency to build trust.
From spaces to places: emerging contexts in mobile privacy	Mobile privacy concerns are central to Ubicomp and yet remain poorly understood. We advocate a diversified approach, enabling the cross-interpretation of data from complementary methods. However, mobility imposes a number of limitations on the methods that can be effectively employed. We discuss how we addressed this problem in an empirical study of mobile social networking. We report on how, by combining a variation of experience sampling and contextual interviews, we have started focusing on a notion of context in relation to privacy, which is subjectively defined by emerging socio-cultural knowledge, functions, relations and rules. With reference to Gieryn's sociological work, we call this place, as opposed to a notion of context that is objectively defined by physical and factual elements, which we call space. We propose that the former better describes the context for mobile privacy.
The Wi-Fi privacy ticker: improving awareness & control of personal information exposure on Wi-Fi	Anyone within range of an 802.11 wireless network ("Wi-Fi") can use free software to collect the unencrypted web traffic of others on the network. However, many Wi-Fi users are completely unaware of the risk that this creates. This work aims to improve users' awareness about what they expose to others on Wi-Fi networks and provide them with some control. Our system, the Wi-Fi Privacy Ticker, displays information about the exposure of sensitive terms that are sent to and from a user's computer and prevents the unencrypted transmission of terms from the user's computer that she has identified as highly sensitive. In a three-week field study with 17 participants, we found that the Wi-Fi Privacy Ticker improved participants' awareness of the circumstances in which their personal information is transmitted. We show that this heightened awareness contributed to changes in their behavior while on Wi-Fi.
Mercury: a wearable sensor network platform for high-fidelity motion analysis	This paper describes Mercury, a wearable, wireless sensor platform for motion analysis of patients being treated for neuromotor disorders, such as Parkinson's Disease, epilepsy, and stroke. In contrast to previous systems intended for short-term use in a laboratory, Mercury is designed to support long-term, longitudinal data collection on patients in hospital and home settings. Patients wear up to 8 wireless nodes equipped with sensors for monitoring movement and physiological conditions. Individual nodes compute high-level features from the raw signals, and a base station performs data collection and tunes sensor node parameters based on energy availability, radio link quality, and application specific policies. Mercury is designed to overcome the core challenges of long battery lifetime and high data fidelity for long-term studies where patients wear sensors continuously 12 to 18 hours a day. This requires tuning sensor operation and data transfers based on energy consumption of each node and processing data under severe computational constraints. Mercury provides a high-level programming interface that allows a clinical researcher to rapidly build up different policies for driving data collection and tuning sensor lifetime. We present the Mercury architecture and a detailed evaluation of two applications of the system for monitoring patients with Parkinson's Disease and epilepsy.
Empirical models of privacy in location sharing	The rapid adoption of location tracking and mobile social networking technologies raises significant privacy challenges. Today our understanding of people's location sharing privacy preferences remains very limited, including how these preferences are impacted by the type of location tracking device or the nature of the locations visited. To address this gap, we deployed Locaccino, a mobile location sharing system, in a four week long field study, where we examined the behavior of study participants (n=28) who shared their location with their acquaintances (n=373.) Our results show that users appear more comfortable sharing their presence at locations visited by a large and diverse set of people. Our study also indicates that people who visit a wider number of places tend to also be the subject of a greater number of requests for their locations. Over time these same people tend to also evolve more sophisticated privacy preferences, reflected by an increase in time- and location-based restrictions. We conclude by discussing the implications our findings.
PEIR, the personal environmental impact report, as a platform for participatory sensing systems research	PEIR, the Personal Environmental Impact Report, is a participatory sensing application that uses location data sampled from everyday mobile phones to calculate personalized estimates of environmental impact and exposure. It is an example of an important class of emerging mobile systems that combine the distributed processing capacity of the web with the personal reach of mobile technology. This paper documents and evaluates the running PEIR system, which includes mobile handset based GPS location data collection, and server-side processing stages such as HMM-based activity classification (to determine transportation mode); automatic location data segmentation into "trips''; lookup of traffic, weather, and other context data needed by the models; and environmental impact and exposure calculation using efficient implementations of established models. Additionally, we describe the user interface components of PEIR and present usage statistics from a two month snapshot of system use. The paper also outlines new algorithmic components developed based on experience with the system and undergoing testing for integration into PEIR, including: new map-matching and GSM-augmented activity classification techniques, and a selective hiding mechanism that generates believable proxy traces for times a user does not want their real location revealed.
Prototyping and sampling experience to evaluate ubiquitous computing privacy in the real world	We developed an inquiry technique, which we called "paratype," based on experience prototyping and event-contingent experience sampling, to survey people in real-life situations about ubiquitous computing (ubicomp) technology. We used this tool to probe the opinions of the conversation partners of users of the Personal Audio Loop, a memory aid that can have a strong impact on their privacy. We present the findings of this study and their implications, specifically the need to broaden public awareness of ubicomp applications and the unfitness of traditional data protection guidelines for tackling the privacy issues of many ubicomp applications. We also point out benefits and methodological issues of paratypes and discuss why they are particularly fit for studying certain classes of mobile and ubicomp applications.
AutoWitness: locating and tracking stolen property while tolerating GPS and radio outages	We present AutoWitness, a system to deter, detect, and track personal property theft, improve historically dismal stolen property recovery rates, and disrupt stolen property distribution networks. A property owner embeds a small tag inside the asset to be protected, where the tag lies dormant until it detects vehicular movement. Once moved, the tag uses inertial sensor-based dead reckoning to estimate position changes, but to reduce integration errors, the relative position is reset whenever the sensors indicate the vehicle has stopped. The sequence of movements, stops, and turns are logged in compact form and eventually transferred to a server using a cellular modem after both sufficient time has passed (to avoid detection) and RF power is detectable (hinting cellular access may be available). Eventually, the trajectory data are sent to a server which attempts to match a path to the observations. The algorithm uses a Hidden Markov Model of city streets and Viterbi decoding to estimate the most likely path. The proposed design leverages low-power radios and inertial sensors, is immune to intransit cloaking, and supports post hoc path reconstruction. Our prototype demonstrates technical viability of the design; the volume market forces driving machine-to-machine communications will soon make the design economically viable.
PoolView: stream privacy for grassroots participatory sensing	This paper develops mathematical foundations and architectural components for providing privacy guarantees on stream data in grassroots participatory sensing applications, where groups of participants use privately-owned sensors to collectively measure aggregate phenomena of mutual interest. Grassroots applications refer to those initiated by members of the community themselves as opposed to by some governing or official entities. The potential lack of a hierarchical trust structure in such applications makes it harder to enforce privacy. To address this problem, we develop a privacy-preserving architecture, called PoolView, that relies on data perturbation on the client-side to ensure individuals' privacy and uses community-wide reconstruction techniques to compute the aggregate information of interest. PoolView allows arbitrary parties to start new services, called pools, to compute new types of aggregate information for their clients. Both the client-side and server-side components of PoolView are implemented and available for download, including the data perturbation and reconstruction components. Two simple sensing services are developed for illustration; one computes traffic statistics from subscriber GPS data and the other computes weight statistics for a particular diet. Evaluation, using actual data traces collected by the authors, demonstrates the privacy-preserving aggregation functionality in PoolView.
Ordinal MDS-based localisation for wireless sensor networks	There are various applications in wireless sensor networks which require knowing the relative or actual position of the sensor nodes. Over the past few years, there have been different localisation algorithms proposed in the literature. The algorithms based on classical Multi-Dimensional Scaling (MDS) only require 3 or 4 anchor nodes and can provide higher accuracy than some other schemes. In this paper, we propose and analyse another type of MDS (called ordinal MDS) for localisation in wireless sensor networks. Ordinal MDS differs from classical MDS in that it only requires a monotonicity constraint between the shortest path distance and the Euclidean distance for each pair of nodes. We conduct simulation studies under square and C-shaped topologies with different connectivity levels and number of anchors. Results show that ordinal MDS provides a lower position estimation error than classical MDS in both hop-based and range-based scenarios.
Localization for mobile sensor networks	Many sensor network applications require location awareness, but it is often too expensive to include a GPS receiver in a sensor network node. Hence, localization schemes for sensor networks typically use a small number of seed nodes that know their location and protocols whereby other nodes estimate their location from the messages they receive. Several such localization techniques have been proposed, but none of them consider mobile nodes and seeds. Although mobility would appear to make localization more difficult, in this paper we introduce the sequential Monte Carlo Localization method and argue that it can exploit mobility to improve the accuracy and precision of localization. Our approach does not require additional hardware on the nodes and works even when the movement of seeds and nodes is uncontrollable. We analyze the properties of our technique and report experimental results from simulations. Our scheme outperforms the best known static localization schemes under a wide range of conditions.
Localization from mere connectivity	It is often useful to know the geographic positions of nodes in a communications network, but adding GPS receivers or other sophisticated sensors to every node can be expensive. We present an algorithm that uses connectivity information who is within communications range of whom to derive the locations of the nodes in the network. The method can take advantage of additional information, such as estimated distances between neighbors or known positions for certain anchor nodes, if it is available. The algorithm is based on multidimensional scaling, a data analysis technique that takes O(n3) time for a network of n nodes. Through simulation studies, we demonstrate that the algorithm is more robust to measurement error than previous proposals, especially when nodes are positioned relatively uniformly throughout the plane. Furthermore, it can achieve comparable results using many fewer anchor nodes than previous methods, and even yields relative coordinates when no anchor nodes are available.
Node Localization Using Mobile Robots in Delay-Tolerant Sensor Networks	We present a novel scheme for node localization in a Delay-Tolerant Sensor Network (DTN). In a DTN, sensor devices are often organized in network clusters that may be mutually disconnected. Some mobile robots may be used to collect data from the network clusters. The key idea in our scheme is to use this robot to perform location estimation for the sensor nodes it passes based on the signal strength of the radio messages received from them. Thus, we eliminate the processing constraints of static sensor nodes and the need for static reference beacons. Our mathematical contribution is the use of a Robust Extended Kalman Filter (REKF)-based state estimator to solve the localization. Compared to the standard extended Kalman filter, REKF is computationally efficient and also more robust. Finally, we have implemented our localization scheme on a hybrid sensor network test bed and show that it can achieve node localization accuracy within 1m in a large indoor setting.
Range-free localization schemes for large scale sensor networks	Wireless Sensor Networks have been proposed for a multitude of location-dependent applications. For such systems, the cost and limitations of the hardware on sensing nodes prevent the use of range-based localization schemes that depend on absolute point-to-point distance estimates. Because coarse accuracy is sufficient for most sensor network applications, solutions in range-free localization are being pursued as a cost-effective alternative to more expensive range-based approaches. In this paper, we present APIT, a novel localization algorithm that is range-free. We show that our APIT scheme performs best when an irregular radio pattern and random node placement are considered, and low communication overhead is desired. We compare our work via extensive simulation, with three state-of-the-art range-free localization schemes to identify the preferable system configurations of each. In addition, we study the effect of location error on routing and tracking performance. We show that routing performance and tracking accuracy are not significantly affected by localization error when the error is less than 0.4 times the communication radio radius.
Wireless sensor networks: a survey	This paper describes the concept of sensor networks which has been made viable by the convergence of micro-electro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are also discussed.
Self-organisation of sensor networks using genetic algorithms	In this paper we propose a reduced-complexity Genetic Algorithm (GA) for optimisation of multihop sensor networks. The goal of the system is to generate optimal number of sensor clusters with Cluster-Heads (CHs). It results in minimisation of the power consumption of the sensor system while maximising the sensor objectives (coverage and exposure). The GA is used to adaptively create various components such as cluster-members, CHs and next-cluster. These components are then used to evaluate the average fitness of the system based on the sequence of communication links towards the sink. In addition, the mechanism supports dynamically changing coverage, task requirements, failures, incremental redeployment and reconfiguration.
Building efficient wireless sensor networks with low-level naming	In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses) and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.
Wireless sensor networks: a survey	This paper describes the concept of sensor networks which has been made viable by the convergence of micro-electro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are also discussed.
Wake on wireless: an event driven energy saving strategy for battery operated devices	The demand for an all-in-one phone with integrated personal information management and data access capabilities is beginning to accelerate. While personal digital assistants (PDAs) with built-in cellular, WiFi, and Voice-Over-IP technologies have the ability to serve these needs in a single package, the rate at which energy is consumed by PDA-based phones is very high. Thus, these devices can quickly drain their own batteries and become useless to their owner.In this paper, we introduce a technique to increase the battery lifetime of a PDA-based phone by reducing its idle power, the power a device consumes in a "standby" state. To reduce the idle power, we essentially shut down the device and its wireless network card when the device is not being used---the device is powered only when an incoming call is received. Using this technique, we can increase the battery lifetime by up to 115%.In this paper, we describe the design of our "wake-on-wireless" energy-saving strategy and the prototype device we implemented. To evaluate our technique, we compare it with alternative approaches. Our results show that our technique can provide a significant lifetime improvement over other technologies.
Twitinfo: aggregating and visualizing microblogs for event exploration	Microblogs are a tremendous repository of user-generated content about world events. However, for people trying to understand events by querying services like Twitter, a chronological log of posts makes it very difficult to get a detailed understanding of an event. In this paper, we present TwitInfo, a system for visualizing and summarizing events on Twitter. TwitInfo allows users to browse a large collection of tweets using a timeline-based display that highlights peaks of high tweet activity. A novel streaming algorithm automatically discovers these peaks and labels them meaningfully using text from the tweets. Users can drill down to subevents, and explore further via geolocation, sentiment, and popular URLs. We contribute a recall-normalized aggregate sentiment visualization to produce more honest sentiment overviews. An evaluation of the system revealed that users were able to reconstruct meaningful summaries of events in a small amount of time. An interview with a Pulitzer Prize-winning journalist suggested that the system would be especially useful for understanding a long-running event and for identifying eyewitnesses. Quantitatively, our system can identify 80-100% of manually labeled peaks, facilitating a relatively complete view of each event studied.
Is it really about me?: message content in social awareness streams	In this work we examine the characteristics of social activity and patterns of communication on Twitter, a prominent example of the emerging class of communication systems we call "social awareness streams." We use system data and message content from over 350 Twitter users, applying human coding and quantitative analysis to provide a deeper understanding of the activity of individuals on the Twitter network. In particular, we develop a content-based categorization of the type of messages posted by Twitter users, based on which we examine users' activity. Our analysis shows two common types of user behavior in terms of the content of the posted messages, and exposes differences between users in respect to these activities.
Microblogging during two natural hazards events: what twitter may contribute to situational awareness	We analyze microblog posts generated during two recent, concurrent emergency events in North America via Twitter, a popular microblogging service. We focus on communications broadcast by people who were "on the ground" during the Oklahoma Grassfires of April 2009 and the Red River Floods that occurred in March and April 2009, and identify information that may contribute to enhancing situational awareness (SA). This work aims to inform next steps for extracting useful, relevant information during emergencies using information extraction (IE) techniques.
Eddi: interactive topic-based browsing of social status streams	Twitter streams are on overload: active users receive hundreds of items per day, and existing interfaces force us to march through a chronologically-ordered morass to find tweets of interest. We present an approach to organizing a user's own feed into coherently clustered trending topics for more directed exploration. Our Twitter client, called Eddi, groups tweets in a user's feed into topics mentioned explicitly or implicitly, which users can then browse for items of interest. To implement this topic clustering, we have developed a novel algorithm for discovering topics in short status updates powered by linguistic syntactic transformation and callouts to a search engine. An algorithm evaluation reveals that search engine callouts outperform other approaches when they employ simple syntactic transformation and backoff strategies. Active Twitter users evaluated Eddi and found it to be a more efficient and enjoyable way to browse an overwhelming status update feed than the standard chronological interface.
Continuum: designing timelines for hierarchies, relationships and scale	Temporal events, while often discrete, also have interesting relationships within and across times: larger events are often collections of smaller more discrete events (battles within wars; artists' works within a form); events at one point also have correlations with events at other points (a play written in one period is related to its performance over a period of time). Most temporal visualisations, however, only represent discrete data points or single data types along a single timeline: this event started here and ended there; this work was published at this time; this tag was popular for this period. In order to represent richer, faceted attributes of temporal events, we present Continuum. Continuum enables hierarchical relationships in temporal data to be represented and explored; it enables relationships between events across periods to be expressed, and in particular it enables user-determined control over the level of detail of any facet of interest so that the person using the system can determine a focus point, no matter the level of zoom over the temporal space. We present the factors motivating our approach, our evaluation and implementation of this new visualisation which makes it easy for anyone to apply this interface to rich, large-scale datasets with temporal data.
Meme-tracking and the dynamics of the news cycle	Tracking new topics, ideas, and "memes" across the Web has been an issue of considerable interest. Recent work has developed methods for tracking topic shifts over long time scales, as well as abrupt spikes in the appearance of particular named entities. However, these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days - the time scale at which we perceive news and events. We develop a framework for tracking short, distinctive phrases that travel relatively intact through on-line text; developing scalable algorithms for clustering textual variants of such phrases, we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis. As our principal domain of study, we show how such a meme-tracking approach can provide a coherent representation of the news cycle - the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis. We tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle. In particular, we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively, with divergent behavior around the overall peak and a "heartbeat"-like pattern in the handoff between news and blogs. We also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits.
Securing Wi-Fi networks with position verification: extended version	In this paper we investigate position-based enhancements to Wi-Fi network security. Specifically, we investigate whether Received Signal Strength (RSS) measurements can identify attempts at network access by malicious nodes exterior to an authorised network perimeter. We assume the malicious nodes will spoof their received or transmitted power-levels in attempts to circumvent standard position-based security techniques. We outline why residual analysis of the RSS measurements cannot robustly identify illegal network access requests. However, we show that by referring the residual RSS analysis to a 'claimed position', interior to the authorised perimeter, a robust position-based verification system for secure network access can be developed. Indoor systems based on RSS fingerprints and differential RSS fingerprints are studied. Outdoor systems under the assumption of log-normal shadowing are also investigated.
Secure verification of location claims	With the growing prevalence of sensor and wireless networks comes a new demand for location-based access control mechanisms. We introduce the concept of secure location verification, and we show how it can be used for location-based access control. Then, we present the Echo protocol, a simple method for secure location verification. The Echo protocol is extremely lightweight: it does not require time synchronization, cryptography, or very precise clocks. Hence, we believe that it is well suited for use in small, cheap, mobile devices.
Secure location verification using radio broadcast	Secure location verification is a recently stated problem that has a number of practical applications. The problem requires a wireless sensor network to confirm that a potentially malicious prover is located in a designated area. The original solution to the problem, as well as solutions to related problems, exploits the difference between propagation speeds of radio and sound waves to estimate the position of the prover. In this paper, we propose a solution that leverages the broadcast nature of the radio signal emitted by the prover and the distributed topology of the network. The idea is to separate the functions of the sensors. Some sensors are placed such that they get the signal from the prover if it is inside the protected area. The others are positioned so that they can only get the signal from the prover outside the area. Hence the latter sensors reject the prover if they hear its signal. Our solution is versatile and deals with provers using either omni-directional or directional propagation of radio signals without requiring any special hardware besides a radio transceiver. We estimate the bounds on the number of sensors required to protect the areas of various shapes and extend our solution to handle complex radio signal propagation, optimize sensor placement and operate without precise topology information.
Robotics-based location sensing using wireless ethernet	A key subproblem in the construction of location-aware systems is the determination of the position of a mobile device. This paper describes the design, implementation and analysis of a system for determining position inside a building from measured RF signal strengths of packets on an IEEE 802.11b wireless Ethernet network. Previous approaches to location awareness with RF signals have been severely hampered by non-linearity, noise and complex correlations due to multi-path effects, interference and absorption. The design of our system begins with the observation that determining position from complex, noisy and non-linear signals is a well-studied problem in the field of robotics. Using only off-the-shelf hardware, we achieve robust position estimation to within a meter in our experimental context and after adequate training of our system. We can also coarsely determine our orientation and can track our position as we move. By applying recent advances in probabilistic inference of position and sensor fusion from noisy signals, we show that the RF emissions from base stations as measured by off-the-shelf wireless Ethernet cards are sufficiently rich in information to permit a mobile device to reliably track its location.
Robust statistical methods for securing wireless localization in sensor networks	Many sensor applications are being developed that require the location of wireless devices, and localization schemes have been developed to meet this need. However, as location-based services become more prevalent, the localization infrastructure will become the target of malicious attacks. These attacks will not be conventional security threats, but rather threats that adversely affect the ability of localization schemes to provide trustworthy location information. This paper identifies a list of attacks that are unique to localization algorithms. Since these attacks are diverse in nature, and there may be many unforseen attacks that can bypass traditional security countermeasures, it is desirable to alter the underlying localization algorithms to be robust to intentionally corrupted measurements. In this paper, we develop robust statistical methods to make localization attack-tolerant. We examine two broad classes of localization: triangulation and RF-based fingerprinting methods. For triangulation-based localization, we propose an adaptive least squares and least median squares position estimator that has the computational advantages of least squares in the absence of attacks and is capable of switching to a robust mode when being attacked. We introduce robustness to fingerprinting localization through the use of a median-based distance metric. Finally, we evaluate our robust localization schemes under different threat conditions.
Attack-resistant location estimation in sensor networks	Many sensor network applications require sensors' locations to function correctly. Despite the recent advances, location discovery for sensor networks in hostile environments has been mostly overlooked. Most of the existing localization protocols for sensor networks are vulnerable in hostile environments. The security of location discovery can certainly be enhanced by authentication. However, the possible node compromises and the fact that location determination uses certain physical features (e.g., received signal strength) of radio signals make authentication not as effective as in traditional security applications. This paper presents two methods to tolerate malicious attacks against beacon-based location discovery in sensor networks. The first method filters out malicious beacon signals on the basis of the "consistency" among multiple beacon signals, while the second method tolerates malicious beacon signals by adopting an iteratively refined voting scheme. Both methods can survive malicious attacks even if the attacks bypass authentication, provided that the benign beacon signals constitute the majority of the "consistent" beacon signals. This paper also presents the implementation of these techniques on MICA2 motes running TinyOS, and the evaluation through both simulation and field experiments. The experimental results demonstrate that the proposed methods are promising for the current generation of sensor networks.
Practical robust localization over large-scale 802.11 wireless networks	We demonstrate a system built using probabilistic techniques that allows for remarkably accurate localization across our entire office building using nothing more than the built-in signal intensity meter supplied by standard 802.11 cards. While prior systems have required significant investments of human labor to build a detailed signal map, we can train our system by spending less than one minute per office or region, walking around with a laptop and recording the observed signal intensities of our building's unmodified base stations. We actually collected over two minutes of data per office or region, about 28 man-hours of effort. Using less than half of this data to train the localizer, we can localize a user to the precise, correct location in over 95% of our attempts, across the entire building. Even in the most pathological cases, we almost never localize a user any more distant than to the neighboring office. A user can obtain this level of accuracy with only two or three signal intensity measurements, allowing for a high frame rate of localization results. Furthermore, with a brief calibration period, our system can be adapted to work with previously unknown user hardware. We present results demonstrating the robustness of our system against a variety of untrained time-varying phenomena, including the presence or absence of people in the building across the day. Our system is sufficiently robust to enable a variety of location-aware applications without requiring special-purpose hardware or complicated training and calibration procedures.
SeRLoc: secure range-independent localization for wireless sensor networks	In many applications of wireless sensor networks (WSN), sensors are deployed un-tethered in hostile environments. For location-aware WSN applications, it is essential to ensure that sensors can determine their location, even in the presence of malicious adversaries. In this paper we address the problem of enabling sensors of WSN to determine their location in an un-trusted environment. Since localization schemes based on distance estimation are expensive for the resource constrained sensors, we propose a range-independent localization algorithm called SeRLoc. SeRLoc is distributed algorithm and does not require any communication among sensors. In addition, we show that SeRLoc is robust against severe WSN attacks, such as the wormhole attack, the sybil attack and compromised sensors. To the best of our knowledge, ours is the first work that provides a security-aware range-independent localization scheme for WSN. We present a threat analysis and comparison of the performance of SeRLoc with state-of-the-art range-independent localization schemes.
Optimal worm-scanning method using vulnerable-host distributions	Most internet worms use random scanning. The distribution of vulnerable hosts on the internet, however, is highly non-uniform over the IP-address space. This implies that random scanning wastes many scans on invulnerable addresses and more virulent scanning schemes may take advantage of the non-uniformity of a vulnerable-host distribution. Questions then arise as to how attackers may exploit such information and how virulent the resulting worm may be. These issues provide 'worst-case scenarios'for defenders and 'best-case scenarios'for attackers when the vulnerable-host distribution is available. This work develops such a scenario, called importance scanning, which results from importance sampling in statistics. Importance scanning scans the IP-address space according to an empirical distribution of vulnerable hosts. An analytical model is developed to relate the infection rate of worms with the Importance-Scanning (IS) strategies. Based on parameters chosen from Witty and Code Red worms, the experimental results show that an IS worm can spread much faster than either a random-scanning worm or a routing worm. In addition, a game-theoretical approach suggests that the best strategy for defenders is to scatter applications uniformly in the entire IP-address space.
On the effectiveness of distributed worm monitoring	Distributed monitoring of unused portions of the IP address space holds the promise of providing early and accurate detection of high-profile security events, especially Internet worms. While this observation has been accepted for some time now, a systematic analysis of the requirements for building an effective distributed monitoring infrastructure is still missing. In this paper, we attempt to quantify the benefits of distributed monitoring and evaluate the practicality of this approach. To do so we developed a new worm propagation model that relaxes earlier assumptions regarding the uniformity of the underlying vulnerable population. This model allows us to evaluate how the size of the monitored address space, as well the number and locations of monitors, impact worm detection time. We empirically evaluate the effect of these parameters using traffic traces from over 1.5 billion suspicious connection attempts observed by more than 1600 intrusion detection systems dispersed across the Internet. Our results show that distributed monitors with half the allocated space of a centralized monitor can detect non-uniform scanning worms in half the time. Moreover, a distributed monitor of the same size as a centralized monitor can detect the worm four times faster. Furthermore, we show that even partial knowledge of the vulnerable population density can be used to improve monitor placement. Exploiting information about the location of the vulnerable population leads, in some cases, to detection time that is seven times as fast compared to random monitor deployment.
On the performance of internet worm scanning strategies	In recent years, fast spreading worms, such as Code Red, Slammer, Blaster and Sasser, have become one of the major threats to the security of the Internet. In order to defend against future worms, it is important to first understand how worms propagate and how different scanning strategies affect worm propagation dynamics. In this paper, we systematically model and analyze worm propagation under various scanning strategies, such as uniform scan, routing scan, hit-list scan, cooperative scan, local preference scan, sequential scan, divide-and-conquer scan, target scan, etc. We also provide an analytical model to accurately model Witty worm's destructive behavior. By using the same modeling framework, we reveal the underlying similarity and relationship between different worm scanning strategies. In addition, based on our simulation and analysis of Blaster worm propagation and monitoring, we provide a guideline for building a better worm monitoring infrastructure.
Monitoring and early warning for internet worms	After the Code Red incident in 2001 and the SQL Slammer in January 2003, it is clear that a simple self-propagating worm can quickly spread across the Internet, infects most vulnerable computers before people can take effective countermeasures. The fast spreading nature of worms calls for a worm monitoring and early warning system. In this paper, we propose effective algorithms for early detection of the presence of a worm and the corresponding monitoring system. Based on epidemic model and observation data from the monitoring system, by using the idea of "detecting the trend, not the rate" of monitored illegitimated scan traffic, we propose to use a Kalman filter to detect a worm's propagation at its early stage in real-time. In addition, we can effectively predict the overall vulnerable population size, and correct the bias in the observed number of infected hosts. Our simulation experiments for Code Red and SQL Slammer show that with observation data from a small fraction of IP addresses, we can detect the presence of a worm when it infects only 1% to 2% of the vulnerable computers on the Internet.
Are we in sync?: synchronization requirements for watching online video together.	Synchronization between locations is an important factor for enabling remote shared experiences. Still, experimental data on what is the acceptable synchronization level is scarce. This paper discusses the synchronization requirements for watching online videos together - a popular set of services that recreate the shared experience of watching TV together by offering tools to communicate while watching. It studies the noticeability and annoyance of synchronization differences of the video being watched, as well as the impact on users' feelings of togetherness, both for voice chat and text chat. Results of an experiment with 36 participants show that when using voice chat, users notice synchronization differences sooner, are more annoyed and feel more together than when using text chat. However, users with high text chat activity notice synchronization differences similar to participants using voice chat.
Enhancing online personal connections through the synchronized sharing of online video	Going to movies in a group and inviting friends over to watch TV are common social activities. This social engagement both improves the viewing experience and helps us stay close with our friends and family. To bring this feeling of co-presence to the Internet, we developed a set of prototypes that enable people to feel more connected by watching web video together in sync. We present the preliminary results of a quantitative usage study and show initial evidence that simultaneous video sharing online can help people feel closer and more connected to their friends and family.
Of social television comes home: a field study of communication choices and practices in tv-based text and voice chat	Social television applications have emerged as a potentially valuable convergence of media and communication, but questions remain about the utility and nature of the communication experiences they will provide. We present our study of STV3, an application that adds freeform text and voice chat capabilities to the conventional television-viewing experience. We conducted an in-depth field study of STV3 to understand how friends integrate communication through social television into their lives. Our results reveal users' choices of communication modality, their topics of conversation, and the sense of connectedness that was fostered through their use of STV3. Our findings indicate that participants overwhelmingly preferred text chat to voice chat, and that they often communicated about topics unrelated to the television content.
Supporting the social uses of television: sociability heuristics for social tv	Various social television systems and applications, enabling remote communication and interaction between viewers, are currently in development. Although usability guidelines exist for interactive television to ensure a usable system, there are no sociability guidelines for designing or evaluating the social interaction these systems enable. In this paper we present twelve sociability heuristics for evaluating social TV, based on several user studies with social TV systems.
Mining behavioral economics to design persuasive technology for healthy choices	Influence through information and feedback has been one of the main approaches of persuasive technology. We propose another approach based on behavioral economics research on decision-making. This approach involves designing the presentation and timing of choices to encourage people to make self-beneficial decisions. We applied three behavioral economics persuasion techniques - the default option strategy, the planning strategy, and the asymmetric choice strategy - to promote healthy snacking in the workplace. We tested the strategies in three experimental case studies using a human snack deliverer, a robot, and a snack ordering website. The default and the planning strategies were effective, but they worked differently depending on whether the participants had healthy dietary lifestyles or not. We discuss designs for persuasive technologies that apply behavioral economics.
Designing persuasion: health technology for low-income African American communities	In the United States, African Americans face a disproportionate amount of diet-related health problems. For example, African American adults are 1.6 times more likely to have diabetes than their Caucasian counterparts. Individuals in low-income communities may face a greater risk because they typically have less access to healthy foods. Due to the significant diet-related problems within the African American community, public health researchers call for approaches to health promotion that take into account the relationship between culture and dietary habits. In this paper, we discuss three important considerations for the design of technologies that address the diet-related health disparities in low-income African American communities. These considerations include designing for cultural relevancy, modeling health behavior, and encouraging healthy behavior through the use of social psychological theories of persuasion. We use a game design example to illustrate how each of these considerations can be incorporated into the development of new technology.
Understanding and Using Context	Context is a poorly used source of information in our computing environments. As a result, we have an impoverished understanding of what context is and how it can be used. In this paper, we provide an operational definition of context and discuss the different ways in which context can be used by context-aware applications. We also present the Context Toolkit, an architecture that supports the building of these context-aware applications. We discuss the features and abstractions in the toolkit that make the task of building applications easier. Finally, we introduce a new abstraction, a situation which we believe will provide additional support to application designers.
Means based adaptive persuasive systems	Large differences in individual responses to persuasive strategies suggest the need for systems that rely on persuasion profiles: estimates of an individual user's susceptibility to different persuasive strategies. Establishing an empirical ground supporting decisions regarding user involvement can provide valuable guidelines for the design of such systems. We describe two studies examining the effects of choice, disclosure, and multiple strategy usage on user compliance to persuasive attempts. We show that involving users in the selection of a specific influence strategy can increase compliance, while disclosing the persuasive intent can reduce compliance. Furthermore, we demonstrate that it is not only feasible, but optimal to choose the single correct influence strategy for a given context; even more so than implementing multiple relevant and congruent influence attempts.
Using social psychology to motivate contributions to online communities	Under-contribution is a problem for many online communities. Social psychology theories of social loafing and goal-setting can provide mid-level design principles to address this problem. We tested the design principles in two field experiments. In one, members of an online movie recommender community were reminded of the uniqueness of their contributions and the benefits that follow from them. In the second, they were given a range of individual or group goals for contribution. As predicted by theory, individuals contributed when they were reminded of their uniqueness and when they were given specific and challenging goals, but other predictions were not borne out. The paper ends with suggestions and challenges for mining social science theories as well as implications for design.
An Attack on RSA Given a Small Fraction of the Private Key Bits	We show that for low public exponent rsa, given a quarter of the bits of the private key an adversary can recover the entire private key. Similar results (though not as strong) are obtained for larger values of e. For instance, when e is a prime in the range [N1/4, N1/2], half the bits of the private key suffice to reconstruct the entire private key. Our results point out the danger of partial key exposure in the rsa public key system.
Finding a small root of a univariate modular equation	We show how to solve a polynomial equation (mod N) of degree k in a single variable x, as long as there is a solution smaller than N1/k. We give two applications to RSA encryption with exponent 3. First, knowledge of all the ciphertext and 2/3 of the plaintext bits for a single message reveals that message. Second, if messages are padded with truly random padding and then encrypted with an exponent 3, then two encryptions of the same message (with different padding) will reveal the message, as long as the padding is less than 1/9 of the length of N. With several encryptions, another technique can (heuristically) tolerate padding up to about 1/6 of the length of N.
Generation of Shared RSA Keys by Two Parties	At Crypto'97 Boneh and Franklin proposed a protocol to efficiently generate shared RSA keys. In the case of two parties, the drawback of their scheme is the need of an independent third party. Furthermore, the security is guaranteed only if the three players follow the protocol. In this paper, we propose a protocol that enables two parties to evaluate any algebraic expression, including an RSA modulus, along the same lines as in the Boneh-Franklin protocol. Our solution does not need the help of a third party and the only assumption we make is the existence of an oblivious transfer protocol. Furthermore, it remains robust even if one of the two players deviates from the protocol.
Multiparty unconditionally secure protocols	Under the assumption that each pair of participants can communicate secretly, we show that any reasonable multiparty protocol can be achieved if at least 2n/3 of the participants are honest. The secrecy achieved is unconditional. It does not rely on any assumption about computational intractability.
Strong Security Against Active Attacks in Information-Theoretic Secret-Key Agreement	The problem of unconditionally secure key agreement, in particular privacy amplification, by communication over an insecure and not even authentic channel, is investigated. The previous definitions of such protocols were weak in the sense that it was only required that after the communication not both parties falsely believe that the key agreement was successful. In such a protocol however it is possible that Eve deceives one of the legitimate partners, i.e., makes him accept the outcome of the protocol although no secret key has been generated. In this paper we introduce the notion of strong protocols which protect each of the parties simultaneously and, in contrast to previous pessimism, it is shown that such protocols exist. For the important special case of privacy amplification, a strong protocol is presented that is based on a new, interactive way of message authentication with an only partially secret key. The use of feedback in such authentication allows to reduce the size of the authenticator, hence of the additional information about the key leaked to the adversary, without increasing the success probability of an active attack. Finally, it is shown that in the scenario where the parties and the adversary have access to repeated realizations of a random experiment, previously derived criteria for the possibility of secret-key agreement against active opponents hold for the new, strong definition of robustness against active attacks rather than for the earlier definition.
Universal Hashing and Authentication Codes	In this paper, we study the application of universal hashing to the construction of unconditionally secure authentication codes without secrecy. This idea is most useful when the number of authenticators is exponentially small compared to the number of possible source states (plaintext messages). We formally define some new classes of hash functions and then prove some new bounds and give some general constructions for these classes of hash functions. Then we discuss the implications to authentication codes.
Modelling misbehaviour in ad hoc networks: a game theoretic approach for intrusion detection	In wireless ad hoc networks, although defence strategies such as Intrusion Detection Systems (IDSs) can be deployed at individual nodes, significant constraints are imposed in terms of the energy expenditure of such systems. In this paper, we use a game theoretic framework to analyse the interactions between pairs of attacking/defending nodes, in both static and dynamic contexts, and considering both complete and incomplete information regarding the maliciousness of neighbouring nodes. The static case analysis provides the defender with an overview of the security situation in terms of risk and monitoring cost. A dynamic Bayesian game formulation allows the defender to adjust his belief about his opponent based on his observations and the game history, and consequently to influence the achievable Nash equilibrium for new stage game. A new Bayesian hybrid detection system is proposed for the defender, which balances energy costs and monitoring gains.
Enhancing profiles for anomaly detection using time granularities	Recently, association rules have been used to generate profiles of "normal" behavior for anomaly detection. However, the time factor (especially in terms of multiple time granularities) has not been utilized extensively in generation of these profiles. In reality, user behavior during different time intervals may be very different. For example, the "normal" number and duration of FTP connections may vary from working hours to midnight, from business day to weekend or holiday. Furthermore, these variations may depend on the day of the month or the week. This paper proposes to build profiles using temporal association rules in terms of multiple time granularities, and describes algorithms to discover these profiles. Because multiple time granularities are used for the profile generation, the proposed method is more flexible and precise than previous methods that use fixed partition of time intervals. Finally, the paper describes an experiment and its preliminary result on TCP-dump data.
Using interface cues in online health community boards to change impressions and encourage user contribution	Online health message boards have become popular, as users not only gain information from other users but also share their own experiences. However, as with most venues of user-generated content, there is need to constantly make quality evaluations as one sifts through enormous amounts of content. Can interface cues, conveying (1) pedigree of users posting content and (2) popularity of the posted content, help new users efficiently make credibility assessments? Furthermore, can the assignment of these same cues to their own posts serve to motivate content generation on their part? These questions were investigated in a 2-session between-subjects experiment (N = 99) with a prototype of a message-board that experimentally varied interface cues, and found that popularity indicators are more influential than pedigree indicators for both evaluation of existing content and contribution of new content. Findings also suggest theoretical mechanisms - involving such concepts as perceived authority, bandwagon effects, sense of agency and sense of community - by which cues affect user experience, providing rich implications for designing and deploying interface cues.
Informational cascades and software adoption on the internet: an empirical investigation	Online users often need to make adoption decisions without accurate information about the product values. An informational cascade occurs when it is optimal for an online user, having observed others' actions, to follow the adoption decision of the preceding individual without regard to his own information. Informational cascades are often rational for individual decision making; however, it may lead to adoption of inferior products. With easy availability of information about other users' choices, the Internet offers an ideal environment for informational cascades. In this paper, we empirically examine informational cascades in the context of online software adoption. We find user behavior in adopting software products is consistent with the predictions of the informational cascades literature. Our results demonstrate that online users' choices of software products exhibit distinct jumps and drops with changes in download ranking, as predicted by informational cascades theory. Furthermore, we find that user reviews have no impact on user adoption of the most popular product, while having an increasingly positive impact on the adoption of lower ranking products. The phenomenon persists after controlling for alternative explanations such as network effects, word-of-mouth effects, and product diffusion. Our results validate informational cascades as an important driver for decision making on the Internet. The finding also offers an explanation for the mixed results reported in prior studies with regard to the influence of online user reviews on product sales. We show that the mixed results could be due to the moderating effect of informational cascades.
Follow the reader: filtering comments on slashdot	Large-scale online communities need to manage the tension between critical mass and information overload. Slashdot is a news and discussion site that has used comment rating to allow massive participation while providing a mechanism for users to filter content. By default, comments with low ratings are hidden. Of users who changed the defaults, more than three times as many chose to use ratings for filtering or sorting as chose to suppress the use of comment ratings. Nearly half of registered users, however, never strayed from the default filtering settings, suggesting that the costs of exploring and selecting custom filter settings exceeds the expected benefit for many users. We recommend leveraging the efforts of the users that actively choose filter settings to reduce the cost of changing settings for all other users. One strategy is to create static schemas that capture the filtering preferences of different groups of readers. Another strategy is to dynamically set filtering thresholds for each conversation thread, based in part on the choices of previous readers. For predicting later readers' choices, the choices of previous readers are far more useful than content features such as the number of comments or the ratings of those comments.
Effects of specialization in computers, web sites, and web agents on e-commerce trust	Suppose you went shopping online for wines and visited several sites, each recommending particular reds and whites. Which kind of site are you likely to trust more-costco.com or wine.com? The specialization implied by the latter suggests more expertise in the domain of wines. Does it mean that you are more likely to purchase wines recommended by sites such as wine.com and vintagecellars.com.au than those recommended by generalist sites such as costco.com and samsclub.com? Our study attempts to answer this question by experimentally investigating how specialization in media technology (specifically, web agent, web site, and computer) influences individuals' perception and attitudes towards sources in online communication, particularly consumer trust and purchase behaviors in e-commerce. All subjects (N=124) went to a specially constructed online site with a virtual shopping cart for a wine-purchasing task, as part of a 2 (specialist vs. generalist web agent)x2 (specialist vs. generalist web site)x2 (specialist computer vs. generalist computer) between-subjects experiment. Results indicate significant main effects and interactions of the agent, site, and computer specialization on trust and purchase decision time. Theoretical and practical implications are discussed.
Using social psychology to motivate contributions to online communities	Under-contribution is a problem for many online communities. Social psychology theories of social loafing and goal-setting can provide mid-level design principles to address this problem. We tested the design principles in two field experiments. In one, members of an online movie recommender community were reminded of the uniqueness of their contributions and the benefits that follow from them. In the second, they were given a range of individual or group goals for contribution. As predicted by theory, individuals contributed when they were reminded of their uniqueness and when they were given specific and challenging goals, but other predictions were not borne out. The paper ends with suggestions and challenges for mining social science theories as well as implications for design.
Evaluating swabbing: a touchscreen input method for elderly users with tremor	Elderly users suffering from hand tremor have difficulties interacting with touchscreens because of finger oscillation. It has been previously observed that sliding one's finger across the screen may help reduce this oscillation. In this work, we empirically confirm this advantage by (1) measuring finger oscillation during different actions and (2) comparing error rate and user satisfaction between traditional tapping and swabbing in which the user slides his finger towards a target on a screen edge to select it. We found that oscillation is generally reduced during sliding. Also, compared to tapping, swabbing resulted in improved error rates and user satisfaction. We believe that swabbing will make touchscreens more accessible to senior users with tremor.
Simple pen interaction performance of young and older adults using handheld computers	Several experiments have documented how older adults have greater difficulty using input devices than young adults. None of these experiments, however, have provided information on the challenges faced by older adults when using pens to interact with handheld computers. To address this need, we conducted a study comparing the performance of twenty 18-22year olds, twenty 50-64year olds, and twenty 65-84year olds conducting selection and steering tasks. We found that for the most part, older adults were able to complete tasks accurately and efficiently. An exception occurred with the low accuracy rates achieved by 65-84year old participants when tapping on targets of the same size as the standard radio buttons on the PocketPC. An alternative selection technique we refer to as ''touch'' enabled 65-84year olds to select targets significantly more accurately. If tapping to select, making standard-sized targets 50% larger provided 65-84year olds with similar advantages to switching to ''touch'' interactions.
Towards a feminist HCI methodology: social science, feminism, and HCI	With substantial efforts in ubiquitous computing, ICT4D, and sustainable interaction design, among others, HCI is increasingly engaging with matters of social change that go beyond the immediate qualities of interaction. In doing so, HCI takes on scientific and moral concerns. This paper explores the potential for feminist social science to contribute to and potentially benefit from HCI's rising interest in social change. It describes how feminist contributions to debates in the philosophy of science have helped clarify relationships among objectivity, values, data collection and interpretation, and social consequences. Feminists have proposed and implemented strategies to pursue scientific and moral agendas together and with equal rigor. In this paper, we assess the epistemologies, methodologies, and methods of feminist social science relative to prior and ongoing research efforts in HCI. We conclude by proposing an outline of a feminist HCI methodology.
Implications for design	Although ethnography has become a common approach in HCI research and design, considerable confusion still attends both ethnographic practice and the criteria by which it should be evaluated in HCI. Often, ethnography is seen as an approach to field investigation that can generate requirements for systems development; by that token, the major evaluative criterion for an ethnographic study is the implications it can provide for design. Exploring the nature of ethnographic inquiry, this paper suggests that "implications for design" may not be the best metric for evaluation and may, indeed, fail to capture the value of ethnographic investigations.
Empathy and experience in HCI	For a decade HCI researchers and practitioners have been developing methods, practices and designs 'for the full range of human experience'. On the one hand, a variety of approaches to design, such as aesthetic, affective, and ludic that emphasize particular qualities and contexts of experience and particular approaches to intervening in interactive experience have become focal. On the other, a variety of approaches to understanding users and user experience, based on narrative, biography, and role-play have been developed and deployed. These developments can be viewed in terms of one of the seminal commitments of HCI, 'to know the user'. Empathy has been used as a defining characteristic of designer-user relationships when design is concerned with user experience. In this article, we use 'empathy' to help position some emerging design and user-experience methodologies in terms of dynamically shifting relationships between designers, users, and artefacts.
Reflective design	As computing moves into every aspect of our daily lives, the values and assumptions that underlie our technical practices may unwittingly be propagated throughout our culture. Drawing on existing critical approaches in computing, we argue that reflection on unconscious values embedded in computing and the practices that it supports can and should be a core principle of technology design. Building on a growing body of work in critical computing, reflective design combines analysis of the ways in which technologies reflect and perpetuate unconscious cultural assumptions, with design, building, and evaluation of new computing devices that reflect alternative possibilities. We illustrate this approach through two design case studies.
Postcolonial computing: a lens on design and development	As our technologies travel to new cultural contexts and our designs and methods engage new constituencies, both our design and analytical practices face significant challenges. We offer postcolonial computing as an analytical orientation to better understand these challenges. This analytic orientation inspires four key shifts in our approach to HCI4D efforts: generative models of culture, development as a historical program, uneven economic relations, and cultural epistemologies. Then, through reconsideration of the practices of engagement, articulation and translation in other contexts, we offer designers and researchers ways of understanding use and design practice to respond to global connectivity and movement.
How HCI interprets the probes	We trace how cultural probes have been adopted and adapted by the HCI community. The flexibility of probes has been central to their uptake, resulting in a proliferation of divergent uses and derivatives. The varying patterns of adaptation of the probes reveal important underlying issues in HCI, suggesting underacknowledged disagreements about valid interpretation and the relationship between methods and their underlying methodology. With this analysis, we aim to clarify discussions around probes, and, more importantly, around how we define and evaluate methods in HCI, especially those grounded in unfamiliar conceptions of how research should be done.
The rogue in the lovely black dress: intimacy in world of warcraft	In this paper we present a critical analysis of player accounts of intimacy and intimate experiences in the massively multiplayer online role-playing game World of Warcraft (WoW). Our analysis explores four characteristics that players articulated about their virtual intimate experiences: the permeability of intimacy across virtual and real worlds, the mundane as the origin of intimacy, the significance of reciprocity and exchange, and the formative role of temporality in shaping understandings and recollections of intimate experiences. We also consider the manifest ways that WoW's software features support and encourage these characteristics.
Identifying emotional states using keystroke dynamics	The ability to recognize emotions is an important part of building intelligent computers. Emotionally-aware systems would have a rich context from which to make appropriate decisions about how to interact with the user or adapt their system response. There are two main problems with current system approaches for identifying emotions that limit their applicability: they can be invasive and can require costly equipment. Our solution is to determine user emotion by analyzing the rhythm of their typing patterns on a standard keyboard. We conducted a field study where we collected participants' keystrokes and their emotional states via self-reports. From this data, we extracted keystroke features, and created classifiers for 15 emotional states. Our top results include 2-level classifiers for confidence, hesitance, nervousness, relaxation, sadness, and tiredness with accuracies ranging from 77 to 88%. In addition, we show promise for anger and excitement, with accuracies of 84%.
Identity verification through dynamic keystroke analysis	Typing rhythms are the rawest form of data stemming from the interaction between users and computers. When properly sampled and analyzed, they may become a useful tool to ascertain personal identity. Moreover, unlike other biometric features, typing dynamics have an important characteristic: they still exist and are available even after an access control phase has been passed. As a consequence, keystroke analysis can be used as a viable tool for user authentication throughout the work session. In this paper we present an original approach to identity verification based on the analysis of the typing rhythms of individuals on different texts. Our experiments involve 130 volunteers and reach the best outcomes found in the literature, using a smaller amount of information than in other works, and avoiding any form of tailoring of the system to the available data set. The method described in the paper is easily tuned to reach an acceptable trade-off between the need to spot most impostors and to avoid false alarms, and, as a consequence, it can become a valid aid to intrusion detection.
Automated Facial Expression Classification and affect interpretation using infrared measurement of facial skin temperature variations	Machines would require the ability to perceive and adapt to affects for achieving artificial sociability. Most autonomous systems use Automated Facial Expression Classification (AFEC) and Automated Affect Interpretation (AAI) to achieve sociability. Varying lighting conditions, occlusion, and control over physiognomy can influence the real life performance of vision-based AFEC systems. Physiological signals provide complementary information for AFEC and AAI. We employed transient facial thermal features for AFEC and AAI. Infrared thermal images with participants' normal expression and intentional expressions of happiness, sadness, disgust, and fear were captured. Facial points that undergo significant thermal changes with a change in expression termed as Facial Thermal Feature Points (FTFPs) were identified. Discriminant analysis was invoked on principal components derived from the Thermal Intensity Values (TIVs) recorded at the FTFPs. The cross-validation and person-independent classification respectively resulted in 66.28&percnt; and 56.0&percnt; success rates. Classification significance tests suggest that (1) like other physiological cues, facial skin temperature also provides useful information about affective states and their facial expression; (2) patterns of facial skin temperature variation can complement other cues for AFEC and AAI; and (3) infrared thermal imaging may help achieve artificial sociability in robots and autonomous systems.
Starcraft from the stands: understanding the game spectator	Video games are primarily designed for the players. However, video game spectating is also a popular activity, boosted by the rise of online video sites and major gaming tournaments. In this paper, we focus on the spectator, who is emerging as an important stakeholder in video games. Our study focuses on Starcraft, a popular real-time strategy game with millions of spectators and high level tournament play. We have collected over a hundred stories of the Starcraft spectator from online sources, aiming for as diverse a group as possible. We make three contributions using this data: i) we find nine personas in the data that tell us who the spectators are and why they spectate; ii) we strive to understand how different stakeholders, like commentators, players, crowds, and game designers, affect the spectator experience; and iii) we infer from the spectators' expressions what makes the game entertaining to watch, forming a theory of distinct types of information asymmetry that create suspense for the spectator. One design implication derived from these findings is that, rather than presenting as much information to the spectator as possible, it is more important for the stakeholders to be able to decide how and when they uncover that information.
Performing perception—staging aesthetics of interaction	In interaction design for experience-oriented uses of technology, a central facet of aesthetics of interaction is rooted in the user's experience of herself “performing her perception.” By drawing on performance (theater) theory, phenomenology and sociology and with references to recent HCI-work on the relation between the system and the performer/user and the spectator's relation to this dynamic, we show how the user is simultaneously operator, performer and spectator when interacting. By engaging with the system, she continuously acts out these three roles and her awareness of them is crucial in her experience. We argue that this 3-in-1 is always already shaping the user's understanding and perception of her interaction as it is staged through her experience of the object's form and expression. Through examples ranging from everyday technologies utilizing performances of interaction to spatial contemporary artworks, digital as well as analogue, we address the notion of the performative spectator and the spectating performer. We demonstrate how perception is also performative and how focus on this aspect seems to be crucial when designing experience-oriented products, systems and services.
Street fighter IV: braggadocio off and on-line	In its heyday, the video arcade was a social scene to prove one's video gaming prowess. The introduction of a revolutionary head-to-head fighting game called "Street Fighter II" in 1991 ushered in an era of competitive video gaming with unparalleled complexity. An influx of copy-cat games and the arrival of consoles with capabilities rivaling coin-ops led to the arcade's demise. However, the release of "Street Fighter IV" (SF4) has brought about a revival. I report on the cultural practices of hardcore gaming that have revolved around SF4. SF4's release on both the console (which enables fighting others online) and the arcade has engendered a new set of challenges in constructing what it means to be competitive and legitimate in the world of head-to-head fighting games. I observe that the "enrolment" of an ecology of technological artifacts allows players to translate braggadocio from the arcade, a central phenomenon in competitive gaming.
Understanding the effects of relationships on the intention of a firm to adopt e-banking	Recently, many banks have adopted the e-banking system as a way to expand their range of available financial products and services, so that they can increase their competitiveness. Unlike the popular view that advanced information technology would automatically lead to adoption, this paper considers the nature of relationships in the banking industry and argues that stronger relationship ties and greater relationship advantages will influence technological strategies. Based on the social embeddedness theory, we investigate how several relationship factors may affect the intention of a firm to adopt an e-banking system that is initiated by its trading bank partner. Our findings show that both relationship benefits and relationship trust beliefs can lessen the concern about possible risks and increase the intention to adopt e-banking. Apparently, relationship is an 'invisible' asset that is specific to the bank and its corporate client. As banks wish to employ IT as a competitive weapon, they must become aware that a long-term, reliable relationship cannot be easily replaced by information technology.
Predicting intention to adopt interorganizational linkages: an institutional perspective	This study used institutional theory as a lens to understand the factors that enable the adoption of interorganizational systems. It posits that mimetic, coercive, and normative pressures existing in an institutionalized environment could influence organizational predisposition toward an information technology-based interorganizational linkage. Survey-based research was carried out to test this theory. Following questionnaire development, validation, and pretest with a pilot study, data were collected from the CEO, the CFO, and the CIO to measure the institutional pressures they faced and their intentions to adopt financial electronic data interchange (FEDI). A firm-level structural model was developed based on the CEO's, the CFO's, and the CIO's data. LISREL and PLS were used for testing the measurement and structural models respectively. Results showed that all three institutional pressures-mimetic pressures, coercive pressures, and normative pressures-had a significant influence on organizational intention to adopt FEDI. Except for perceived extent of adoption among suppliers, all other subconstructs were significant in the model. These results provide strong support for institutional-based variables as predictors of adoption intention for interorganizational linkages. These findings indicate that organizations are embedded in institutional networks and call for greater attention to be directed at understanding institutional pressures when investigating information technology innovations adoption.
Coping with systems risk: security planning models for management decision making	The likelihood that the firm's information systems are insufficiently protected against certain kinds of damage or loss is known as "systems risk." Risk can be managed or reduced when managers are aware of the full range of controls available and implement the most effective controls. Unfortunately, they often lack this knowledge, and their subsequent actions to cope with systems risk are less effective than they might otherwise be. This is one viable explanation for why losses from computer abuse and computer disasters today are uncomfortably large and still so potentially devastating after many years of attempting to deal with the problem. Results of comparative qualitative studies in two information services Fortune 500 firms identify an approach that can effectively deal with the problem. This theory-based security program includes (1) use of a security risk planning model, (2) education/training in security awareness, and (3) Countermeasure Matrix analysis.
Your noise is my command: sensing gestures using the body as an antenna	Touch sensing and computer vision have made human-computer interaction possible in environments where keyboards, mice, or other handheld implements are not available or desirable. However, the high cost of instrumenting environments limits the ubiquity of these technologies, particularly in home scenarios where cost constraints dominate installation decisions. Fortunately, home environments frequently offer a signal that is unique to locations and objects within the home: electromagnetic noise. In this work, we use the body as a receiving antenna and leverage this noise for gestural interaction. We demonstrate that it is possible to robustly recognize touched locations on an uninstrumented home wall using no specialized sensors. We conduct a series of experiments to explore the capabilities that this new sensing modality may offer. Specifically, we show robust classification of gestures such as the position of discrete touches around light switches, the particular light switch being touched, which appliances are touched, differentiation between hands, as well as continuous proximity of hand to the switch, among others. We close by discussing opportunities, limitations, and future work.
PowerLine positioning: a practical sub-room-level indoor location system for domestic use	Using existing communications infrastructure, such as 802.11 and GSM, researchers have demonstrated effective indoor localization. Inspired by these previous approaches, and recognizing some limitations of relying on infrastructure users do not control, we present an indoor location system that uses an even more ubiquitous domestic infrastructure—the residential powerline. PowerLine Positioning (PLP) is an inexpensive technique that uses fingerprinting of multiple tones transmitted along the powerline to achieve sub-room-level localization. We describe the basics behind PLP and demonstrate how it compares favorably to other fingerprinting techniques.
The WEKA data mining software: an update	More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.
At the flick of a switch: detecting and classifying unique electrical events on the residential power line	Activity sensing in the home has a variety of important applications, including healthcare, entertainment, home automation, energy monitoring and post-occupancy research studies. Many existing systems for detecting occupant activity require large numbers of sensors, invasive vision systems, or extensive installation procedures. We present an approach that uses a single plug-in sensor to detect a variety of electrical events throughout the home. This sensor detects the electrical noise on residential power lines created by the abrupt switching of electrical devices and the noise created by certain devices while in operation. We use machine learning techniques to recognize electrically noisy events such as turning on or off a particular light switch, a television set, or an electric stove. We tested our system in one home for several weeks and in five homes for one week each to evaluate the system performance over time and in different types of houses. Results indicate that we can learn and classify various electrical events with accuracies ranging from 85-90%.
Is e-banking a competitive weapon? A causal analysis	This study considers the coherence of the financial service sector and adopts different observational variables to identify innovation capital (training and R&D density) and process capital (IT system sufficiency). The results show that human capital has a direct impact on both innovation capital and process capital, which in turn affect customer capital, while finally, customer capital affects business performance. In addition, there is a negative relationship between process capital and customer capital in the financial service sector. It suggests that in the financial service sector, customer satisfaction relies on a sufficient degree of training and R&D density. Intemperate investment on the support of e-banking operation systems may not be a good answer.
Understanding the effects of relationships on the intention of a firm to adopt e-banking	Recently, many banks have adopted the e-banking system as a way to expand their range of available financial products and services, so that they can increase their competitiveness. Unlike the popular view that advanced information technology would automatically lead to adoption, this paper considers the nature of relationships in the banking industry and argues that stronger relationship ties and greater relationship advantages will influence technological strategies. Based on the social embeddedness theory, we investigate how several relationship factors may affect the intention of a firm to adopt an e-banking system that is initiated by its trading bank partner. Our findings show that both relationship benefits and relationship trust beliefs can lessen the concern about possible risks and increase the intention to adopt e-banking. Apparently, relationship is an 'invisible' asset that is specific to the bank and its corporate client. As banks wish to employ IT as a competitive weapon, they must become aware that a long-term, reliable relationship cannot be easily replaced by information technology.
Homebrew databases: complexities of everyday information management in nonprofit organizations	Many people manage a complex assortment of digital information in their lives. Volunteer coordinators at nonprofit organizations are no exception; they collectively manage information about millions of volunteers every year. Yet current information management systems are insufficient for their needs. In this paper, we present results of a qualitative study of the information management practices of volunteer coordinators. We identify the resource constraints and the diverse and fluid information needs, stakeholders, and work contexts that motivate their information management strategies. We characterize the assemblages of information systems that volunteer coordinators have created to satisfice their needs as 'homebrew databases.' Finally, we identify additional information management challenges that result from the use of these 'homebrew databases,' highlighting deficiencies in the appropriateness and usability of databases and information management systems, more generally.
The view from the trenches: organization, power, and technology at two nonprofit homeless outreach centers	Nonprofit social service organizations provide the backbone of social support infrastructure in the U.S. and around the world. As the ecology of information exchange moves evermore digital, nonprofit organizations with limited resources and expertise struggle to keep pace. We present a qualitative investigation of two nonprofit outreach centers providing service to the homeless in a U.S. metropolitan city. Despite similar goals shared by these organizations, apparent differences in levels of computerization, volunteerism, and organizational structure demonstrate the challenges in attempting to adopt technology systems when resources and technical expertise are highly constrained.
Making database systems usable	Database researchers have striven to improve the capability of a database in terms of both performance and functionality. We assert that the usability of a database is as important as its capability. In this paper, we study why database systems today are so difficult to use. We identify a set of five pain points and propose a research agenda to address these. In particular, we introduce a presentation data model and recommend direct data manipulation with a schema later approach. We also stress the importance of provenance and of consistency across presentation models.
Quality versus quantity: e-mail-centric task management and its relation with overload	It is widely acknowledged that many professionals suffer from "e-mail overload." This article presents findings from in-depth fieldwork that examined this phenomenon, uncovering six key challenges of taskmanagement in e-mail. Analysis of qualitative and quantitative data suggests that it is not simply the quantity but also the collaborative quality of e-mail task and project management that causes this overload. We describe how e-mail becomes especially overwhelming when people use it for tasks that involve participation of others; tasks cannot be completed until a response is obtained and so they are interleaved. Interleaving means that the email user must somehow simultaneously keep track of multiple incomplete tasks, often with the only reminder for each one being an e-mail message somewhere in the inbox or a folder. This and other insights from our fieldwork led us to a new design philosophy for e-mail in which resources for task and project management are embedded directly within an e-mail client as opposed to being added on as separate components of the application. A client, TaskMaster, embodying these ideas, was developed and tested by users in managing their real e-mail over an extended period. The design of the client and results of its evaluation are also reported.
Musical fingerprints: collaboration around home media collections	As people collect more and more digital music, photos, and video, the growing scale of the collection challenges how families share and collaborate around home media collections. We studied the intersection between physical and digital media collections. Through 20 two hour, in home interviews, we explored the when, why, and how of the households' organization, access and sharing. Our grounded approach is framed through the use of the media lifecycle and the spectrum of intimacy of the collector and others involved in the stages of the lifecycle. We found a range of accommodations to facilitate collaboration around media collections in the home. For example, media collections often begin with an individual, but as they become shared and integrated into a household, a member of the household will often play a key curatorial role that includes making changes to the organizational scheme, setting aside sub-collections and selecting items to play that account for the entire household's taste. Our findings identify key practices that can inform the design of future media software for the home.
Managing technology use and learning in nonprofit community organizations: methodological challenges and opportunities	We are investigating how to empower nonprofit community organizations to develop the information technology management practices required to carry out their civic goals. We highlight our methodology of working with nonprofit organizations through three case examples from the field. These examples illustrate that nonprofit organizations are able to and can indeed sustain their IT management practices through various methodological techniques. These techniques---such as scenario development, technology inventory assessment, and volunteer management practices---emphasize the importance of long-term critical planning and design skills. Based on our fieldwork, we enumerate lessons that may be valuable for community stakeholders, designers, researchers, and practitioners.
Dealing with death in design: developing systems for the bereaved	Increasingly, systems are being developed and used in ways that involve end of life issues such as death, dying, and bereavement. Yet design considerations and guidelines for technologists working in this sensitive area are not well-established. We therefore report on exploratory fieldwork consisting of focus groups, observations, and consultation with bereavement experts aimed at understanding how technology might be designed to support bereaved parents. From this fieldwork, we derive a set of considerations useful for researchers and designers developing systems that deal specifically with bereavement, and with the end of life more broadly. These considerations focus on interpersonal communication, new ways of being in the world, and materiality. We conclude with a distillation of these considerations into practical design guidelines for working in this area.
Broadening Ubicomp's vision: an exploratory study of charismatic pentecostals and technology use in Brazil	We present results from a qualitative study examining how Charismatic Pentecostals use Information and Communications Technologies (ICTs) in São Paulo, Brazil. This work contributes to the growing body of research that broadens Weiser's vision by exploring technology use in novel and unfamiliar contexts. Our findings reveal how "extreme" and non-rational beliefs frame users' ICT experiences. We argue that if ubicomp is to be global and ubiquitous, accounting for alternative value systems is necessary. We discuss the implications of our findings and present issues the ubicomp community should consider when imagining a future that includes users from parts of the global south.
Empathy and experience in HCI	For a decade HCI researchers and practitioners have been developing methods, practices and designs 'for the full range of human experience'. On the one hand, a variety of approaches to design, such as aesthetic, affective, and ludic that emphasize particular qualities and contexts of experience and particular approaches to intervening in interactive experience have become focal. On the other, a variety of approaches to understanding users and user experience, based on narrative, biography, and role-play have been developed and deployed. These developments can be viewed in terms of one of the seminal commitments of HCI, 'to know the user'. Empathy has been used as a defining characteristic of designer-user relationships when design is concerned with user experience. In this article, we use 'empathy' to help position some emerging design and user-experience methodologies in terms of dynamically shifting relationships between designers, users, and artefacts.
Passing on & putting to rest: understanding bereavement in the context of interactive technologies	While it can be a delicate and emotionally-laden topic, new technological trends compel us to confront a range of problems and issues about death and bereavement. This area presents complex challenges and the associated literature is extensive. In this paper we offer a way of slicing through several perspectives in the social sciences to see clearly a set of salient issues related to bereavement. Following this, we present a theoretical lens to provide a way of conceptualizing how the HCI community could begin to approach such issues. We then report field evidence from 11 in-depth interviews conducted with bereaved participants and apply the proposed lens to unpack key emergent problems and tensions. We conclude with a discussion on how the HCI design space might be sensitized to better support the social processes that unfold when bereavement occurs.
Dying, death, and mortality: towards thanatosensitivity in HCI	What happens to human-computer "interaction" when the human user is no longer alive? This exploratory paper uses insights from the critical humanist tradition to argue for the urgent need to consider the facts of mortality, dying, and death in HCI research. Using an interdisciplinary approach, we critically reflect upon how the intersection of death and computing is currently navigated and illustrate the conceptual and practical complexities presented by mortality, dying, and death in HCI. Finally, we introduce the concept of thanatosensitivity to describe an approach that actively integrates the facts of mortality, dying, and death into HCI research and design.
The effects of life disruptions on home technology routines	Conflict and disruption are a part of everyday life, yet research in the home largely examines consensus and rituals. In this paper, we use Holmes and Rahe's categorization of major life events in order to investigate disruption within the home. We examine posts contributed to an online technology support board and show how life disruptions fundamentally impact technology practices and routines. We conclude that examining technology in the context of life disruption is a worthwhile area for further work.
Motionbeam: a metaphor for character interaction with handheld projectors	We present the MotionBeam metaphor for character interaction with handheld projectors. Our work draws from the tradition of pre-cinema handheld projectors that use direct physical manipulation to control projected imagery. With our prototype system, users interact and control projected characters by moving and gesturing with the handheld projector itself. This creates a unified interaction style where input and output are tied together within a single device. We introduce a set of interaction principles and present prototype applications that provide clear examples of the MotionBeam metaphor in use. Finally we describe observations and insights from a preliminary user study with our system.
Interaction with a Projection Screen Using a Camera-tracked Laser Pointer	A system for direct interaction with a video projection screen using a laser pointer is presented. The laser point on the screen is captured by a video camera, and its location recognized by image processing techniques. The behavior of the point is translated into signals sent to the mouse input of the computer causing the same reactions as if they came from the mouse. More complex interaction paradigms are composed from the elementary operations "switch on/off" and pointing of the laser pen.
Interacting with dynamically defined information spaces using a handheld projector and a pen	The recent trend towards miniaturization of projection technology indicates that handheld devices will soon have the ability to project information onto any surface, thus enabling interfaces that are not possible with current handhelds. We explore the design space of dynamically defining and interacting with multiple virtual information spaces embedded in a physical environment using a handheld projector and a passive pen tracked in 3D. We develop techniques for defining and interacting with these spaces, and explore usage scenarios.
A pre-history of handheld projector-based interaction	I present a pre-history of contemporary handheld projector-based interaction to inform the design of future interactive systems. I begin by documenting the two main types of pre-cinema handheld projection from Europe and Japan, the handheld magic lantern and the utsushi-e performance. I then present a summary of projection techniques used by performers when interacting with these devices. I situate these techniques within contemporary research and illustrate how they are being used and built upon with contemporary technology. Finally, I discuss how knowledge of pre-cinema handheld projection devices, techniques, and performance can inform the design of future handheld projector systems.
CoGAME: manipulation using a handheld projector	An example of an application enhanced by the "manipulation-by-projection" technique, this cooperative game allows players to visually and intuitively control a robot with projectors. Players interchangeably move and connect their projected images to create a path that leads the robot to its goal.
The new good: exploring the potential of philosophy of technology to contribute to human-computer interaction	As a result of the increased interest in issues such as engagement, affection, and meaning, contemporary human-computer interaction (HCI) has increasingly come to examine the nature of interactions between artifacts, humans, and environments through concepts such as user experience and meaning. In the transition from usability metrics to user experience, what appears lacking is a more explicit characterization of what it is HCI now strives for as a discipline--i.e. what constitutes a 'good' user experience? Through a detailed look at two contemporary philosophies of technology--Albert Borgmann's notion of the device paradigm and Don Ihde's non-neutrality of technology-mediated experience--this paper seeks to explore the potential of the philosophy of technology to contribute new insights and provide well-grounded conceptual tools for coming to terms with what may become HCI's 'new good'.
Avaaj Otalo: a field study of an interactive voice forum for small farmers in rural India	In this paper we present the results of a field study of Avaaj Otalo (literally, "voice stoop"), an interactive voice application for small-scale farmers in Gujarat, India. Through usage data and interviews, we describe how 51 farmers used the system over a seven month pilot deployment. The most popular feature of Avaaj Otalo was a forum for asking questions and browsing others' questions and responses on a range of agricultural topics. The forum developed into a lively social space with the emergence of norms, persistent moderation, and a desire for both structured interaction with institutionally sanctioned authorities and open discussion with peers. For all 51 users this was the first experience participating in an online community of any sort. In terms of usability, simple menu-based navigation was readily learned, with users preferring numeric input over speech. We conclude by discussing implications of our findings for designing voice-based social media serving rural communities in India and elsewhere.
Values as lived experience: evolving value sensitive design in support of value discovery	The Value Sensitive Design (VSD) methodology provides a comprehensive framework for advancing a value-centered research and design agenda. Although VSD provides helpful ways of thinking about and designing value-centered computational systems, we argue that the specific mechanics of VSD create thorny tensions with respect to value sensitivity. In particular, we examine limitations due to value classifications, inadequate guidance on empirical tools for design, and the ways in which the design process is ordered. In this paper, we propose ways of maturing the VSD methodology to overcome these limitations and present three empirical case studies that illustrate a family of methods to effectively engage local expressions of values. The findings from our case studies provide evidence of how we can mature the VSD methodology to mitigate the pitfalls of classification and engender a commitment to reflect on and respond to local contexts of design.
Reflective design	As computing moves into every aspect of our daily lives, the values and assumptions that underlie our technical practices may unwittingly be propagated throughout our culture. Drawing on existing critical approaches in computing, we argue that reflection on unconscious values embedded in computing and the practices that it supports can and should be a core principle of technology design. Building on a growing body of work in critical computing, reflective design combines analysis of the ways in which technologies reflect and perpetuate unconscious cultural assumptions, with design, building, and evaluation of new computing devices that reflect alternative possibilities. We illustrate this approach through two design case studies.
Implications for design	Although ethnography has become a common approach in HCI research and design, considerable confusion still attends both ethnographic practice and the criteria by which it should be evaluated in HCI. Often, ethnography is seen as an approach to field investigation that can generate requirements for systems development; by that token, the major evaluative criterion for an ethnographic study is the implications it can provide for design. Exploring the nature of ethnographic inquiry, this paper suggests that "implications for design" may not be the best metric for evaluation and may, indeed, fail to capture the value of ethnographic investigations.
In-car gps navigation: engagement with and disengagement from the environment	Although in-car GPS navigation technology is proliferating, it is not well understood how its use alters the ways people interpret their environment and navigate through it. We argue that GPS-based car navigation might disengage people from their surrounding environment, but also has the potential to open up novel ways to engage with it. We present an ethnographically-informed study with GPS users, showing evidence for practices of disengagement as well as new opportunities for engagement, illustrating our findings using rich descriptions from the field. Grounded in our observations we propose design principles for GPS systems that support richer experiences of driving. We argue that for a fuller understanding of issues of disengagement and engagement with the environment we need to move beyond a focus on the (re)design of GPS devices, and point to future directions of work that embrace a broader perspective.
Skim reading by satisficing: evidence from eye tracking	Readers on the Web often skim through text to cope with the volume of available information. In a previous study, Duggan and Payne [11] tracked readers' eye movements as they skimmed through expository text under time pressure. This article presents novel analyses of these eye-movement data. Results indicated that readers were able to explicitly direct attention to the most important information in the text and that this improved performance on a subsequent test of memory for the meaning of text. We suggest readers achieve this by satisficing reading through text until the rate of information gain drops below threshold and then skipping to the next section of text. Further analyses of gaze patterns for paragraphs and pages supported this explanation. Combining satisficing with some form of scanning or sampling behaviour could explain patterns of reading found on the Web. A greater understanding of the way that text is read on the Web would assist many producers of online content.
Improving skim reading for document triage	When users seek for information, they repeatedly make relevance judgements on individual documents: the act of document triage. Recent research demonstrates that document triage decisions are prone to significant error rates. Document triage also affects the future course of information seeking: users form beliefs about the availability of information, determine new information goals and conclude others. Developing effective interactions to support document triage is therefore critical. This paper investigates how improve support for the quick review of a document, exploiting the principle of semantic zooming. We discover that applying semantic zooming improves the legibility of heading text during the rapid scrolling and overview reading that is associated with the earliest phases of document triage.
Hello, is grandma there? let's read! StoryVisit: family video chat and connected e-books	StoryVisit allows children and long-distance adults to experience a sense of togetherness by reading children's story books together over a distance. StoryVisit combines video conferencing and connected books: remote grown-up and child readers can see and hear each other, and can also see and control the same e-book. We report on research with 61 families - over 200 users including parents, children and long-distance readers - who used StoryVisit in their homes with a long-distance reader for at least one reading session. In addition, we report qualitative findings regarding nineteen of the families who participated in telephone interviews and four families who were monitored and interviewed by researchers at home. Results show that connected e-book video chat sessions last about five times as long as the typical video chats reported in previous research on families with young children. Moreover, the addition of an animated character increased session lengths by another 50%. StoryVisit usage peaked for families with three year olds, showing that sustained distance interactions with very young children are possible if communication technologies incorporate joint activities that engage children and adults.
Developing a media space for remote synchronous parent-child interaction	While supporting family communication has traditionally been a domain of interest for interaction designers, few research initiatives have explicitly investigated remote synchronous communication between children and parents. We discuss the design of the ShareTable, a media space that supports synchronous interaction with children by augmenting videoconferencing with a camera-projector system to allow for shared viewing of physical artifacts. We present an exploratory evaluation of this system, highlighting how such a media space may be used by families for learning and play activities. The ShareTable was positively received by our participants and preferred over standard videoconferencing. Informed by the results of our exploratory evaluation, we discuss the next design iteration of the ShareTable and directions for future investigations in this area.
Sharing conversation and sharing life: video conferencing in the home	Video conferencing is a technology that families and friends use to connect with each other over distance. However, even with such technology readily available, we still do not have a good understanding of how video conferencing systems are used by people as a part of their domestic communication practices. For this reason, we have conducted interviews with 21 adults in the United States to understand video conferencing routines in the home and to inform the design of future domestic communication technologies. Our findings illustrate the importance of discerning availability and willingness to video conference prior to calling, the need to share everyday life activities in addition to conversation, and a need for new privacy protecting strategies that focus on autonomy and solitude as opposed to confidentiality.
Family story play: reading with young children (and elmo) over a distance	We introduce Family Story Play, a system that supports grandparents to read books together with their grandchildren over the Internet. Family Story Play is designed to improve communication across generations and over a distance, and to support parents and grandparents in fostering the literacy development of young children. The interface encourages active child participation in the book reading experience by combining a paper book, a sensor-enhanced frame, video conferencing technology, and video content of a Sesame Street Muppet (Elmo). Results with users indicate that Family Story Play improves child engagement in long-distance communication and increases the quality of interaction between young children and distant grandparents. Additionally, Family Story Play encourages dialogic reading styles that are linked with literacy development. Ultimately, reading with Family Story Play becomes a creative shared activity that suggests a new kind of collaborative story telling.
Family storytelling for grandparents and grandchildren living apart	Grandparents may feel revitalized when a grandchild joins the family, but the physical separation that often exists between grandparents and grandchildren can make it difficult to develop a close relationship. Current communication technologies, such as the phone, are inadequate for developing close relationships with children. This paper presents the design, implementation and evaluation of a technology probe exploring how technology can be designed to alleviate this problem. Based on the evaluation, four important themes for designing technology for distributed intergenerational bonding are elicited and discussed. The four themes are Conversational Context (to have something to talk about), Facilitation (to be given the opportunity to talk), Diversified Interaction Forms (to maintain attention of the child) and Supporting Grandparent caring for grandchild (to adapt activity to the mood of the child).
Video play: playful interactions in video conferencing for long-distance families with young children	Long-distance families are increasingly staying connected with free video conferencing tools. However research has highlighted a need for shared activities for long-distance family communication. While video technology is reportedly superior to audio-only tools for children under age 7, the tools themselves are not designed to accommodate children's or families' needs. This paper introduces four design explorations of shared play activities over video conferencing that support family togetherness between children and remote adult family members. We build on research in CSCW and child development to create opportunities for silliness and open-ended play between adults and young children. Our goal is to scaffold interaction across distance and generations.
Making love in the network closet: the benefits and work of family videochat	In this paper, we explore the benefits of videochat for families and the corresponding work that home users engage in to make a video call run smoothly. We explore the varieties of social work required, including coordination work, presentation work, behavioral work, and scaffolding work, as well as the technical work necessary. We outline the benefits families enjoy for doing this work and discuss the ways in which families use videochat to reinforce their identity as a family and reinforce their family values, in effect making - as in creating - love. We conclude with recommendations for improving videochat and for designing with family values in mind more generally.
Family portals: connecting families through a multifamily media space	Video conferencing allows distance-separated family members to interact somewhat akin to being together at the same place and time. Yet most video conferencing systems are designed for phone-like calls between only two locations. Using such systems for long interactions or social gatherings with multiple families is cumbersome, if not impossible. For this reason, we wanted to explore how families would make use of a video system that permitted sharing everyday life over extended periods of time between multiple locations. We designed a media space called Family Portals that provides shared video between three locations and deployed it within the homes of six families. Results show that the media space increased feelings of connectedness and the focus on a triad, in contrast to a dyad, caused new styles of interaction to emerge. Despite this, families experienced new privacy challenges and non-adoption by some family members, not previously seen in dyadic family media spaces.
Developing a media space for remote synchronous parent-child interaction	While supporting family communication has traditionally been a domain of interest for interaction designers, few research initiatives have explicitly investigated remote synchronous communication between children and parents. We discuss the design of the ShareTable, a media space that supports synchronous interaction with children by augmenting videoconferencing with a camera-projector system to allow for shared viewing of physical artifacts. We present an exploratory evaluation of this system, highlighting how such a media space may be used by families for learning and play activities. The ShareTable was positively received by our participants and preferred over standard videoconferencing. Informed by the results of our exploratory evaluation, we discuss the next design iteration of the ShareTable and directions for future investigations in this area.
Sharing conversation and sharing life: video conferencing in the home	Video conferencing is a technology that families and friends use to connect with each other over distance. However, even with such technology readily available, we still do not have a good understanding of how video conferencing systems are used by people as a part of their domestic communication practices. For this reason, we have conducted interviews with 21 adults in the United States to understand video conferencing routines in the home and to inform the design of future domestic communication technologies. Our findings illustrate the importance of discerning availability and willingness to video conference prior to calling, the need to share everyday life activities in addition to conversation, and a need for new privacy protecting strategies that focus on autonomy and solitude as opposed to confidentiality.
A face(book) in the crowd: social Searching vs. social browsing	Large numbers of college students have become avid Facebook users in a short period of time. In this paper, we explore whether these students are using Facebook to find new people in their offline communities or to learn more about people they initially meet offline. Our data suggest that users are largely employing Facebook to learn more about people they meet offline, and are less likely to use the site to initiate new connections.
Designing a technological playground: a field study of the emergence of play in household messaging	We present findings from a field study of Wayve, a situated messaging device for the home that incorporates handwriting and photography. Wayve was used by 24 households (some of whom were existing social networks of family and friends) over a three-month period. We consider the various types of playfulness that emerged during the study, both through the sending of Wayve messages and through the local display of photos and notes. The findings are explored in the context of the literature on play, with the aim of identifying aspects of Wayve's design, as well as the context in which it was used, that engendered playfulness. We also highlight the role of play in social relationships, before concluding with design implications.
Family story play: reading with young children (and elmo) over a distance	We introduce Family Story Play, a system that supports grandparents to read books together with their grandchildren over the Internet. Family Story Play is designed to improve communication across generations and over a distance, and to support parents and grandparents in fostering the literacy development of young children. The interface encourages active child participation in the book reading experience by combining a paper book, a sensor-enhanced frame, video conferencing technology, and video content of a Sesame Street Muppet (Elmo). Results with users indicate that Family Story Play improves child engagement in long-distance communication and increases the quality of interaction between young children and distant grandparents. Additionally, Family Story Play encourages dialogic reading styles that are linked with literacy development. Ultimately, reading with Family Story Play becomes a creative shared activity that suggests a new kind of collaborative story telling.
Interpersonal awareness in the domestic realm	Family and friends naturally maintain an awareness of each other on an ongoing basis (e.g., knowing one's schedule, health issues) and many technologies are now being contemplated to help fulfill these needs. We use findings from a contextual study along with related work to present interpersonal awareness--a spectrum that differentiates how people desire and gather awareness for individuals across three different social groupings: home inhabitants, intimate socials, and extended socials. We compare this spectrum to workplace awareness and discuss how our study findings can be used to analyze and design domestic awareness technologies.
SPARCS: exploring sharing suggestions to enhance family connectedness	Staying in touch with extended family members can be a challenge in part because of the time and effort required, even with the help of current technologies. To explore the value of sharing suggestions in sparking communication and facilitating sharing between extended families, we iteratively built SPARCS, a prototype that encourages frequent sharing of photos and calendar information between extended families. Results from a five-week field study with 7 pairs of families highlight a number of important features for an ideal sharing system to help families stay connected, including asynchronous chat and easily configurable sharing suggestions.
Exploring communication and sharing between extended families	In recent years, computer and Internet technologies have broadened the ways that people can stay in touch. Through interviews with parents and grandparents, we examined how people use existing technologies to communicate and share with their extended family. While most of our participants expressed a desire for more communication and sharing with their extended family, many felt that an increase would realistically be difficult to achieve due to challenges such as busy schedules or extended family members' lack of technology use. Our results also highlight the complexity of factors that researchers and designers must understand when attempting to design technology to support and enhance relationships, including trade-offs between facilitating interaction while minimizing new obligations, reducing effort without trivializing communication, and balancing awareness with privacy.
Making love in the network closet: the benefits and work of family videochat	In this paper, we explore the benefits of videochat for families and the corresponding work that home users engage in to make a video call run smoothly. We explore the varieties of social work required, including coordination work, presentation work, behavioral work, and scaffolding work, as well as the technical work necessary. We outline the benefits families enjoy for doing this work and discuss the ways in which families use videochat to reinforce their identity as a family and reinforce their family values, in effect making - as in creating - love. We conclude with recommendations for improving videochat and for designing with family values in mind more generally.
Changes in use and perception of facebook	As social computing systems persist over time, the user experiences and interactions they support may change. One type of social computing system, Social Network Sites (SNSs), are becoming more popular across broad segments of Internet users. Facebook, in particular, has very broad participation amongst college attendees, and has been growing in other populations as well. This paper looks at how use of Facebook has changed over time, as indicated by three consecutive years of survey data and interviews with a subset of survey respondents. Reported uses of the site remain relatively constant over time, but the perceived audience for user profiles and attitudes about the site show differences over the study period.
Using fast interaction to create intense experiences	Several emerging strands of HCI involve connecting physical exercise activity with digital interactive systems to create intense combined experiences, for example pervasive games, GPS based exercise games and 'exertion interfaces'. Many of these systems are mobile, used outside in public, whilst moving quickly through the environment. In this paper, we argue that the combination of moving fast and interacting with a digital system allows us to create a powerfully intense experience for participants, and that key to this is careful attention to the way in which movement is combined with digital content. We study an interactive art experience in which a person runs whilst listening to poetry. Based on this study and other HCI research, we present a framework for mixing physical and interactive content, based on 3 dimensions, which describe ways that a movement activity may itself create intense experiences, followed by a set of tactics for combining intense movement and interactive content.
On being supple: in search of rigor without rigidity in meeting new design and evaluation challenges for HCI practitioners	In this paper, we argue that HCI practitioners are facing new challenges in design and evaluation that can benefit from the establishment of commonly valued use qualities, with associated strategies for producing and rigorously evaluating work. We present a particular use quality 'suppleness' as an example. We describe ways that use qualities can help shape design and evaluation process, and propose tactics for the CHI community to use to encourage the evolution of bodies of knowledge around use qualities.
The Frame of the Game: Blurring the Boundary between Fiction and Reality in Mobile Experiences	Mobile experiences that take place in public settings such as on city streets create new opportunities for interweaving the fictional world of a performance or game with the everyday physical world. A study of a touring performance reveals how designers generated excitement and dramatic tension by implicating bystanders and encouraging the (apparent) crossing of normal boundaries of behaviour. The study also shows how designers dealt with associated risks through a process of careful orchestration. Consequently, we extend an existing framework for designing spectator interfaces with the concept of performance frames, enabling us to distinguish audience from bystanders. We conclude that using ambiguity to blur the frame can be a powerful design tactic, empowering players to willingly suspend disbelief, so long as a safety-net of orchestration ensures that they do not stray into genuine difficulty.
Evaluating effects of structural holds on pointing and dragging performance with flexible displays	In this paper, we present a study of the effects of structural holds and rigidity of a flexible display on touch pointing and dragging performance. We discuss an observational study in which we collected common holds used when pointing on a mockup paper display. We also measured the force patterns each hold generated within the display surface. We analyzed this data to produce 3 force zones in the display for each of the four most frequently observed holds: the grip zone, rigid zone, and the flexible zone. We report on an empirical evaluation in which we compared the efficiency of pointing and dragging operations between holds, and between structural zones within holds, using a real flexible Lumalive display. Results suggest that structural force distributions in a flexible display affect the Index of Performance of both pointing and dragging tasks, irrespective of hold, with rigid parts of the display yielding a 12% average performance gain over flexible areas.
How users manipulate deformable displays as input devices	This study is aimed at understanding deformation-based user gestures by observing users interacting with artificial deformable displays with various levels of flexibility. We gained user-defined gestures that would help with the design and implementation of deformation-based interface, without considering current technical limitations. We found that when a display material gave more freedom from deformation, the level of consensus of gestures among the users as well as the intuitiveness and preferences were all enhanced. This study offers implications for deformation-based interaction which will be helpful for both designers and engineers who are trying to set the direction for future interface and technology development.
Paper windows: interaction techniques for digital paper	In this paper, we present Paper Windows, a prototype windowing environment that simulates the use of digital paper displays. By projecting windows on physical paper, Paper Windows allows the capturing of physical affordances of paper in a digital world. The system uses paper as an input device by tracking its motion and shape with a Vicon Motion Capturing System. We discuss the design of a number of interaction techniques for manipulating information on paper displays.
Interconnecting Smart Objects with IP: The Next Internet	Smart object technology, sometimes called the Internet of Things, is having a profound impact on our day-to-day lives. Interconnecting Smart Objects with IP is the first book that takes a holistic approach to the revolutionary area of IP-based smart objects. Smart objects are the intersection of networked embedded systems, wireless sensor networks, ubiquitous and pervasive computing, mobile telephony and telemetry, and mobile computer networking. This book consists of three parts, Part I focuses on the architecture of smart objects networking, Part II covers the hardware, software, and protocols for smart objects, and Part III provides case studies on how and where smart objects are being used today and in the future. The book covers the fundamentals of IP communication for smart objects, IPv6, and web services, as well as several newly specified low-power IP standards such as the IETF 6LoWPAN adaptation layer and the RPL routing protocol. This book contains essential information not only for the technical reader but also for policy makers and decision makers in the area of smart objects both for private IP networks and the Internet.Shows in detail how connecting smart objects impacts our lives with practical implementation examples and case studies Provides an in depth understanding of the technological and architectural aspects underlying smart objects technology Offers an in-depth examination of relevant IP protocols to build large scale smart object networks in support of a myriad of new services Table of ContentsPart I: The Architecture Chapter 1: What are Smart objects? Chapter 2: The IP protocol architecture Chapter 3: Why IP for smart objects? Chapter 4: IPv6 for Smart Object Networks and The Internet of Things Chapter 5: Routing Chapter 6: Transport Protocols Chapter 7: Service Discovery Chapter 8: Security for Smart Objects Chapter 9: Web services For Smart Objects Chapter 10: Connectivity models for smart object networks Part II: The Technology Chapter 11: What is a Smart Object? Chapter 12: Low power link layer for smart objects networks Chapter 13: uIP A Lightweight IP Stack Chapter 14: Standardization Chapter 15: IPv6 for Smart Object Networks - A Technology Refresher Chapter 16: The 6LoWPAN Adaptation Layer Chapter 17: RPL Routing in Smart Object Networks Chapter 18: The IPSO Alliance Chapter 19: Non IP Technology Part III: The Applications Chapter 20: Smart Grid Chapter 21: Industrial Automation Chapter 22: Smart Cities and Urban Networks Chapter 23: Home Automation Chapter 24: Building Automation Chapter 25: Structural Health Monitoring Chapter 26: Container Tracking
Building efficient wireless sensor networks with low-level naming	In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses) and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.
An adaptive communication architecture for wireless sensor networks	As sensor networks move towards increasing heterogeneity, the number of link layers, MAC protocols, and underlying transportation mechanisms increases. System developers must adapt their applications and systems to accommodate a wide range of underlying protocols and mechanisms. However, existing communication architectures for sensor networks are not designed for this heterogeneity and therefore the system developer must redevelop their systems for each underlying communication protocol or mechanism. To remedy this situation, we present a communication architecture that adapts to a wide range of underlying communication mechanisms, from the MAC layer to the transport layer, without requiring any changes to applications or protocols. We show that the architecture is expressive enough to accommodate typical sensor network protocols. Measurements show that the increase in execution time over a non-adaptive architecture is small.
Making sensor networks IPv6 ready	With emerging IPv6-based standards such as 6LowPAN and ISA100a, full IPv6 sensor networks are the next major step. With millions of deployed embedded IPv6 devices, interoperability is of major importance, both within the sensor networks and between the sensors and the Internet hosts. We present uIPv6, the first IPv6 stack for memory-constrained devices that passes all Phase-1 IPv6 Ready certification tests. This is an important step for end-to-end interoperability between IPv6 sensors and any IPv6 capable device. To allow widespread community adoption, we release uIPv6 under a permissive open source license that allows both commercial and non-commercial use.
IP is dead, long live IP for wireless sensor networks	A decade ago as wireless sensor network research took off many researchers in the field denounced the use of IP as inadequate and in contradiction to the needs of wireless sensor networking. Since then the field has matured, standard links have emerged, and IP has evolved. In this paper, we present the design of a complete IPv6-based network architecture for wireless sensor networks. We validate the architecture with a production-quality implementation that incorporates many techniques pioneered in the sensor network community, including duty-cycled link protocols, header compression, hop-by-hop forwarding, and efficient routing with effective link estimation. In addition to providing interoperability with existing IP devices, this implementation was able to achieve an average duty-cycle of 0.65%, average per-hop latency of 62ms, and a data reception rate of 99.98% over a period of 4 weeks in a real-world home-monitoring application where each node generates one application packet per minute. Our results outperform existing systems that do not adhere to any particular standard or architecture. In light of this demonstration of full IPv6 capability, we review the central arguments that led the field away from IP. We believe that the presence of an architecture, specifically an IPv6-based one, provides a strong foundation for wireless sensor networks going forward.
X-MAC: a short preamble MAC protocol for duty-cycled wireless sensor networks	In this paper we present X-MAC, a low power MAC protocol for wireless sensor networks (WSNs). Standard MAC protocols developed for duty-cycled WSNs such as BMAC, which is the default MAC protocol for TinyOS, employ an extended preamble and preamble sampling. While this "low power listening" approach is simple, asynchronous, and energy-efficient, the long preamble introduces excess latency at each hop, is suboptimal in terms of energy consumption, and suffers from excess energy consumption at nontarget receivers. X-MAC proposes solutions to each of these problems by employing a shortened preamble approach that retains the advantages of low power listening, namely low power communication, simplicity and a decoupling of transmitter and receiver sleep schedules. We demonstrate through implementation and evaluation in a wireless sensor testbed that X-MAC's shortened preamble approach significantly reduces energy usage at both the transmitter and receiver, reduces per-hop latency, and offers additional advantages such as flexible adaptation to both bursty and periodic sensor data sources.
Macro-programming wireless sensor networks using Kairos	The literature on programming sensor networks has focused so far on providing higher-level abstractions for expressing local node behavior. Kairos is a natural next step in sensor network programming in that it allows the programmer to express, in a centralized fashion, the desired global behavior of a distributed computation on the entire sensor network. Kairos’ compile-time and runtime subsystems expose a small set of programming primitives, while hiding from the programmer the details of distributed-code generation and instantiation, remote data access and management, and inter-node program flow coordination. In this paper, we describe Kairos’ programming model, and demonstrate its suitability, through actual implementation, for a variety of distributed programs—both infrastructure services and signal processing tasks—typically encountered in sensor network literature: routing tree construction, localization, and object tracking. Our experimental results suggest that Kairos does not adversely affect the performance or accuracy of distributed programs, while our implementation experiences suggest that it greatly raises the level of abstraction presented to the programmer.
Efficient application integration in IP-based sensor networks	Sensor networks are seen as an important part in emerging office and building energy management system, but the integration of sensor networks with future energy management systems is still an open problem. We present an IP-based sensor network system where nodes communicate their information using Web services, allowing direct integration in modern IT systems. Our system uses two mechanisms to provide a good performance and low-power operation: a session-aware power-saving radio protocol and the use of the HTTP Conditional GET mechanism. We perform an extensive evaluation of our system and show that Web services are a viable mechanism for use in low-power sensor networks. Our results show that Web service requests can be completed well below one second and with a low power consumption, even in a multi-hop setting.
Increasing ZigBee network lifetime with X-MAC	The ZigBee standard builds on the assumption that infrastructure nodes have a constant power supply. ZigBee therefore does not provide any power-saving mechanisms for routing nodes, which limits the lifetime of battery-powered ZigBee networks to a few days. This short lifetime drastically restricts the possible application scenarios for ZigBee. To expand the usefulness of ZigBee, we present a ZigBee implementation where we replace the default ZigBee MAC protocol with the power-saving MAC protocol X-MAC. Our results show that X-MAC reduces the power consumption for ZigBee routing nodes with up to 90%, leading to a ten-fold increase in network lifetime at the price of a slight increase in network latency. Furthermore, we are the first to experimentally quantify the energy-efficiency of X-MAC in a multihop scenario. Our results indicate that X-MAC reduces the power consumption of idle nodes that are within communication range of two communicating nodes, suggesting that X-MAC may be able to mitigate the hot-spot problem in sensor networks.
Protothreads: simplifying event-driven programming of memory-constrained embedded systems	Event-driven programming is a popular model for writing programs for tiny embedded systems and sensor network nodes. While event-driven programming can keep the memory overhead down, it enforces a state machine programming style which makes many programs difficult to write, maintain, and debug. We present a novel programming abstraction called protothreads that makes it possible to write event-driven programs in a thread-like style, with a memory overhead of only two bytes per protothread. We show that protothreads significantly reduce the complexity of a number of widely used programs previously written with event-driven state machines. For the examined programs the majority of the state machines could be entirely removed. In the other cases the number of states and transitions was drastically decreased. With protothreads the number of lines of code was reduced by one third. The execution time overhead of protothreads is on the order of a few processor cycles.
Enabling large-scale storage in sensor networks with the Coffee file system	Persistent storage offers multiple advantages for sensor networks, yet the available storage systems have been unwieldy because of their complexity and device-specific designs. We present the Coffee file system for flash-based sensor devices. Coffee provides a programming interface for building efficient and portable storage abstractions. Unlike previous flash file systems, Coffee uses a small and constant RAM footprint per file, making it scale elegantly with workloads consisting of large files or many files. In addition, the performance overhead of Coffee is low: the throughput is at least 92% of the achievable direct flash driver throughput. We show that network layer components such as routing tables and packet queues can be implemented on top of Coffee, leading to increased performance and reduced memory requirements for routing and transport protocols.
TAG: a Tiny AGgregation service for Ad-Hoc sensor networks	We present the Tiny AGgregation (TAG) service for aggregation in low-power, distributed, wireless environments. TAG allows users to express simple, declarative queries and have them distributed and executed efficiently in networks of low-power, wireless sensors. We discuss various generic properties of aggregates, and show how those properties affect the performance of our in network approach. We include a performance study demonstrating the advantages of our approach over traditional centralized, out-of-network methods, and discuss a variety of optimizations for improving the performance and fault-tolerance of the basic solution.
Collection tree protocol	This paper presents and evaluates two principles for wireless routing protocols. The first is datapath validation: data traffic quickly discovers and fixes routing inconsistencies. The second is adaptive beaconing: extending the Trickle algorithm to routing control traffic reduces route repair latency and sends fewer beacons. We evaluate datapath validation and adaptive beaconing in CTP Noe, a sensor network tree collection protocol. We use 12 different testbeds ranging in size from 20--310 nodes, comprising seven platforms, and six different link layers, on both interference-free and interference-prone channels. In all cases, CTP Noe delivers 90% of packets. Many experiments achieve 99.9%. Compared to standard beaconing, CTP Noe sends 73% fewer beacons while reducing topology repair latency by 99.8%. Finally, when using low-power link layers, CTP Noe has duty cycles of 3% while supporting aggregate loads of 30 packets/minute.
2d touching of 3d stereoscopic objects	Recent developments in the area of touch and display technologies have suggested to combine multi-touch systems and stereoscopic visualization. Stereoscopic perception requires each eye to see a slightly different perspective of the same scene, which results in two distinct projections on the display. Thus, if the user wants to select a 3D stereoscopic object in such a setup, the question arises where she would touch the 2D surface to indicate the selection. A user may apply different strategies, for instance touching the midpoint between the two projections, or touching one of them. In this paper we analyze the relation between the 3D positions of stereoscopically rendered objects and the on-surface touch points, where users touch the surface. We performed an experiment in which we determined the positions of the users' touches for objects, which were displayed with positive, negative or zero parallaxes. We found that users tend to touch between the projections for the two eyes with an offset towards the projection for the dominant eye. Our results give implications for the development of future touch-enabled interfaces, which support 3D stereoscopic visualization.
Sphere: multi-touch interactions on a spherical display	Sphere is a multi-user, multi-touch-sensitive spherical display in which an infrared camera used for touch sensing shares the same optical path with the projector used for the display. This novel configuration permits: (1) the enclosure of both the projection and the sensing mechanism in the base of the device, and (2) easy 360-degree access for multiple users, with a high degree of interactivity without shadowing or occlusion. In addition to the hardware and software solution, we present a set of multi-touch interaction techniques and interface concepts that facilitate collaborative interactions around Sphere. We designed four spherical application concepts and report on several important observations of collaborative activity from our initial Sphere installation in three high-traffic locations.
Bimanual Interaction with Interscopic Multi-Touch Surfaces	Multi-touch interaction has received considerable attention in the last few years, in particular for natural two-dimensional (2D) interaction. However, many application areas deal with three-dimensional (3D) data and require intuitive 3D interaction techniques therefore. Indeed, virtual reality (VR) systems provide sophisticated 3D user interface, but then lack efficient 2D interaction, and are therefore rarely adopted by ordinary users or even by experts. Since multi-touch interfaces represent a good trade-off between intuitive, constrained interaction on a touch surface providing tangible feedback, and unrestricted natural interaction without any instrumentation, they have the potential to form the foundation of the next generation user interface for 2D as well as 3D interaction. In particular, stereoscopic display of 3D data provides an additional depth cue, but until now the challenges and limitations for multi-touch interaction in this context have not been considered. In this paper we present new multi-touch paradigms and interactions that combine both traditional 2D interaction and novel 3D interaction on a touch surface to form a new class of multi-touch systems, which we refer to as interscopic multi-touch surfaces (iMUTS). We discuss iMUTS-based user interfaces that support interaction with 2D content displayed in monoscopic mode and 3D content usually displayed stereoscopically. In order to underline the potential of the proposed iMUTS setup, we have developed and evaluated two example interaction metaphors for different domains. First, we present intuitive navigation techniques for virtual 3D city models, and then we describe a natural metaphor for deforming volumetric datasets in a medical context.
A screen-space formulation for 2D and 3D direct manipulation	Rotate-Scale-Translate (RST) interactions have become the de facto standard when interacting with two-dimensional (2D) contexts in single-touch and multi-touch environments. Because the use of RST has thus far focused almost entirely on 2D, there are not yet standard techniques for extending these principles into three dimensions. In this paper we describe a screen-space method which fully captures the semantics of the traditional 2D RST multi-touch interaction, but also allows us to extend these same principles into three-dimensional (3D) interaction. Just like RST allows users to directly manipulate 2D contexts with two or more points, our method allows the user to directly manipulate 3D objects with three or more points. We show some novel interactions, which take perspective into account and are thus not available in orthographic environments. Furthermore, we identify key ambiguities and unexpected behaviors that arise when performing direct manipulation in 3D and offer solutions to mitigate the difficulties each presents. Finally, we show how to extend our method to meet application-specific control objectives, as well as show our method working in some example environments.
Selection using a one-eyed cursor in a fish tank VR environment	This study investigates the use of a 2D cursor presented to one eye for target selection in Fish Tank VR and other stereo environments. It is argued that 2D selection of 3D objects should be less difficult than 3D selection. Vision research concerning binocular rivalry and the tendency we have to project images onto surfaces suggests that this mode of viewing will not seem particularly unnatural. A Fitt's Law experiment was done to directly compare target acquisition with a one-eyed 2D cursor and target acquisition using a 3D cursor. In both cases we used the same input device (Polhemus Fastrak) so that the device lag and gain parameters were exactly matched. The results show a large improvement in target acquisition time using the 2D cursor. The practical implications of this is that the 2D selection method using a one-eyed cursor in preferable to the 3D selection method. Theoretical implications relate to methods for extending Fitts' Law from the one-dimensional task for which it was designed to 2D and 3D tasks. We conclude that the existing approaches to this problem are not adequate.
Shallow-depth 3d interaction: design and evaluation of one-, two- and three-touch techniques	On traditional tables, people frequently use the third dimension to pile, sort and store objects. However, while effective and informative for organization, this use of the third dimension does not usually extend far above the table. To enrich interaction with digital tables, we present the concept of shallow-depth 3D -- 3D interaction with limited depth. Within this shallow-depth 3D environment several common interaction methods need to be reconsidered. Starting from any of one, two and three touch points, we present interaction techniques that provide control of all types of 3D rotation coupled with translation (6DOF) on a direct-touch tabletop display. The different techniques exemplify a wide range of interaction possibilities: from the one-touch technique, which is designed to be simple and natural, but inherits a degree of imprecision from its simplicity; through to three-touch interaction, which allows precise bimanual simultaneous control of multiple degrees of freedom, but at the cost of simplicity. To understand how these techniques support interaction in shallow-depth 3D, we present a user study that examines the efficiency of, and preferences for, the techniques developed. Results show that users are fastest and most accurate when using the three-touch technique and that their preferences were also strongly in favour of the expressive power available from three-touch.
Precise selection techniques for multi-touch screens	The size of human fingers and the lack of sensing precision can make precise touch screen interactions difficult. We present a set of five techniques, called Dual Finger Selections, which leverage the recent development of multi-touch sensitive displays to help users select very small targets. These techniques facilitate pixel-accurate targeting by adjusting the control-display ratio with a secondary finger while the primary finger controls the movement of the cursor. We also contribute a "clicking" technique, called SimPress, which reduces motion errors during clicking and allows us to simulate a hover state on devices unable to sense proximity. We implemented our techniques on a multi-touch tabletop prototype that offers computer vision-based tracking. In our formal user study, we tested the performance of our three most promising techniques (Stretch, X-Menu, and Slider) against our baseline (Offset), on four target sizes and three input noise levels. All three chosen techniques outperformed the control technique in terms of error rate reduction and were preferred by our participants, with Stretch being the overall performance and preference winner.
Touching floating objects in projection-based virtual reality environments	Touch-sensitive screens enable natural interaction without any instrumentation and support tangible feedback on the touch surface. In particular multi-touch interaction has proven its usability for 2D tasks, but the challenges to exploit these technologies in virtual reality (VR) setups have rarely been studied. In this paper we address the challenge to allow users to interact with stereoscopically displayed virtual environments when the input is constrained to a 2D touch surface. During interaction with a large-scale touch display a user changes between three different states: (1) beyond the arm-reach distance from the surface, (2) at arm-reach distance and (3) interaction. We have analyzed the user's ability to discriminate stereoscopic display parallaxes while she moves through these states, i. e., if objects can be imperceptibly shifted onto the interactive surface and become accessible for natural touch interaction. Our results show that the detection thresholds for such manipulations are related to both user motion and stereoscopic parallax, and that users have problems to discriminate whether they touched an object or not, when tangible feedback is expected.
A geotemporal role-based authorisation system	Geospatial databases include any data with reference to geocoordinate information. The geospatial data can either be digital raster images that represent the data on the earth in the form of pixels or digital vector data that is primarily from satellites. Due to the fact that many of the high-resolution satellites are commercial in nature, uncontrolled dissemination of the high resolution imagery may cause severe threats to national security as well as personal privacy. The severity of the threats is even more significant when this information is combined with vector maps or other publicly available vector data. In this paper, we present a GeoSpatial Authorisation System (GSAS), which is based on a GeoSpatial Authorisation Model (GSAM), for specifying and enforcing access control policies that makes reference to the spatial regions and locational credentials. The specification of authorisations is based on the spatial and temporal attributes associated with the image data, resolution of the images, geospatial credentials associated with users and privilege modes including view, zoom-in, overlay, view-thumbnail, view-annotation, identify, animate and fly-by that are relevant for geospatial image data. We present the GSAS system and its functionalities.
An authorization model for temporal and derived data: securing information portals	The term information portals refers to Web sites that serve as main providers of focused information, gathered from distributed data sources. Gathering and disseminating information through information portals introduce new security challenges. In particular, the authorization specifications, as well as the granting process, are temporal by nature. Also, more often than not, the information provided by the portal is in fact derived from more than one backend data source. Therefore, any authorization model for information portals should support access control based on temporal characteristics of the data, and also should provide tools to prevent indirect unauthorized access through the use of derived data. In this article we focus our attention on devising such an authorization model. The distinguishing features of this model include: (1) the specification of authorizations based on temporal characteristics of data, and (2) a formal framework to derive authorizations in a consistent and safe manner, based on relationships among data.
An Authorization Model for a Distributed Hypertext System	Digital libraries support quick and efficient access to a large number of information sources that are distributed but interlinked. As the amount of information to be shared grows, the need to restrict access only to specific users or for specific usage will surely arise. The protection of information in digital libraries, however, is difficult because of the peculiarity of the hypertext paradigm which is generally used to represent information in digital libraries, together with the fact that related data in a hypertext are often distributed at different sites. In this paper, we present an authorization model for distributed hypertext systems. Our model supports authorizations at different granularity levels, takes into consideration different types of data and the relationships among them, and allows administrative privileges to be delegated.
GEO-RBAC: a spatially aware RBAC	Securing access to data in location-based services and mobile applications requires the definition of spatially aware access control systems. Even if some approaches have already been proposed either in the context of geographic database systems or context-aware applications, a comprehensive framework, general and flexible enough to cope with spatial aspects in real mobile applications, is still missing. In this paper, we make one step towards this direction and we present GEO-RBAC, an extension of the RBAC model to deal with spatial and location-based information. In GEO-RBAC, spatial entities are used to model objects, user positions, and geographically bounded roles. Roles are activated based on the position of the user. Besides a physical position, obtained from a given mobile terminal or a cellular phone, users are also assigned a logical and device independent position, representing the feature (the road, the town, the region) in which they are located. To make the model more flexible and re-usable, we also introduce the concept of role schema, specifying the name of the role as well as the type of the role spatial boundary and the granularity of the logical position. We then extend GEO-RBAC to cope with hierarchies, modeling permission, user, and activation inheritance.
Dynamic Context-aware Access Control for Grid Applications	The emerging Grid infrastructure presents many challengesdue to its inherent heterogeneity, multi-domain characteristic,and highly dynamic nature. One critical challengeis providing authentication, authorization and accesscontrol guarantees. In this paper, we present the SESAMEdynamic context-aware access control mechanism for pervasiveGrid applications. SESAME complements currentauthorization mechanisms to dynamically grant and adaptpermissions to users based on their current context. The underlingdynamic role based access control (DRBAC) modelextends the classic role based access control (RBAC). Wealso present a prototype implementation of SESAME andDRBAC with the Discover computational collaboratory andan experimental evaluation of its overheads.
A Content-Based Authorization Model for Digital Libraries	Digital Libraries (DLs) introduce several challenging requirements with respect to the formulation, specification, and enforcement of adequate data protection policies. Unlike conventional database environments, a DL environment typically is characterized by dynamic user population, often making accesses from remote locations, and by an extraordinarily large amount of multimedia information, stored in a variety of formats. Moreover, in a DL environment, access policies are often specified based on user qualifications and characteristics, rather than user identity (for example, a user can be given access to an R-rated video only if he/she is older than 18 years). Another crucial requirement is the support for content-dependent authorizations on digital library objects (for example, all documents containing discussions on how to operate guns must be made available only to users who are 18 or older). Since traditional authorization models do not adequately meet access control requirements typical to DLs, in this paper, we propose a content-based authorization model suitable for a DL environment. Specifically, the most innovative features of our authorization model are: 1) flexible specification of authorizations based on the qualifications and characteristics of users (including positive and negative), 2) both content-dependent and content-independent access control to digital library objects, and 3) varying granularity of authorization objects ranging from sets of library objects to specific portions of objects.
Context sensitive access control	We investigate the practical feasibility of using context information for controlling access to services. Based solely on situational context, we show that users can be transparently provided anonymous access to services and that service providers can still impose various security levels. Thereto, we propose context-sensitive verification methods that allow checking the user's claimed authenticity in various ways and to various degrees. More precisely, conventional information management approaches are used to compare historic contextual (service usage) data of an individual user or group. The result is a relatively strong, less intrusive and more flexible access control process that mimics our natural way of authentication and authorization in the physical world.
Directing attention and influencing memory with visual saliency modulation	In augmented reality, it is often necessary to draw the user's attention to particular objects in the real world without distracting her from her task. We explore the effectiveness of directing a user's attention by imperceptibly modifying existing features of a video. We present three user studies of the effects of applying a saliency modulation technique to video; evaluating modulation awareness, attention, and memory. Our results validate the saliency modulation technique as an alternative means to convey information to the user, suggesting attention shifts and influencing recall of selected regions without perceptible changes to visual input.
Visual interest and NPR: an evaluation and manifesto	Using eye tracking, we study the way viewers look at photos and image based NPR illustrations. Viewers examine the same number of locations in photos and in NPR images with uniformly high or low detail. In contrast, viewers are attracted to areas where detail is locally preserved in meaningfully abstracted images. This accords with the idea that artists carefully manipulate detail to control interest and understanding. It also validates the method of meaningful abstraction used in DeCarlo and Santella [2002]. Results also suggest eye tracking can be a useful tool for evaluation of NPR systems.
Saliency-guided Enhancement for Volume Visualization	Recent research in visual saliency has established a computational measure of perceptual importance. In this paper we present a visual-saliency-based operator to enhance selected regions of a volume. We show how we use such an operator on a user-specified saliency field to compute an emphasis field. We further discuss how the emphasis field can be integrated into the visualization pipeline through its modifications of regional luminance and chrominance. Finally, we validate our work using an eye-tracking-based user study and show that our new saliency enhancement operator is more effective at eliciting viewer attention than the traditional Gaussian enhancement operator.
Teenagers and their virtual possessions: design opportunities and issues	Over the past several years, people have increasingly acquired virtual possessions. We consider these things to include artifacts that are increasingly becoming immaterial (e.g. books, photos, music, movies) and things that have never traditionally had a lasting material form (e.g. SMS archives, social networking profiles, personal behavior logs). To date, little research exists about how people value and form attachments to virtual possessions. To investigate, we conducted a study with 21 teenagers exploring the perceived value of their virtual possessions, and the comparative similarities and differences with their material things. Findings are interpreted to detail design and research opportunities and issues in this emerging space.
Opening up the family archive	The Family Archive device is an interactive multi-touch tabletop technology with integrated capture facility for the archiving of sentimental artefacts and memorabilia. It was developed as a technology probe to help us open up current family archiving practices and to explore family archiving in situ. We detail the deployment and study of three of these devices in family homes and discuss how deploying a new, potentially disruptive, technology can foreground the social relations and organizing systems in domestic life. This in turn facilitates critical reflection on technology design.
Aesthetics and experience-centered design	The aesthetics of human-computer interaction and interaction design are conceptualized in terms of a pragmatic account of human experience. We elaborate this account through a framework for aesthetic experience built around three themes: (1) a holistic approach wherein the person with feelings, emotions, and thoughts is the focus of design; (2) a constructivist stance in which self is seen as continuously engaged and constituted in making sense of experience; and (3) a dialogical ontology in which self, others, and technology are constructed as multiple centers of value. We use this framework to critically reflect on research into the aesthetics of interaction and to suggest sensibilities for designing aesthetic interaction. Finally, a digital jewelery case study is described to demonstrate a design approach that is open to the perspectives presented in the framework and to consider how the framework and sensibilities are reflected in engagement with participants and approach to design.
Life editing: third-party perspectives on lifelog content	Lifelog collections digitally capture and preserve personal experiences and can be mined to reveal insights and understandings of individual significance. These rich data sources also offer opportunities for learning and discovery by motivated third parties. We employ a custom-designed storytelling application in constructing meaningful lifelog summaries from third-party perspectives. This storytelling initiative was implemented as a core component in a university media-editing course. We present promising results from a preliminary study conducted to evaluate the utility and potential of our approach in creatively interpreting a unique experiential dataset.
An examination of a large visual lifelog	Lifelogging is the act of recording some aspect of your life in digital format. A basic and common form of lifelogging is the creation and maintenance of blogs, which are typically textual in nature, though often with multi-media elements. In this paper we are concerned with visual lifelogging, a new form of lifelogging based on the passive capture of photos of a person's experiences. We examine the nature of visual lifelogs, and the differences between visual lifelog photos and explicitly captured digital photos. This is done by examining a million lifelog photos encompassing a year of a visual lifelog from the life of one individual.
Review spotlight: a user interface for summarizing user-generated reviews using adjective-noun word pairs	Many people read online reviews written by other users to learn more about a product or venue. However, the overwhelming amount of user-generated reviews and variance in length, detail and quality across the reviews make it difficult to glean useful information. In this paper, we present the iterative design of our system, called Review Spotlight. It provides a brief overview of reviews using adjective-noun word pairs, and allows the user to quickly explore the reviews in greater detail. Through a laboratory user study which required participants to perform decision making tasks, we showed that participants could form detailed impressions about restaurants and decide between two options significantly faster with Review Spotlight than with traditional review webpages.
Opinion observer: analyzing and comparing opinions on the Web	The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly.
A multimedia interface for facilitating comparisons of opinions	Written opinion on products and other entities can be important to consumers and researchers, but expensive and difficult to analyze. We present a multimedia interface designed to facilitate the analysis of opinions on multiple entities, which could be beneficial to many individuals and organizations. It integrates an information visualization and an intelligent system that selects notable comparisons in the data and summarizes them in text. This system applies a set of statistics for comparing opinions across entities. We conducted a study of our interface with 36 subjects. Subjects liked the visualization overall and our system's selections overlapped with those of subjects more than did the selections of baseline systems. Given the choice, subjects often changed their selections to be more consistent with those of our system. This suggests that system selections were valuable to them.
The effects of task dimensionality, endpoint deviation, throughput calculation, and experiment design on pointing measures and models	Fitts' law (1954) characterizes pointing speed-accuracy performance as throughput, whose invariance to target distances (A) and sizes (W) is known. However, it is unknown whether throughput and Fitts' law models in general are invariant to task dimensionality (1-D vs. 2-D), whether univariate (SDx) or bivariate (SDx,y) endpoint deviation is used, whether throughput is calculated using the mean-of-means approach or the slope-inverse approach, or whether Guiard's (2009) Form - Scale experiment design is used instead of fully crossed A-W factors. We empirically investigate the confluence of these issues, finding that Fitts' law is largely invariant across 1-D and 2-D, provided that univariate endpoint deviation (SDx) is used in both, but that for 2-D pointing data, bivariate endpoint deviation (SDx,y) results in better Fitts' law models. Also, the mean-of-means throughput calculation exhibits lower variance across subjects and dimensionalities than the slope-inverse calculation. In light of these and other findings, we offer recommendations for pointing evaluations, especially in 2-D. We also offer an evaluation tool called Fitts Study to facilitate comparisons.
Fitts' throughput and the speed-accuracy tradeoff	We describe an experiment to test the hypothesis that Fitts' throughput is independent of the speed-accuracy tradeoff. Eighteen participants used a mouse in performing a total of 5,400 target selection trials. Comparing nominal, speed-emphasis, and accuracy-emphasis conditions, significant main effects were found on movement time (ms) and error rate (%), but not on throughput (bits/s). In the latter case, failure to reject the null hypothesis of "no significant difference" (i.e., .05
Speed-accuracy tradeoff in Fitts' law tasks: on the equivalency of actual and nominal pointing precision	Pointing tasks in human-computer interaction obey certain speed-accuracy tradeoff rules. In general, the more accurate the task to be accomplished, the longer it takes and vice versa. Fitts' law models the speed-accuracy tradeoff effect in pointing as imposed by the task parameters, through Fitts' index of difficulty (Id) based on the ratio of the nominal movement distance and the size of the target. Operating with different speed or accuracy biases, performers may utilize more or less area than the target specifies, introducing another subjective layer of speed-accuracy tradeoff relative to the task specification. A conventional approach to overcome the impact of the subjective layer of speed-accuracy tradeoff is to use the a posteriori "effective" pointing precision We in lieu of the nominal target width W. Such an approach has lacked a theoretical or empirical foundation. This study investigates the nature and the relationship of the two layers of speed-accuracy tradeoff by systematically controlling both Id and the index of target utilization Iu in a set of four experiments. Their results show that the impacts of the two layers of speed-accuracy tradeoff are not fundamentally equivalent. The use of We could indeed compensate for the difference in target utilization, but not completely. More logical Fitts' law parameter estimates can be obtained by the We adjustment, although its use also lowers the correlation between pointing time and the index of difficulty. The study also shows the complex interaction effect between Id and Iu, suggesting that a simple and complete model accommodating both layers of speed-accuracy tradeoff may not exist.
A probabilistic approach to modeling two-dimensional pointing	We investigate and model two-dimensional pointing where the target distance and size vary as does the angle of movement. We first study the spread of hits in a rapid approximate pointing task at varied distances and movement angles. Consistent with the literature, our results show that the spread of hits along the movement direction deviate more than the spread of hits in the direction perpendicular to movement, and both spreads increase with distance. Based on the distribution of this spread of hits, we propose and validate a new probabilistic model that describes two-dimensional pointing. Unlike previous models, our model accounts for more variables of two-dimensional pointing and can be generalized to any target shape, size, orientation, location, and dimension. In contrast to previous work, which suggests that target height has minimal impact on performance when it is larger than the width, our results show that, even when height is greater than width, it can significantly impact movement time.
Characterizing computer input with Fitts' law parameters: the information and non-information aspects of pointing	Throughput (TP), also known as index of performance or bandwidth in Fitts' law tasks, has been a fundamental metric in quantifying input system performance. The operational definition of TP is varied in the literature. In part thanks to the common interpretations of International Standard ISO 9241-9, the "Ergonomic requirements for office work with visual display terminals-- Part 9: Requirements for non-keyboard input devices", the measurements of throughput have increasingly converged onto the average ratio of index of difficulty (ID) and trial completion time (MT), i.e. TP=ID/MT. In lieu of the complete Fitts' law regression results that can only be represented by both slope (b) and intercept (a) (or MT=a+b ID), TP has been used as the sole performance characteristic of input devices, which is problematic. We show that TP defined as ID/ MT is an ill-defined concept that may change its value with the set of ID values used for the same input device and cannot be generalized beyond specific experimental target distances and sizes. The greater the absolute value of a is, the more variable TP (=ID/MT) is. ID/MT only equals a constant 1/b when a = 0. We suggest that future studies should use the complete Fitts' law regression characterized by (a, b) parameters to characterize an input system, a reflects the noninformational aspect and b the informational aspect of input performance. For convenience, 1/b can be named as throughput which, unlike ID/MT, is conceptually a true constant.
Refining Fitts' law models for bivariate pointing	We investigate bivariate pointing in light of the recent progress in the modeling of univariate pointing. Unlike previous studies, we focus on the effect of target shape (width and height ratio) on pointing performance, particularly when such a ratio is between 1 and 2. Results showed unequal impact of amplitude and directional constraints, with the former dominating the latter. Investigating models based on the notion of weighted Lp norm, we found that our empirical findings were best captured by an Euclidean model with one free weight. This model significantly outperforms the best model to date.
Into the wild: challenges and opportunities for field trial methods	Field trials of experimental systems in the wild have developed into a standard method within HCI - testing new systems with groups of users in relatively unconstrained settings outside of the laboratory. In this paper we discuss methodological challenges in running user trials. Using a trial of trials we examined the practices of investigators and participants - documenting demand characteristics, where users adjust their behaviour to fit the expectations of those running the trial, the interdependence of how trials are run and the result they produce, and how trial results can be dependent on the insights of a subset of trial participants. We develop three strategies that researchers can use to leverage these challenges to run better trials.
Living with tableau machine: a longitudinal investigation of a curious domestic intelligence	We present a longitudinal investigation of Tableau Machine, an intelligent entity that interprets and reflects the lives of occupants in the home. We created Tableau Machine (TM) to explore the parts of home life that are unrelated to accomplishing tasks. Task support for "smart homes" has inspired many researchers in the community. We consider design for experience, an orthogonal dimension to task-centric home life. TM produces abstract visualizations on a large LCD every few minutes, driven by a set of four overhead cameras that capture a sense of the social life of a domestic space. The openness and ambiguity of TM allow for a cycle of co-interpretation with householders. We report on three longitudinal deployments of TM for a period of six weeks. Participant families engaged with TM at the outset to understand how their behaviors were influencing the machine, and, while TM remained puzzling, householders interacted richly with TM and its images. We extract some key design implications for an experience-focused smart home.
Interweaving mobile games with everyday life	We introduce a location--based game called Feeding Yoshi that provides an example of seamful design, in which key characteristics of its underlying technologies-the coverage and security characteristics of WiFi-are exposed as a core element of gameplay. Feeding Yoshi is also a long--term, wide--area game, being played over a week between three different cities during an initial user study. The study, drawing on participant diaries and interviews, supported by observation and analysis of system logs, reveals players' reactions to the game. We see the different ways in which they embedded play into the patterns of their daily lives, augmenting existing practices and creating new ones, and observe the impact of varying location on both the ease and feel of play. We identify potential design extensions to Feeding Yoshi and conclude that seamful design provides a route to creating engaging experiences that are well adapted to their underlying technologies.
How HCI interprets the probes	We trace how cultural probes have been adopted and adapted by the HCI community. The flexibility of probes has been central to their uptake, resulting in a proliferation of divergent uses and derivatives. The varying patterns of adaptation of the probes reveal important underlying issues in HCI, suggesting underacknowledged disagreements about valid interpretation and the relationship between methods and their underlying methodology. With this analysis, we aim to clarify discussions around probes, and, more importantly, around how we define and evaluate methods in HCI, especially those grounded in unfamiliar conceptions of how research should be done.
Cultural commentators: Non-native interpretations as resources for polyphonic assessment	Designs for everyday life must be considered in terms of the many facets of experience they affect, including their aesthetics, emotional effects, genre, social niche, and cultural connotations. In this paper, I discuss the use of cultural commentators, people whose profession it is to inform and shape public opinion, as resources for multi-layered assessments of designs for everyday life. I describe our work with a team of movie screenwriters to help interpret the results of a Cultural Probe study, and with film-makers to document the experiences of people living with prototype designs in their homes. The value of employing cultural commentators is that they work outside our usual community of discourse, and are often accustomed to reflecting issues of aesthetics, emotions, social fit or cultural implication that are difficult to address from traditional HCI perspectives. They help to focus and articulate people's accounts of their experiences, extrapolating narratives from incomplete information, and dramatising relationships to create powerful and provocative stories. In so doing, they create the grounds for a polyphonic assessment of prototypes, in which a multiplicity of perspectives encourages a multi-layered assessment.
Anatomy of a failure: how we knew when our design went wrong, and what we learned from it	In this paper, we describe the failure of a novel sensor-based system intended to evoke user interpretation and appropriation in domestic settings. We contrast participants' interactions in this case study with those observed during more successful deployments to identify 'symptoms of failure' under four themes: engagement, reference, accommodation, and surprise and insight. These themes provide a set of sensitivities or orientations that may complement traditional task-based approaches to evaluation as well as the more open-ended ones we describe here. Our system showed symptoms of failure under each of these themes. We examine the reasons for this at three levels: problems particular to the specific design hypothesis; problems relevant for input-output mapping more generally; and problems in the design process we used. We conclude by noting that, although interpretive systems such as the one we describe here may succeed in a myriad of different ways, it is reassuring to know that they can also fail, and fail incontrovertibly, yet instructively.
Taking technomethodology seriously: hybrid change in the ethnomethodology-design relationship	The incorporation of ethnomethodology in professional systems development has prompted the call for the approach to move from design critique to design practice and the invention of the future. This has resulted in the development of a variety of mixing pot hybrids that have had marginal impact upon product-based development, whose needs the approach has been configured to meet. This paper suggests that a concern to fit ethnomethodology into product-based development life cycles is a primary source of the difficulties encountered in moving ethnomethodology from design critique to design practice. In practice, ethnomethodology is largely employed in research rather than product development settings. Recognition of the real-world uses of ethnomethodology in design practice opens up the possibility of devising a hybrid methodology that actively supports the invention of the future. Accordingly, this paper articulates a distinct socio-technical model that provides an iterative structure for the constructive involvement of ethnomethodology in processes of technological innovation, the results of which may subsequently be subject to the rationalities and constraints of product development.
Field trial of Tiramisu: crowd-sourcing bus arrival times to spur co-design	Crowd-sourcing social computing systems represent a new material for HCI designers. However, these systems are difficult to work with and to prototype, because they require a critical mass of participants to investigate social behavior. Service design is an emerging research area that focuses on how customers co-produce the services that they use, and thus it appears to be a great domain to apply this new material. To investigate this relationship, we developed Tiramisu, a transit information system where commuters share GPS traces and submit problem reports. Tiramisu processes incoming traces and generates real-time arrival time predictions for buses. We conducted a field trial with 28 participants. In this paper we report on the results and reflect on the use of field trials to evaluate crowd-sourcing prototypes and on how crowd sourcing can generate co-production between citizens and public services.
Biketastic: sensing and mapping for better biking	Bicycling is an affordable, environmentally friendly alternative transportation mode to motorized travel. A common task performed by bikers is to find good routes in an area, where the quality of a route is based on safety, efficiency, and enjoyment. Finding routes involves trial and error as well as exchanging information between members of a bike community. Biketastic is a platform that enriches this experimentation and route sharing process making it both easier and more effective. Using a mobile phone application and online map visualization, bikers are able to document and share routes, ride statistics, sensed information to infer route roughness and noisiness, and media that documents ride experience. Biketastic was designed to ensure the link between information gathering, visualization, and bicycling practices. In this paper, we present architecture and algorithms for route data inferences and visualization. We evaluate the system based on feedback from bicyclists provided during a two-week pilot.
Cooperative transit tracking using smart-phones	Real-time transit tracking is gaining popularity as a means for transit agencies to improve the rider experience. However, many transit agencies lack either the funding or initiative to provide such tracking services. In this paper, we describe a crowd-sourced alternative to official transit tracking, which we call cooperative transit tracking. Participating users install an application on their smart-phone. With the help of built-in sensors, such as GPS, WiFi, and accelerometer, the application automatically detects when the user is riding in a transit vehicle. On these occasions (and only these), it sends periodic, anonymized, location updates to a central tracking server. Our technical contributions include (a) an accelerometer-based activity classification algorithm for determining whether or not the user is riding in a vehicle, (b) a memory and time-efficient route matching algorithm for determining whether the user is in a bus vs. another vehicle, (c) a method for tracking underground vehicles, and an evaluation of the above on real-world data. By simulating the Chicago transit network, we find that the proposed system would shorten expected wait times by 2 minutes with only 5% of transit riders using the system. At a 20% penetration level, the mean wait time is reduced from 9 to 3 minutes.
Understanding the space for co-design in riders' interactions with a transit service	The recent advances in web 2.0 technologies and the rapid adoption of smart phones raises many opportunities for public services to improve their services by engaging their users (who are also owners of the service) in co-design: a dialog where users help design the services they use. To investigate this opportunity, we began a service design project investigating how to create repeated information exchanges between riders and a transit agency in order to create a virtual "place" from which the dialog on services could take place. Through interviews with riders, a workshop with a transit agency, and speed dating of design concepts, we have developed a design direction. Specifically, we propose a service that combines vehicle location and "fullness" ratings provided by riders with dynamic route change information from the transit agency as a foundation for a dialog around riders conveying input for continuous service improvement.
Publics in practice: ubiquitous computing at a shelter for homeless mothers	Today, commodity technologies like mobile phones - once symbols of status and wealth - have become deeply woven into social and economic participation in Western society. Despite the pervasiveness of these technologies, there remain groups who may not have extensive access to them but who are nonetheless deeply affected by their presence in everyday life. In light of this, we designed, built, and deployed a ubiquitous computing system for one such overlooked group: the staff and residents at a shelter for homeless mothers. Our system connects mobile phones, a shared display, and a Web application to help staff and residents stay connected. We report on the adoption and use of this system over the course of a 30 week deployment, discussing the substantial impact our system had on shelter life and the broader implications for such socio-technical systems that sit at the juncture of social action and organizational coordination.
The view from the trenches: organization, power, and technology at two nonprofit homeless outreach centers	Nonprofit social service organizations provide the backbone of social support infrastructure in the U.S. and around the world. As the ecology of information exchange moves evermore digital, nonprofit organizations with limited resources and expertise struggle to keep pace. We present a qualitative investigation of two nonprofit outreach centers providing service to the homeless in a U.S. metropolitan city. Despite similar goals shared by these organizations, apparent differences in levels of computerization, volunteerism, and organizational structure demonstrate the challenges in attempting to adopt technology systems when resources and technical expertise are highly constrained.
Improving the safety of homeless young people with mobile phones: values, form and function	By their pervasiveness and by being worn on our bodies, mobile phones seem to have become intrinsic to safety. To examine this proposition, 43 participants, from four stakeholder groups (homeless young people, service providers, police officers, and community members), were asked to consider how homeless young people could use mobile phones to keep safe. Participants were asked to express their knowledge for place-based safety and to envision how mobile phones might be used to improve safety. Detailed analysis of the resulting data, which included value sketches, written value scenarios, and semi-structured discussion, led to specific design opportunities, related to values (e.g., supporting trust and desire to help others), function (e.g., documenting harms for future purposes), and form (e.g., leveraging social expectations for how mobile phones can be used to influence behavior). Together, these findings bound a design space for how mobile phones can be used to manage unsafe situations.
Laying the foundations for public participation and value advocacy: interaction design for a large scale urban simulation	Supporting public participation is often a key goal in the design of digital government systems. However, years of work may be required before a complex system, such as the UrbanSim urban simulation system, is deployed and ready for such participation. In this paper, we investigate laying the foundations for public participation in advance of wide-scale public deployment, with the goal of having interaction designs ready when the system is put into such use. Moreover, in a highly politicized domain such as this one, value advocacy as well as factual information plays a central role. Using the theory and methods of Value Sensitive Design, we address three design goals toward public participation and value advocacy, and provide evidence that each of them was achieved: (1) enabling indirect stakeholders to become direct stakeholders (i.e. enabling more people to interact directly with UrbanSim in useful ways); (2) developing a participatory process by which these stakeholders can help guide the development of the system itself; and (3) enabling participating organizations to engage in value advocacy while at the same time enhancing overall system legitimation.
The photostroller: supporting diverse care home residents in engaging with the world	The Photostroller is a device designed for use by residents of a care home for older people. It shows a continuous slideshow of photographs retrieved from the Flickr™ image website using a set of six predefined categories modified by a tuneable degree of 'semantic drift'. In this paper, we describe the design process that led to the Photostroller, and summarise observations made during a deployment in the care home that has lasted over two months at the time of writing. We suggest that the Photostroller balances constraint with openness, and control with drift, to provide an effective resource for the ludic engagement of a diverse group of older people with each other and the world outside their home.
Age and experience: ludic engagement in a residential care setting	The "older old" (people over eighty) are a largely invisible group for those not directly involved in their lives; this project explores the ways that technology might strengthen links between different generations. This paper describes findings from a two-year study of a residential care home and develops the notion of cross-generational engagement through ludic systems which encourage curiosity and playfulness. It outlines innovative ways of engaging the older old through "digital curios" such as Bloom, the Tenori On and Google Earth. The use of these curios was supplemented with portraiture by three local artists, nine school children and the field researcher. The paper describes four technological interventions: "video window", "projected portraiture", "blank canvas", and "soundscape radio". These interventions attempt to reposition "off the shelf technologies to provide a space for cross-generational engagement The notion of inter-passivity (the obverse of interaction) is explored in relation to each intervention.
Automics: souvenir generating photoware for theme parks	Automics is a photo-souvenir service which utilises mobile devices to support the capture, sharing and annotation of digital images amongst groups of visitors to theme parks. The prototype service mixes individual and group photo-capture with existing in-park, on-ride photo services, to allow users to create printed photo-stories. Herein we discuss initial fieldwork in theme parks that grounded the design of Automics, our development of the service prototype, and its real-world evaluation with theme park visitors. We relate our findings on user experience of the service to a literature on mobile photoware, finding implications for the design of souvenir services.
Active construction of experience through mobile media: a field study with implications for recording and sharing	To fully appreciate the opportunities provided by interactive and ubiquitous multimedia to record and share experiences, we report on an ethnographic investigation on the settings and nature of human memory and experience at a large-scale event. We studied two groups of spectators at a FIA World Rally Championship in Finland, both equipped with multimedia mobile phones. Our analysis of the organization of experience-related activities in the mass event focuses on the active role of technology-mediated memories in constructing experiences. Continuity, reflexivity with regard to the Self and the group, maintaining and re-creating group identity, protagonism and active spectatorship were important social aspects of the experience and were directly reflected in how multimedia was used. Particularly, we witnessed multimedia-mediated forms of expression, such as staging, competition, storytelling, joking, communicating presence, and portraying others; and the motivation for these stemmed from the engaging, processual, and shared nature of experience. Moreover, we observed how temporality and spatiality provided a platform for constructing experiences. The analysis advocates applications that not only store or capture human experience for sharing or later use but also actively participates in the very construction of experience. The approach conveys several valuable design implications.
Opening up the family archive	The Family Archive device is an interactive multi-touch tabletop technology with integrated capture facility for the archiving of sentimental artefacts and memorabilia. It was developed as a technology probe to help us open up current family archiving practices and to explore family archiving in situ. We detail the deployment and study of three of these devices in family homes and discuss how deploying a new, potentially disruptive, technology can foreground the social relations and organizing systems in domestic life. This in turn facilitates critical reflection on technology design.
Give and take: a study of consumer photo-sharing culture and practice	In this paper, we present initial findings from the study of a digital photo-sharing website: Flickr.com. In particular, we argue that Flickr.com appears to support-for some people-a different set of photography practices, socialization styles, and perspectives on privacy that are unlike those described in previous research on consumer and amateur photographers. Further, through our examination of digital photographers' photowork activities-organizing, finding, sharing and receiving-we suggest that privacy concerns and lack of integration with existing communication channels have the potential to prevent the 'Kodak Culture' from fully adopting current photo-sharing solutions.
Zero-Knowledge and Code Obfuscation	In this paper, we investigate the gap between auxiliary-input zero-knowledge (AIZK) and blackbox-simulation zero-knowledge (BSZK). It is an interesting open problem whether or not there exists a protocol which achieves AIZK, but not BSZK. We show that the existence of such a protocol is closely related to the existence of secure code obfuscators. A code obfuscator is used to convert a code into an equivalent one that is difficult to reverse-engineer. This paper provides security definitions of code obfuscation. By their definitions, it is easy to see that the existence of the gap implies the existence of a cheating verifier such that it is impossible to obfuscate any code of it. Intuitively, this means that it is possible to reverse-engineer any code of such a cheating verifier. Furthermore, we consider the actual behavior of such a cheating verifier. In order to do so, we focus on two special cases in which the gap exists: (1) there exists a constant round public-coin AIZK interactive argument for a language outside of BPP. (2) there exists a 3-round secret-coin AIZK interactive argument for a language outside of BPP. In the former case, we show that it is impossible to securely obfuscate a code of a cheating verifier behaving as a pseudorandom function. A similar result is shown also in the latter case. Our results imply that any construction of constant round public-coin or 3-round secret-coin AIZK arguments for non-trivial languages essentially requires a computational assumption with a reverse-engineering property.
Magic Functions	We consider three apparently unrelated fundamental problems in distributed computing, cryptography, and complexity theory, and prove that they are essentially the same problem. These three problems and brief descriptions of them follow. (1) The selective decommitment problem. An adversary is given commitments to a collection of messages, and the adversary can ask for some subset of the commitments to be opened. The question is whether seeing these open plain texts allows the adversary to learn something unexpected about the plain texts that are still hidden. (2) The power of 3-round weak zero-knowledge arguments. The question is what can be proved in (a possibly weakened form of) zero-knowledge in a 3-round argument. In particular, is there a language outside of BPP that has a 3-round public-coin weak zero-knowledge argument? (3) The Fiat-Shamir methodology. This is a method for converting a 3-round public-coin argument (viewed as an identification scheme) to a 1-round signature scheme. The method requires what we call a "magic function" that the signer applies to the first-round message of the argument to obtain a second-round message (queries from the verifier). An open question here is whether every 3-round public-coin argument for a language outside of BPP has a magic function.We define a hierarchy of definitions of zero-knowledge, starting with well-known definitions at the top and proceeding down through a series of weakening. We show that if there is a gap in this hierarchy (a place where two adjacent levels differ) exhibited by a public-coin interactive proof, then the question in (3) has a negative answer (there is no magic function for this interactive proof). We also give a partial converse to this: informally, if there is no gap, then some form of magic is possible for every public-coin 3-round argument for a language outside of BPP. Finally, we relate the selective decommitment problem to public-coin proof systems at an intermediate level of the hierarchy, and obtain several positive security results for selective decommitment.
Mix and Match: Secure Function Evaluation via Ciphertexts	We introduce a novel approach to general secure multiparty computation that avoids the intensive use of verifiable secret sharing characterizing nearly all previous protocols in the literature. Instead, our scheme involves manipulation of ciphertexts for which the underlying private key is shared by participants in the computation. The benefits of this protocol include a high degree of conceptual and structural simplicity, low message complexity, and substantial flexibility with respect to input and output value formats. We refer to this new approach as mix and match. While the atomic operations in mix and match are logical operations, rather than full field operations as in previous approaches, the techniques we introduce are nonetheless highly practical for computations involving intensive bitwise manipulation. One application for which mix and match is particularly well suited is that of sealed-bid auctions. Thus, as another contribution in this paper, we present a practical, mix-and-match-based auction protocol that is fully private and non-interactive and may be readily adapted to a wide range of auction strategies.
Efficient Secure Multi-party Computation	Since the introduction of secure multi-party computation, all proposed protocols that provide security against cheating players suffer from very high communication complexities. The most efficient unconditionally secure protocols among n players, tolerating cheating by up to t n/3 of them, require communicating O(n6) field elements for each multiplication of two elements, even if only one player cheats. In this paper, we propose a perfectly secure multi-party protocol which requires communicating O(n3) field elements per multiplication. In this protocol, the number of invocations of the broadcast primitive is independent of the size of the circuit to be computed. The proposed techniques are generic and apply to other protocols for robust distributed computations. Furthermore, we show that a sub-protocol proposed in [GRR98] for improving the efficiency of unconditionally secure multi-party computation is insecure.
Multiparty unconditionally secure protocols	Under the assumption that each pair of participants can communicate secretly, we show that any reasonable multiparty protocol can be achieved if at least 2n/3 of the participants are honest. The secrecy achieved is unconditional. It does not rely on any assumption about computational intractability.
Privacy preserving auctions and mechanism design	First Page of the Article
Non-Interactive and Information-Theoretic Secure Verifiable Secret Sharing	It is shown how to distribute a secret to n persons such that each person can verify that he has received correct information about the secret without talking with other persons. Any k of these persons can later find the secret (1 驴 k 驴 n), whereas fewer than k persons get no (Shannon) information about the secret. The information rate of the scheme is 1/2 and the distribution as well as the verification requires approximately 2k modular multiplications pr. bit of the secret. It is also shown how a number of persons can choose a secret "in the well" and distribute it veritably among themselves.
Private Selective Payment Protocols	We consider the following generic type of payment protocol: a server is willing to make a payment to one among several clients, to be selectively chosen; for instance, the one whose private input is maximum. Instances of this protocol arise in several financial transactions, such as auctions, lotteries and prize-winning competitions.We define such a task by introducing the notion of private selective payment protocol for a given function, deciding which client is selected. We then present an efficient private selective payment protocol for the especially interesting case in which the function selects the client with maximum private input. Our protocol can be performed in constant rounds, does not require any interaction among the clients, and does not use general circuit evaluation techniques. Moreover, our protocol satisfies strong privacy properties: it is information-theoretically private with respect to all but-one clients trying to learn the other client's private input or which client is selected; and assuming the hardness of deciding quadratic residuosity modulo Blum integers, a honest-but-curious server does not learn any information about which client is selected, or about the private inputs of selected or non-selected clients. The techniques underlying this protocol involve the introduction and constructions for a novel variant of oblivious transfer, of independent interest, which we call symmetrically-private conditional oblivious transfer.
Electronic auctions with private bids	Auctions are a fundamental electronic commerce technology. We describe a set of protocols for performing sealed-bid electronic auctions which preserve the privacy of the submitted bids using a form of secure distributed computation. Bids are never revealed to any party, even after the auction is completed. Both first-price and second-price (Vickrey) auctions are supported, and the computational costs of the methods are low enough to allow their use in many real-world auction situations.
Proving in zero-knowledge that a number is the product of two safe primes	We present the first efficient statistical zero-knowledge protocols to prove statements such as: - A committed number is a prime. - A committed (or revealed) number is the product of two safe primes, i.e., primes p and q such that (p - 1)=2 and (q - 1)=2 are prime. - A given integer has large multiplicative order modulo a composite number that consists of two safe prime factors. The main building blocks of our protocols are statistical zero-knowledge proofs of knowledge that are of independent interest. We show how to prove the correct computation of a modular addition, a modular multiplication, and a modular exponentiation, where all values including the modulus are committed to but not publicly known. Apart from the validity of the equations, no other information about the modulus (e.g., a generator whose order equals the modulus) or any other operand is exposed. Our techniques can be generalized to prove that any multivariate modular polynomial equation is satisfied, where only commitments to the variables of the polynomial and to the modulus need to be known. This improves previous results, where the modulus is publicly known. We show how these building blocks allow to prove statements such as those listed earlier.
Adaptive Security for Threshold Cryptosystems	We present adaptively-secure efficient solutions to several central problems in the area of threshold cryptography. We prove these solutions to withstand adaptive attackers that choose parties for corruption at any time during the run of the protocol. In contrast, all previously known efficient protocols for these problems were proven secure only against less realistic static adversaries that choose and fix the subset of corrupted parties before the start of the protocol run. Specifically, we provide adaptively-secure solutions for distributed key generation in discrete-log based cryptosystems, and for the problem of distributed generation of DSS signatures (threshold DSS). We also show how to transform existent static solutions for threshold RSA and proactive schemes to withstand the stronger adaptive attackers. In doing so, we introduce several techniques for the design and analysis of adaptively-secure protocols that may well find further applications.
ShadowStory: creative and collaborative digital storytelling inspired by cultural heritage	With the fast economic growth and urbanization of many developing countries come concerns that their children now have fewer opportunities to express creativity and develop collaboration skills, or to experience their local cultural heritage. We propose to address these concerns by creating technologies inspired by traditional arts, and allowing children to create and collaborate through playing with them. ShadowStory is our first attempt in this direction, a digital storytelling system inspired by traditional Chinese shadow puppetry. We present the design and implementation of ShadowStory and a 7-day field trial in a primary school. Findings illustrated that ShadowStory promoted creativity, collaboration, and intimacy with traditional culture among children, as well as interleaved children's digital and physical playing experience.
Exploring the feasibility of video mail for illiterate users	We present work that explores whether the asynchronous peer-to-peer communication capabilities of email can be made accessible to illiterate populations in the developing world. Building on metaphors from traditional communication systems such as postal mail, and relevant design principles established by previous research into text-free interfaces, we designed and evaluated a prototype asynchronous communication application built on standard email protocols. We considered different message formats -- text, freeform ink, audio, and video + audio -- and via iterative usage and design sessions, determined that video + audio was the most viable. Design alternatives for authentication processes were also explored. Our prototype was refined over three usability iterations, and the final version evaluated in a two-stage study with 20 illiterate users from an urban slum in Bangalore, India. Our results are mixed: On the one hand, the results show that users can understand the concept of video mail. They were able to successfully complete tasks ranging from account setup to login to viewing and creating mail, but required assistance from an online audio assistant. On the other hand, there were some surprising challenges such as a consistent difficulty understanding the notion of asynchronicity. The latter suggests that more work on the paradigm is required before the benefits of email can be brought to illiterate users.
Telling the whole story: anticipation, inspiration and reputation in a field deployment of TellTable	We present a field study of TellTable, a new storytelling system designed to support creativity and collaboration amongst children. The application was deployed on a multi-touch interactive table in the library of a primary school, where children could use it to create characters and scenery based on elements of the physical world (captured through photography) as well as through drawing. These could then be used to record a story which could be played back. TellTable allowed children to collaborate in devising stories that mixed the physical and the digital in creative ways and that could include themselves as characters. Additionally, the field deployment illustrated how children took inspiration from one another's stories, how they planned elements of their own tales before using the technology, and how the fact that stories could be accessed in the library led some to become well-known and popular within the school community. The real story here, we argue, needs to take into account all that happens within the wider context of use of this system.
On θ-episturmian words	In this paper we study a class of infinite words on a finite alphabet A whose factors are closed under the image of an involutory antimorphism @q of the free monoid A^*. We show that given a recurrent infinite word @w@?A^N, if there exists a positive integer K such that for each n=1 the word @w has (1) cardA+(n-1)K distinct factors of length n, and (2) a unique right and a unique left special factor of length n, then there exists an involutory antimorphism @q of the free monoid A^* preserving the set of factors of @w.
Fine and Wilf words for any periods II	Given positive integers n, and p"1,...,p"r, we establish a fast word combinatorial algorithm for constructing a word w=w"1...w"n of length n, with periods p"1,...,p"r, and on the maximal number of distinct letters. Moreover, we show that the constructed word, which is unique up to word isomorphism, is a pseudo-palindrome - i.e. it is a fixed point of an involutory antimorphism.
Efficient Secure Multi-party Computation	Since the introduction of secure multi-party computation, all proposed protocols that provide security against cheating players suffer from very high communication complexities. The most efficient unconditionally secure protocols among n players, tolerating cheating by up to t n/3 of them, require communicating O(n6) field elements for each multiplication of two elements, even if only one player cheats. In this paper, we propose a perfectly secure multi-party protocol which requires communicating O(n3) field elements per multiplication. In this protocol, the number of invocations of the broadcast primitive is independent of the size of the circuit to be computed. The proposed techniques are generic and apply to other protocols for robust distributed computations. Furthermore, we show that a sub-protocol proposed in [GRR98] for improving the efficiency of unconditionally secure multi-party computation is insecure.
Efficient Multiparty Protocols Using Circuit Randomization	The difference between theory and practice often rests on one major factor: efficiency. In distributed systems, communication is usually expensive, and protocols designed for practical use must require as few rounds of communication and as small messages as possible.A secure multiparty protocol to compute function F is a protocol that, when each player i of n players starts with private input xi, provides each participant i with F(x1,...xn) without revealing more information than what can be derived from learning the function value. Some number l of players may be corrupted by an adversary who may then change the messages they send. Recent solutions to this problem have suffered in practical terms: while theoretically using only polynomially-many rounds, in practice the constants and exponents of such polynomials are too great. Normally, such protocols express F as a circuit CF, call on each player to secretly share xi, and proceed to perform "secret addition and multiplication" on secretly shared values. The cost is proportional to the depth of CF times the cost of secret multiplication; and multiplication requires several rounds of interaction.We present a protocol that simplifies the body of such a protocol and significantly reduces the number of rounds of interaction. The steps of our protocol take advantage of a new and counterintuitive technique for evaluating a circuit: set every input to every gate in the circuit completely at random, and then make corrections. Our protocol replaces each secret multiplication -- multiplication that requires further sharing, addition, zero-knowledge proofs, and secret reconstruction -- that is used during the body of a standard protocol by a simple reconstruction of secretly shared values, thereby reducing rounds by an order of magnitude. Furthermore, these reconstructions require only broadcast messages (but do not require Byzantine Agreement). The simplicity of broadcast and reconstruction provides efficiency and ease of implementation. Our transformation is simple and compatible with other techniques for reducing rounds.
Communication complexity of secure computation (extended abstract)	A secret-ballot vote for a single proposition is an example of a secure distributed computation. The goal is for m participants to jointly compute the output of some n-ary function (in this case, the sum of the votes), while protecting their individual inputs against some form of misbehavior.In this paper, we initiate the investigation of the communication complexity of unconditionally secure multi-party computation, and its relation with various fault-tolerance models. We present upper and lower bounds on communication, as well as tradeoffs among resources.First, we consider the “direct sum problem” for communications complexity of perfectly secure protocols: Can the communication complexity of securely computing a single function f : Fn → F at k sets of inputs be smaller if all are computed simultaneously than if each is computed individually? We show that the answer depends on the failure model. A factor of O(n/log n) can be gained in the privacy model (where processors are curious but correct); specifically, when f is n-ary addition (mod 2), we show a lower bound of &OHgr;(n2 log n) for computing f O(n) times simultaneously. No gain is possible in a slightly stronger fault model (fail-stop mode); specifically, when f is n-ary addition over GF(q), we show an exact bound of &THgr;(kn2 log q) for computing f at k sets of inputs simultaneously (for any k ≥ 1).However, if one is willing to pay an additive cost in fault tolerance (from t to t-k+1), then a variety of known non-cryptographic protocols (including “provably unparallelizable” protocols from above!) can be systematically compiled to compute one function at k sets of inputs with no increase in communication complexity. Our compilation technique is based on a new compression idea of polynomial-based multi-secret sharing.Lastly, we show how to compile private protocols into error-detecting protocols at a big savings of a factor of O(n3) (up to a log factor) over the best known error-correcting protocols. This is a new notion of fault-tolerant protocols, and is especially useful when malicious behavior is infrequent, since error-detection implies error-correction in this case.
Multiparty unconditionally secure protocols	Under the assumption that each pair of participants can communicate secretly, we show that any reasonable multiparty protocol can be achieved if at least 2n/3 of the participants are honest. The secrecy achieved is unconditional. It does not rely on any assumption about computational intractability.
Security with Low Communication Overhead	We consider the communication complexity of secure multiparty computations by networks of processors each with unlimited computing power. Say that an n-party protocol for a function of m bits is efficient if it uses a constant number of rounds of communication and a total number of message bits that is polynomial in max(m, n). We show that any function has an efficient protocol that achieves (n log n)/m resilience. Ours is the first secure multiparty protocol in which the communication complexity is independent of the computational complexity of the function being computed.We also consider the communication complexity of zero-knowledge proofs of properties of committed bits. We show that every function f of m bits has an efficient notarized envelope scheme; that is, there is a protocol in which a computationally unlimited prover commits a sequence of bits x to a computationally unlimited verifier and then proves in perfect zero-knowledge (without decommitting x) that f(x) = 1, using a constant number of rounds and poly(m) message bits. Ours is the first notarized envelope scheme in which the communication complexity is independent of the computational complexity of f.Finally, we establish a new upper bound on the number of oracles needed in instance-hiding schemes for arbitrary functions. These schemes allow a computationally limited querier to capitalize on the superior power of one or more computationally unlimited oracles in order to obtain f(x) without revealing its private input x to any one of the oracles. We show that every function of m bits has an (m/logm)-oracle instance-hiding scheme.The central technique used in all of these results is locally random reducibility, which was used for the first time in [7] and is formally defined for the first time here. In addition to the applications that we present, locally random reducibility has been applied to interactive proof systems, program checking, and program testing.
General secure multi-party computation from any linear secret-sharing scheme	We show that verifiable secret sharing (VSS) and secure multi-party computation (MPC) among a set of n players can efficiently be based on any linear secret sharing scheme (LSSS) for the players, provided that the access structure of the LSSS allows MPC or VSS at all. Because an LSSS neither guarantees reconstructability when some shares are false, nor verifiability of a shared value, nor allows for the multiplication of shared values, an LSSS is an apparently much weaker primitive than VSS or MPC. Our approach to secure MPC is generic and applies to both the information-theoretic and the cryptographic setting. The construction is based on 1) a formalization of the special multiplicative property of an LSSS that is needed to perform a multiplication on shared values, 2) an efficient generic construction to obtain from any LSSS a multiplicative LSSS for the same access structure, and 3) an efficient generic construction to build verifiability into every LSSS (always assuming that the adversary structure allows for MPC or VSS at all). The protocols are efficient. In contrast to all previous information-theoretically secure protocols, the field size is not restricted (e.g, to be greater than n). Moreover, we exhibit adversary structures for which our protocols are polynomial in n while all previous approaches to MPC for non-threshold adversaries provably have super-polynomial complexity.
Efficient Anonymous Fingerprinting with Group Signatures	Fingerprinting schemes enable a merchant to identify the buyer of an illegally distributed digital good by providing each buyer with a slightly different version. Asymmetric fingerprinting schemes further prevent the merchant from framing a buyer by making the fingerprinted version known to the buyer only. In addition, an anonymous fingerprinting scheme allows the buyer to purchase goods without revealing her identity to the merchant. However, as soon as the merchant finds a sold version that has been (illegally) distributed, he is able to retrieve a buyer's identity and take her to court. This paper proposes a new and more efficient anonymous fingerprinting scheme that uses group signature schemes as a building block. A byproduct of independent interest is an asymmetric fingerprinting scheme that allows so-called two-party trials, which is unmet so far.
Coin-based anonymous fingerprinting	Fingerprinting schemes are technical means to discourage people from illegally redistributing the digital data they have legally purchased. These schemes enable the original merchant to identify the original buyer of the digital data. In so-called asymmetric fingerprinting schemes the fingerprinted data item is only known to the buyer after a sale and if the merchant finds an illegally redistributed copy, he obtains a proof convincing a third party whom this copy belonged to. All these fingerprinting schemes require the buyers to identify themselves just for the purpose of fingerprinting and thus offer the buyers no privacy. Hence anonymous asymmetric fingerprinting schemes were introduced, which preserve the anonymity of the buyers as long as they do not redistribute the data item. In this paper a new anonymous fingerprinting scheme based on the principles of digital coins is introduced. The construction replaces the general zero-knowledge techniques from the known certificate-based construction by explicit protocols, thus bringing anonymous fingerprinting far nearer to practicality.
Exploring reactive access control	As users store and share more digital content at home, access control becomes increasingly important. One promising approach for helping non-expert users create accurate access policies is reactive policy creation, in which users can update their policy dynamically in response to access requests that would not otherwise succeed. An earlier study suggested reactive policy creation might be a good fit for file access control at home. To test this, we conducted an experience-sampling study in which participants used a simulated reactive access-control system for a week. Our results bolster the case for reactive policy creation as one mode by which home users specify access-control policy. We found both quantitative and qualitative evidence of dynamic, situational policies that are hard to implement using traditional models but that reactive policy creation can facilitate. While we found some clear disadvantages to the reactive model, they do not seem insurmountable.
Over-exposed?: privacy patterns and considerations in online and mobile photo sharing	As sharing personal media online becomes easier and widely spread, new privacy concerns emerge - especially when the persistent nature of the media and associated context reveals details about the physical and social context in which the media items were created. In a first-of-its-kind study, we use context-aware camerephone devices to examine privacy decisions in mobile and online photo sharing. Through data analysis on a corpus of privacy decisions and associated context data from a real-world system, we identify relationships between location of photo capture and photo privacy settings. Our data analysis leads to further questions which we investigate through a set of interviews with 15 users. The interviews reveal common themes in privacy considerations: security, social disclosure, identity and convenience. Finally, we highlight several implications and opportunities for design of media sharing applications, including using past privacy patterns to prevent oversights and errors.
Improving user-interface dependability through mitigation of human error	Security may be compromised when humans make mistakes at the user interface. Cleartext is mistakenly sent to correspondents, sensitive files are left unprotected, and erroneously configured systems are left vulnerable to attackers. Such mistakes may be blamed on human error, but the regularity of human error suggests that mistakes may be preventable through better interface design. Certain user-interface constructs drive users toward error, while others facilitate success.Two security-sensitive user interfaces were evaluated in a laboratory user study: the Windows XP file-permissions interface and an alternative interface, called Salmon, designed in accordance with an error-avoiding principle to counteract the misleading constructs in the XP interface. The alternative interface was found to be more dependable; it increased successful task completion by up to 300%, reduced commission of a class of errors by up to 94%, and provided a nearly 3× speed-up in task completion time. Moreover, users spent less time searching for information with the alternative interface, and a greater proportion of time on essential task steps. An explanatory theory in its early stages of development is presented.
User-centred multimodal reminders for assistive living	While there has been a lot of research on the usability of reminders and alarms in the work context, the home has been somewhat neglected despite the importance of reminder systems for telecare and assistive living systems. We conducted a comprehensive mixed-methods study into the requirements for useable and acceptable reminders in the home. The study consisted of a questionnaire (N=379), 6 focus groups, and 7 home tour interviews. Our results highlight the need for highly flexible and contextualized multimodal and multi-device reminder solutions that build on existing successful strategies for remembering in and around the home. We suggest that developers of home care reminder systems should design for diversity, context, priorities, autonomy, shared spaces, and optimal care.
Constructing identities through storytelling in diabetes management	The continuing epidemics of diabetes and obesity create much need for information technologies that can help individuals engage in proactive health management. Yet many of these technologies focus on such pragmatic issues as collecting and presenting health information and modifying individuals' behavior. At the same time, researchers in clinical community argue that individuals' perception of their identity has dramatic consequences for their health behaviors. In this paper we discuss results of a deployment study of a mobile health monitoring application. We show how individuals with considerable diabetes experience found a unique way to adopt this health-monitoring application to construct and negotiate their identities as persons with a chronic disease. We argue that viewing health management from identity construction perspective opens new opportunities for research and design in technologies for health.
Because I carry my cell phone anyway: functional location-based reminder applications	Although they have potential, to date location-based information systems have not radically improved the way we interact with our surroundings. To study related issues, we developed a location-based reminder system, PlaceMail, and demonstrate its utility in supporting everyday tasks through a month-long field study. We identify current tools and practices people use to manage distributed tasks and note problems with current methods, including the common "to-do list". Our field study shows that PlaceMail supports useful location-based reminders and functional place-based lists. The study also sheds rich and surprising light on a new issue: when and where to deliver location-based information. The traditional 'geofence' radius around a place proves insufficient. Instead, effective delivery depends on people's movement patterns through an area and the geographic layout of the space. Our results both provide a compelling demonstration of the utility of location-based information and raise significant new challenges for location-based information distribution.
Designing eco-feedback systems for everyday life	Eco-feedback systems currently frame householders as micro-resource managers, who weigh up the costs and benefits of their consumption, and make autonomous, rational and efficient decisions. Reporting on findings from a qualitative study of three Australian energy and water eco-feedback programs utilising an in-home display (IHD) system, this paper challenges this view. The research finds that householders consume energy and water to carry out everyday practices, such as showering, laundering and cooling, which are mediated by social, cultural, technical and institutional dynamics. The paper proposes an alternative design paradigm for eco-feedback systems premised on the realities of everyday life and identifies several design directions that emerge from this new starting point.
It's not all about "Green": energy use in low-income communities	Personal energy consumption, specifically home energy consumption such as heating, cooling, and electricity, has been an important environmental and economic topic for decades. Despite the attention paid to this area, few researchers have specifically explored these issues within a community that makes up approximately 30% of U.S. households -- those below the federal poverty line. We present a study of 26 low-income households in two very different locations -- a small town in the Southern U.S. and a northerly metropolitan area. Through a photo-elicitation study and directed interviews, we explore the relationship between energy saving behaviors, external factors, and users' intrinsic values and beliefs. Most of our participants are committed to saving energy for non-financial reasons, even when not responsible for paying bills. Challenges to saving energy include safety and lack of control over the environment. We discuss how Ubicomp technologies for saving energy can address some of these challenges.
Cross currents: water scarcity and sustainable CHI	Growing awareness of the threats posed by global freshwater shortages coupled with increased interest in environmental sustainability among CHI researchers make water management a ripe area for new CHI applications. This paper presents a qualitative study of practices and attitudes in a water-stressed region of the United States. We describe water conservation as a culturally-situated activity influenced by a variety of social factors, and show "sustainability" to be a complicated concept rife with competing, often incompatible interpretations and prescriptions. We discuss implications for designing interfaces that encourage personal conservation, and identify environmental policy making as an area ripe for new CHI activity. Finally, we suggest that sustainability has the potential to move from the periphery of CHI research and become a galvanizing force for the community at large.
Smart metering demand management programs: challenging the comfort and cleanliness habitus of households	Drawing on Pierre Bourdieu's concepts of habitus, field and capital, this paper outlines how smart metering demand management programs could be redesigned to bring together the competing fields of resource management and domestic life. Comfort and cleanliness expectations, which are ingrained in the habitus of householders and the field of domestic life, are often overlooked in demand management programs, which focus instead on making existing and evolving expectations more efficient. This paper draws on preliminary findings from qualitative research activities with householders who received consumption feedback through an in-home display, and/or variable price signals --- both enabled by smart meters. The paper offers insights for designers of interactive demand management strategies about how to go beyond achieving efficiency benefits in the home in order to fundamentally change expectations and norms ingrained in the habitus.
Getting to green: understanding resource consumption in the home	Rising global energy demands, increasing costs and limited natural resources mean that householders are more conscious about managing their domestic resource consumption. Yet, the question of what tools Ubicomp researchers can create for residential resource management remains open. To begin to address this omission, we present a qualitative study of 15 households and their current management practices around the water, electricity and natural gas systems in the home. We find that in-the-moment resource consumption is mostly invisible to householders and that they desire more real-time information to help them save money, keep their homes comfortable and be environmentally friendly. Designing for domestic sustainability therefore turns on improving the visibility of resource production and consumption costs as well as supporting both individuals and collectives in behavior change. Domestic sustainability also highlights the caveat of potentially creating a green divide by making resource management available only to those who can afford the technologies to support being green. Finally, we suggest that the Ubicomp community can contribute to the domestic and broader sustainability agenda by incorporating green values in designs and highlight the challenge of collecting data on being green.
Representing users in accessibility research	The need to study representative users is widely accepted within the human-computer interaction (HCI) community. While exceptions exist, and alternative populations are sometimes studied, virtually any introduction to the process of designing user interfaces will discuss the importance of understanding the intended users as well as the significant impact individual differences can have on how effectively individuals can use various technologies. HCI researchers are expected to provide relevant demographics regarding study participants as well as information about experience using similar technologies. Yet, in the field of accessibility we continue to see studies that do not appropriately include representative users. Highlighting ways to remedy this multifaceted problem, we argue that expectations regarding how accessibility research is conducted and reported must be raised if this field is to have the desired impact with regard to inclusive design, the information technologies studied, and the lives of the individuals being studied.
Improving the performance of motor-impaired users with automatically-generated, ability-based interfaces	We evaluate two systems for automatically generating personalized interfaces adapted to the individual motor capabilities of users with motor impairments. The first system, SUPPLE, adapts to users' capabilities indirectly by first using the ARNAULD preference elicitation engine to model a user's preferences regarding how he or she likes the interfaces to be created. The second system, SUPPLE++, models a user's motor abilities directly from a set of one-time motor performance tests. In a study comparing these approaches to baseline interfaces, participants with motor impairments were 26.4% faster using ability-based user interfaces generated by SUPPLE++. They also made 73% fewer errors, strongly preferred those interfaces to the manufacturers' defaults, and found them more efficient, easier to use, and much less physically tiring. These findings indicate that rather than requiring some users with motor impairments to adapt themselves to software using separate assistive technologies, software can now adapt itself to the capabilities of its users.
Evaluating existing audio CAPTCHAs and an interface optimized for non-visual use	Audio CAPTCHAs were introduced as an accessible alternative for those unable to use the more common visual CAPTCHAs, but anecdotal accounts have suggested that they may be more difficult to solve. This paper demonstrates in a large study of more than 150 participants that existing audio CAPTCHAs are clearly more difficult and time-consuming to complete as compared to visual CAPTCHAs for both blind and sighted users. In order to address this concern, we developed and evaluated a new interface for solving CAPTCHAs optimized for non-visual use that can be added in-place to existing audio CAPTCHAs. In a subsequent study, the optimized interface increased the success rate of blind participants by 59% on audio CAPTCHAs, illustrating a broadly applicable principle of accessible design: the most usable audio interfaces are often not direct translations of existing visual interfaces.
Collaboration personas: a new approach to designing workplace collaboration tools	The success of social computing has generated a host of workplace collaboration tools. However, adoption of these tools by entire groups is a major problem. One reason for the adoption problem is a lack of methods for considering collaborative groups in technology design. Even when designing collaboration tools, designers often employ methods that focus on individuals. This leads to tools that are not well targeted at the groups who will use them. To solve this problem, we propose the notion of collaboration personas, which are empirically derived descriptions of hypothetical groups, including details that inform the design of collaboration tools. Collaboration personas differ from individual personas in having (1) multiple, inter-related individuals playing specific roles; (2) a focus on collective goals and elaboration of individual goals that affect the collective goal; and (3) new attributes that characterize collaborative aspects of the group's work. We contrast collaboration personas with other design approaches and provide examples of how they can be used to design new collaborative tools that better meet the needs of typical groups.
Fitting an activity-centric system into an ecology of workplace tools	Knowledge workers expend considerable effort managing fragmentation, characterized by constant switching among digital artifacts, when executing work activities. Activity-centric computing (ACC) systems attempt to address this problem by organizing activity-related artifacts together. But are ACC systems effective at reducing fragmentation? In this paper, we present a two-part study of workers using Lotus Activities, an ACC system deployed for over two years in a large company. First, we surveyed workers to understand the ecology of workplace tools they use for various tasks. Second, we interviewed 22 Lotus Activities users to investigate how this ACC tool fits amongst their ecology of existing collaboration tools and affects work fragmentation. Our results indicate that Lotus Activities works in concert with certain other tools to successfully ease fragmentation for a specific type of activity. We identify design characteristics that contribute to this result.
From garments to gardens: negotiating material relationships online and 'by hand'	From home improvement to scrapbooking, leisure activities performed "by hand" increasingly involve digital tools. In turn, software and devices to support handwork are proliferating. We use data from an observational field study of gardening and knitting to examine relationships to information technology. Handwork experiences of patience, effort, sensation, and cleverness can shift with the introduction of new tools. Our participants' attachment to these experiences made them sensitive to the potential consequences of introducing new tools. Digital tools were sometimes rejected and other times woven into handwork activities. In response, we propose three metaphors for handwork practice - extending, interjecting, and segmenting - as a resource for moving beyond the binary opposition of digital and physical practices.
Pottering by design	The last decade of work in HCI has seen an increasing emphasis on the role of technology in the home, and a corresponding need for novel approaches for studying the needs, activities and relationships that constitute home life, so as to inform technology design. In this vein, we report on a particular aspect of home life in Britain: pottering. We investigate the ways in which pottering---unplanned and serendipitous tidying, cleaning, gardening and minor home improvement---can be used as a lens to understand the non-task-focused roles that technology may play in the home. We also describe the strategies we used to study this curious class of activities and hopefully illustrate how open, and sometimes opportunistic, approaches to research can have value.
Emerging technologies for real-time and integrated agriculture decisions	The papers in this special issue arise from the premise that precision agriculture information increases in value when data collection, data processing, and management actions are integrated. It seems evident that precision agriculture adoption has been hindered, in part, due to the lack of products that bring together engineering and agronomics. Additionally, the idea has been forwarded in recent years suggesting that precision agricultural systems should be developed to achieve conservation and other environmental benefits. In the end, users of precision agriculture systems want to know that the best science and technology are employed, but that the information-gathering and decision-making process does not hinder their day-to-day operations of producing the crop. The papers in this special issue were presented at a symposium held at the annual meetings of the American Society of Agronomy, Soil Science Society of America, and Crop Science Society of America in 2005. They highlight examples of spatial information collection and processing to accomplish real- or near real-time management operations.
Aesthetics and experience-centered design	The aesthetics of human-computer interaction and interaction design are conceptualized in terms of a pragmatic account of human experience. We elaborate this account through a framework for aesthetic experience built around three themes: (1) a holistic approach wherein the person with feelings, emotions, and thoughts is the focus of design; (2) a constructivist stance in which self is seen as continuously engaged and constituted in making sense of experience; and (3) a dialogical ontology in which self, others, and technology are constructed as multiple centers of value. We use this framework to critically reflect on research into the aesthetics of interaction and to suggest sensibilities for designing aesthetic interaction. Finally, a digital jewelery case study is described to demonstrate a design approach that is open to the perspectives presented in the framework and to consider how the framework and sensibilities are reflected in engagement with participants and approach to design.
How bodies matter: five themes for interaction design	Our physical bodies play a central role in shaping human experience in the world, understandingof the world, and interactions in the world. This paper draws on theories of embodiment - from psychology, sociology, and philosophy - synthesizing five themes we believe are particularly salient for interaction design: thinking through doing, performance, visibility, risk, and thick practice. We intro-duce aspects of human embodied engagement in the world with the goal of inspiring new interaction design ap-proaches and evaluations that better integrate the physical and computational worlds.
Rise of the expert amateur: DIY projects, communities, and cultures	This paper presents a large-scale study of Do-It-Yourself (DIY) communities, cultures and projects. We focus on the adoption and appropriation of human-computer interaction and collaboration technologies and their role in motivating and sustaining communities of builders, crafters and makers. Our survey of over 2600 individuals across a range of DIY communities (Instructables, Dorkbot, Craftster, Ravelry, Etsy, and Adafruit) reveals a unique set of values, emphasizing open sharing, learning, and creativity over profit and social capital. We derive design implications to embed these values into other everyday practices, and hope that our work serves to engage CHI practitioners with DIY expert amateurs.
Typing on flat glass: examining ten-finger expert typing patterns on touch surfaces	Touch screen surfaces large enough for ten-finger input have become increasingly popular, yet typing on touch screens pales in comparison to physical keyboards. We examine typing patterns that emerge when expert users of physical keyboards touch-type on a flat surface. Our aim is to inform future designs of touch screen keyboards, with the ultimate goal of supporting touch-typing with limited tactile feedback. To study the issues inherent to flat-glass typing, we asked 20 expert typists to enter text under three conditions: (1) with no visual keyboard and no feedback on input errors, then (2) with and (3) without a visual keyboard, but with some feedback. We analyzed touch contact points and hand contours, looking at attributes such as natural finger positioning, the spread of hits among individual keys, and the pattern of non-finger touches. We also show that expert typists exhibit spatially consistent key press distributions within an individual, which provides evidence that eyes-free touch-typing may be possible on touch surfaces and points to the role of personalization in such a solution. We conclude with implications for design.
Usability guided key-target resizing for soft keyboards	Soft keyboards offer touch-capable mobile and tabletop devices many advantages such as multiple language support and room for larger displays. On the other hand, because soft keyboards lack haptic feedback, users often produce more typing errors. In order to make soft keyboards more robust to noisy input, researchers have developed key-target resizing algorithms, where underlying target areas for keys are dynamically resized based on their probabilities. In this paper, we describe how overly aggressive key-target resizing can sometimes prevent users from typing their desired text, violating basic user expectations about keyboard functionality. We propose an anchored key-target method which incorporates usability principles so that soft keyboards can remain robust to errors while respecting usability principles. In an empirical evaluation, we found that using anchored dynamic key-targets significantly reduce keystroke errors as compared to the state-of-the-art.
Distal tactile feedback for text entry on tabletop computers	In this paper we present an initial study into the feasibility of using a mobile phone as a personal tactile display when interacting with a tabletop computer. There has been an increase in recent years in large touchscreen computers that use soft keyboards for text input. Text entry performance on such keyboards can be poor due to the lack of tactile feedback from the keys. Our approach is to use the vibration motor in a user's mobile phone to provide personal haptic feedback for interactions with the touchscreen computer. We ran an experiment to compare text entry on a touchscreen device with the tactile feedback being presented at different distal locations on the body (locations at which a user might keep a mobile device. The conditions were: no tactile feedback, feedback directly on the device, feedback at the wrist, upper arm, chest, belt and trouser pocket). The results showed that distal tactile feedback significantly increased text entry rates when presented to the wrist and upper arm. This was not at the expense of a reduction in text entry accuracy. This shows that the concept of presenting tactile feedback on a user's phone is an effective one and can improve interaction and text entry on tabletop computers.
WeSearch: supporting collaborative search and sensemaking on a tabletop display	Groups of users often have shared information needs -- for example, business colleagues need to conduct research relating to joint projects and students must work together on group homework assignments. In this paper, we introduce WeSearch, a collaborative Web search system designed to leverage the benefits of tabletop displays for face-to-face collaboration and organization tasks. We describe the design of WeSearch and explain the interactions it affords. We then describe an evaluation in which eleven groups used WeSearch to conduct real collaborative search tasks. Based on our study's findings, we analyze the effectiveness of the features introduced by WeSearch.
AirStroke: bringing unistroke text entry to freehand gesture interfaces	In this paper, we explore the opportunity of bringing unistroke text entry to freehand gesture interfaces. Using existing text entry methods directly in such interfaces is impractical because of the differences between freehand gestures and traditional forms of input. To address this problem, we consider the design constraints of text entry methods using freehand gestures, and present AirStroke, a new technique based on a reengineering of the well-known unistroke technique Graffiti. Using Graffiti's alphabet, AirStroke takes advantage of the richer input capabilities of two-handed freehand gestures by providing combined mode selection and character entry with one hand, as well as word completion with the other hand. A longitudinal study suggests that AirStroke has competitive speed and accuracy to unistroke methods based on stylus input.
Disappearing mobile devices	In this paper, we extrapolate the evolution of mobile devices in one specific direction, namely miniaturization. While we maintain the concept of a device that people are aware of and interact with intentionally, we envision that this concept can become small enough to allow invisible integration into arbitrary surfaces or human skin, and thus truly ubiquitous use. This outcome assumed, we investigate what technology would be most likely to provide the basis for these devices, what abilities such devices can be expected to have, and whether or not devices that size can still allow for meaningful interaction. We survey candidate technologies, drill down on gesture-based interaction, and demonstrate how it can be adapted to the desired form factors. While the resulting devices offer only the bare minimum in feedback and only the most basic interactions, we demonstrate that simple applications remain possible. We complete our exploration with two studies in which we investigate the affordance of these devices more concretely, namely marking and text entry using a gesture alphabet.
Beyond flat surface computing: challenges of depth-aware and curved interfaces	In the past decade, multi-touch-sensitive interactive surfaces have transitioned from pure research prototypes in the lab, to commercial products with wide-spread adoption. One of the longer term visions of this research follows the idea of ubiquitous computing, where everyday surfaces in our environment are made interactive. However, most of current interfaces remain firmly tied to the traditional flat rectangular displays of the today's computers and while they benefit from the directness and the ease of use, they are often not much more than touch-enabled standard desktop interfaces. In this paper, we argue for explorations that transcend the traditional notion of the flat display, and envision interfaces that are curved, three-dimensional, or that cross the boundary between the digital and physical world. In particular, we present two research directions that explore this idea: (a) exploring the three-dimensional interaction space above the display and (b) enabling gestural and touch interactions on curved devices for novel interaction possibilities. To illustrate both of these, we draw examples from our own work and the work of others, and guide the reader through several case studies that highlight the challenges and benefits of such novel interfaces. The implications on media requirements and collaboration aspects are discussed in detail, and, whenever possible, we highlight promising directions of future research. We believe that the compelling application design for future non-flat user interfaces will greatly depend on exploiting the unique characteristics of the given form factor.
Enhancing physicality in touch interaction with programmable friction	Touch interactions have refreshed some of the 'glowing enthusiasm' of thirty years ago for direct manipulation interfaces. However, today's touch technologies, whose interactions are supported by graphics, sounds or crude clicks, have a tactile sameness and gaps in usability. We use a Large Area Tactile Pattern Display (LATPaD) to examine design possibilities and outcomes when touch interactions are enhanced with variable surface friction. In a series of four studies, we first confirm that variable friction gives significant performance advantages in low-level targeting activities. We then explore the design space of variable friction interface controls and assess user reactions. Most importantly, we demonstrate that variable friction can have a positive impact on the enjoyment, engagement and sense of realism experienced by users of touch interfaces.
Evaluating tactile feedback and direct vs. indirect stylus input in pointing and crossing selection tasks	We present a pair of experiments that explore the effects of tactile-feedback and direct vs. indirect pen input on pointing and crossing selection tasks. While previous work has demonstrated the validity of crossing as a useful selection mechanism for pen-based computing, those experiments were conducted using an indirect input device -- one in which the pen-input and display were separated. We investigate users' performance with pointing and crossing interfaces controlled via not only an indirect input device, but also a direct input device -- one in which the pen-input and display are co-located. Results show that direct input significantly outperforms indirect input for crossing selection, but the two modalities are essentially equivalent in pointing selection. A small amount of tactile feedback is shown to be beneficial for both pointing and crossing selection, most noticeably in crossing tasks when using direct input where visual feedback is often occluded by a hand or stylus.
Sphere: multi-touch interactions on a spherical display	Sphere is a multi-user, multi-touch-sensitive spherical display in which an infrared camera used for touch sensing shares the same optical path with the projector used for the display. This novel configuration permits: (1) the enclosure of both the projection and the sensing mechanism in the base of the device, and (2) easy 360-degree access for multiple users, with a high degree of interactivity without shadowing or occlusion. In addition to the hardware and software solution, we present a set of multi-touch interaction techniques and interface concepts that facilitate collaborative interactions around Sphere. We designed four spherical application concepts and report on several important observations of collaborative activity from our initial Sphere installation in three high-traffic locations.
Tactile interfaces for small touch screens	We present the design, implementation, and informal evaluation of tactile interfaces for small touch screens used in mobile devices. We embedded a tactile apparatus in a Sony PDA touch screen and enhanced its basic GUI elements with tactile feedback. Instead of observing the response of interface controls, users can feel it with their fingers as they press the screen. In informal evaluations, tactile feedback was greeted with enthusiasm. We believe that tactile feedback will become the next step in touch screen interface design and a standard feature of future mobile devices.
Shallow-depth 3d interaction: design and evaluation of one-, two- and three-touch techniques	On traditional tables, people frequently use the third dimension to pile, sort and store objects. However, while effective and informative for organization, this use of the third dimension does not usually extend far above the table. To enrich interaction with digital tables, we present the concept of shallow-depth 3D -- 3D interaction with limited depth. Within this shallow-depth 3D environment several common interaction methods need to be reconsidered. Starting from any of one, two and three touch points, we present interaction techniques that provide control of all types of 3D rotation coupled with translation (6DOF) on a direct-touch tabletop display. The different techniques exemplify a wide range of interaction possibilities: from the one-touch technique, which is designed to be simple and natural, but inherits a degree of imprecision from its simplicity; through to three-touch interaction, which allows precise bimanual simultaneous control of multiple degrees of freedom, but at the cost of simplicity. To understand how these techniques support interaction in shallow-depth 3D, we present a user study that examines the efficiency of, and preferences for, the techniques developed. Results show that users are fastest and most accurate when using the three-touch technique and that their preferences were also strongly in favour of the expressive power available from three-touch.
Understanding touch	Current touch devices, such as capacitive touchscreens are based on the implicit assumption that users acquire targets with the center of the contact area between finger and device. Findings from our previous work indicate, however, that such devices are subject to systematic error offsets. This suggests that the underlying assumption is most likely wrong. In this paper, we therefore revisit this assumption. In a series of three user studies, we find evidence that the features that users align with the target are visual features. These features are located on the top of the user's fingers, not at the bottom, as assumed by traditional devices. We present the projected center model, under which error offsets drop to 1.6mm, compared to 4mm for the traditional model. This suggests that the new model is indeed a good approximation of how users conceptualize touch input. The primary contribution of this paper is to help understand touch-one of the key input technologies in human-computer interaction. At the same time, our findings inform the design of future touch input technology. They explain the inaccuracy of traditional touch devices as a -Sparallax- artifact between user control based on the top of the finger and sensing based on the bottom side of the finger. We conclude that certain camera-based sensing technologies can inherently be more accurate than contact area-based sensing.
Precise selection techniques for multi-touch screens	The size of human fingers and the lack of sensing precision can make precise touch screen interactions difficult. We present a set of five techniques, called Dual Finger Selections, which leverage the recent development of multi-touch sensitive displays to help users select very small targets. These techniques facilitate pixel-accurate targeting by adjusting the control-display ratio with a secondary finger while the primary finger controls the movement of the cursor. We also contribute a "clicking" technique, called SimPress, which reduces motion errors during clicking and allows us to simulate a hover state on devices unable to sense proximity. We implemented our techniques on a multi-touch tabletop prototype that offers computer vision-based tracking. In our formal user study, we tested the performance of our three most promising techniques (Stretch, X-Menu, and Slider) against our baseline (Offset), on four target sizes and three input noise levels. All three chosen techniques outperformed the control technique in terms of error rate reduction and were preferred by our participants, with Stretch being the overall performance and preference winner.
A probabilistic approach to modeling two-dimensional pointing	We investigate and model two-dimensional pointing where the target distance and size vary as does the angle of movement. We first study the spread of hits in a rapid approximate pointing task at varied distances and movement angles. Consistent with the literature, our results show that the spread of hits along the movement direction deviate more than the spread of hits in the direction perpendicular to movement, and both spreads increase with distance. Based on the distribution of this spread of hits, we propose and validate a new probabilistic model that describes two-dimensional pointing. Unlike previous models, our model accounts for more variables of two-dimensional pointing and can be generalized to any target shape, size, orientation, location, and dimension. In contrast to previous work, which suggests that target height has minimal impact on performance when it is larger than the width, our results show that, even when height is greater than width, it can significantly impact movement time.
Lucid touch: a see-through mobile device	Touch is a compelling input modality for interactive devices; however, touch input on the small screen of a mobile device is problematic because a user's fingers occlude the graphical elements he wishes to work with. In this paper, we present LucidTouch, a mobile device that addresses this limitation by allowing the user to control the application by touching the back of the device. The key to making this usable is what we call pseudo-transparency: by overlaying an image of the user's hands onto the screen, we create the illusion of the mobile device itself being semi-transparent. This pseudo-transparency allows users to accurately acquire targets while not occluding the screen with their fingers and hand. Lucid Touch also supports multi-touch input, allowing users to operate the device simultaneously with all 10 fingers. We present initial study results that indicate that many users found touching on the back to be preferable to touching on the front, due to reduced occlusion, higher precision, and the ability to make multi-finger input.
Direct-touch vs. mouse input for tabletop displays	We investigate the differences -- in terms of bothquantitative performance and subjective preference -- between direct-touch and mouse input for unimanual andbimanual tasks on tabletop displays. The results of twoexperiments show that for bimanual tasks performed ontabletops, users benefit from direct-touch input. However,our results also indicate that mouse input may be moreappropriate for a single user working on tabletop tasksrequiring only single-point interaction.
Magic desk: bringing multi-touch surfaces into desktop work	Despite the prominence of multi-touch technologies, there has been little work investigating its integration into the desktop environment. Bringing multi-touch into desktop computing would give users an additional input channel to leverage, enriching the current interaction paradigm dominated by a mouse and keyboard. We provide two main contributions in this domain. First, we describe the results from a study we performed, which systematically evaluates the various potential regions within the traditional desktop configuration that could become multi-touch enabled. The study sheds light on good or bad regions for multi-touch, and also the type of input most appropriate for each of these regions. Second, guided by the results from our study, we explore the design space of multi-touch-integrated desktop experiences. A set of new interaction techniques are coherently integrated into a desktop prototype, called Magic Desk, demonstrating potential uses for multi-touch enabled desktop configurations.
Mouse 2.0: multi-touch meets the mouse	In this paper we present novel input devices that combine the standard capabilities of a computer mouse with multi-touch sensing. Our goal is to enrich traditional pointer-based desktop interactions with touch and gestures. To chart the design space, we present five different multi-touch mouse implementations. Each explores a different touch sensing strategy, which leads to differing form-factors and hence interactive possibilities. In addition to the detailed description of hardware and software implementations of our prototypes, we discuss the relative strengths, limitations and affordances of these novel input devices as informed by the results of a preliminary user study.
Interactive environment-aware display bubbles	We present a novel display metaphor which extends traditional tabletop projections in collaborative environments by introducing freeform, environment-aware display representations and a matching set of interaction schemes. For that purpose, we map personalized widgets or ordinary computer applications that have been designed for a conventional, rectangular layout into space-efficient bubbles whose warping is performed with a potential-based physics approach. With a set of interaction operators based on laser pointer tracking, these freeform displays can be transformed and elastically deformed using focus and context visualization techniques. We also provide operations for intuitive instantiation of bubbles, cloning, cut & pasting, deletion and grouping in an interactive way, and we allow for user-drawn annotations and text entry using a projected keyboard. Additionally, an optional environment-aware adaptivity of the displays is achieved by imperceptible, realtime scanning of the projection geometry. Subsequently, collision-responses of the bubbles with non-optimal surface parts are computed in a rigid body simulation. The extraction of the projection surface properties runs concurrently with the main application of the system. Our approach is entirely based on off the-shelf, low-cost hardware including DLP-projectors and FireWire cameras.
Clipping lists and change borders: improving multitasking efficiency with peripheral information design	Information workers often have to balance many tasks and interruptions. In this work, we explore peripheral display techniques that improve multitasking efficiency by helping users maintain task flow, know when to resume tasks, and more easily reacquire tasks. Specifically, we compare two types of abstraction that provide different task information: semantic content extraction, which displays only the most relevant content in a window, and change detection, which signals when a change has occurred in a window (all de-signed as modifications to Scalable Fabric [17]). Results from our user study suggest that semantic content extraction improves multitasking performance more so than either change detection or our base case of scaling. Results also show that semantic content extraction provides significant benefits to task flow, resumption timing, and reacquisition. We discuss the implication of these findings on the design of peripheral interfaces that support multitasking.
Table-centric interactive spaces for real-time collaboration	Tables have historically played a key role in many real-time collaborative environments, often referred to as "war rooms". Today, these environments have been transformed by computational technology into spaces with large vertical displays surrounded by numerous desktop computers. However, despite significant research activity in the area of tabletop computing, very little is known about how to best integrate a digital tabletop into these multi-surface environments. In this paper, we identify various design requirements for the implementation of a system intended to support such an environment. We then present a set of designs that demonstrate how an interactive tabletop can be used in a real-time operations center to facilitate collaborative situation-assessment and decision-making.
LensMouse: augmenting the mouse with an interactive touch display	We introduce LensMouse, a novel device that embeds a touch-screen display -- or tangible 'lens' -- onto a mouse. Users interact with the display of the mouse using direct touch, whilst also performing regular cursor-based mouse interactions. We demonstrate some of the unique capabili-ties of such a device, in particular for interacting with auxil-iary windows, such as toolbars, palettes, pop-ups and dia-log-boxes. By migrating these windows onto LensMouse, challenges such as screen real-estate use and window man-agement can be alleviated. In a controlled experiment, we evaluate the effectiveness of LensMouse in reducing cursor movements for interacting with auxiliary windows. We also consider the concerns involving the view separation that results from introducing such a display-based device. Our results reveal that overall users are more effective with LenseMouse than with auxiliary application windows that are managed either in single or dual-monitor setups. We conclude by presenting other application scenarios that LensMouse could support.
Determining the benefits of direct-touch, bimanual, and multifinger input on a multitouch workstation	Multitouch workstations support direct-touch, bimanual, and multifinger interaction. Previous studies have separately examined the benefits of these three interaction attributes over mouse-based interactions. In contrast, we present an empirical user study that considers these three interaction attributes together for a single task, such that we can quantify and compare the performances of each attribute. In our experiment users select multiple targets using either a mouse-based workstation equipped with one mouse, or a multitouch workstation using either one finger, two fingers (one from each hand), or multiple fingers. We find that the fastest multitouch condition is about twice as fast as the mouse-based workstation, independent of the number of targets. Direct-touch with one finger accounts for an average of 83% of the reduction in selection time. Bimanual interaction, using at least two fingers, one on each hand, accounts for the remaining reduction in selection time. Further, we find that for novice multitouch users there is no significant difference in selection time between using one finger on each hand and using any number of fingers for this task. Based on these observations we conclude with several design guidelines for developing multitouch user interfaces.
Indirect mappings of multi-touch input using one and two hands	Touchpad and touchscreen interaction using multiple fingers is emerging as a valuable form of high-degree-of-freedom input. While bimanual interaction has been extensively studied, touchpad interaction using multiple fingers of the same hand is not yet well understood. We describe two experiments on user perception and control of multi-touch interaction using one and two hands. The first experiment addresses how to maintain perceptual-motor compatibility in multi-touch interaction, while the second measures the separability of control of degrees-of-freedom in the hands and fingers. Results indicate that two-touch interaction using two hands is compatible with control of two points, while twotouch interaction using one hand is compatible with control of a position, orientation, and hand-span. A slight advantage is found for two hands in separating the control of two positions.
Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays	Recent advances in sensing technology have enabled a new generation of tabletop displays that can sense multiple points of input from several users simultaneously. However, apart from a few demonstration techniques [17], current user interfaces do not take advantage of this increased input bandwidth. We present a variety of multifinger and whole hand gestural interaction techniques for these displays that leverage and extend the types of actions that people perform when interacting on real physical tabletops. Apart from gestural input techniques, we also explore interaction and visualization techniques for supporting shared spaces, awareness, and privacy. These techniques are demonstrated within a prototype room furniture layout application, called RoomPlanner.
Direct-touch vs. mouse input for tabletop displays	We investigate the differences -- in terms of bothquantitative performance and subjective preference -- between direct-touch and mouse input for unimanual andbimanual tasks on tabletop displays. The results of twoexperiments show that for bimanual tasks performed ontabletops, users benefit from direct-touch input. However,our results also indicate that mouse input may be moreappropriate for a single user working on tabletop tasksrequiring only single-point interaction.
HeatWave: thermal imaging for surface user interaction	We present HeatWave, a system that uses digital thermal imaging cameras to detect, track, and support user interaction on arbitrary surfaces. Thermal sensing has had limited examination in the HCI research community and is generally under-explored outside of law enforcement and energy auditing applications. We examine the role of thermal imaging as a new sensing solution for enhancing user surface interaction. In particular, we demonstrate how thermal imaging in combination with existing computer vision techniques can make segmentation and detection of routine interaction techniques possible in real-time, and can be used to complement or simplify algorithms for traditional RGB and depth cameras. Example interactions include (1) distinguishing hovering above a surface from touch events, (2) shape-based gestures similar to ink strokes, (3) pressure based gestures, and (4) multi-finger gestures. We close by discussing the practicality of thermal sensing for naturalistic user interaction and opportunities for future work.
Visual tracking of bare fingers for interactive surfaces	Visual tracking of bare fingers allows more direct manipulation of digital objects, multiple simultaneous users interacting with their two hands, and permits the interaction on large surfaces, using only commodity hardware. After presenting related work, we detail our implementation. Its design is based on our modeling of two classes of algorithms that are key to the tracker: Image Differencing Segmentation (IDS) and Fast Rejection Filters (FRF). We introduce a new chromatic distance for IDS and a FRF that is independent to finger rotation. The system runs at full frame rate (25 Hz) with an average total system latency of 80 ms, independently of the number of tracked fingers. When used in a controlled environment such as a meeting room, its robustness is satisfying for everyday use.
Moveable interactive projected displays using projector based tracking	Video projectors have typically been used to display images on surfaces whose geometric relationship to the projector remains constant, such as walls or pre-calibrated surfaces. In this paper, we present a technique for projecting content onto moveable surfaces that adapts to the motion and location of the surface to simulate an active display. This is accomplished using a projector based location tracking techinque. We use light sensors embedded into the moveable surface and project low-perceptibility Gray-coded patterns to first discover the sensor locations, and then incrementally track them at interactive rates. We describe how to reduce the perceptibility of tracking patterns, achieve interactive tracking rates, use motion modeling to improve tracking performance, and respond to sensor occlusions. A group of tracked sensors can define quadrangles for simulating moveable displays while single sensors can be used as control inputs. By unifying the tracking and display technology into a single mechanism, we can substantially reduce the cost and complexity of implementing applications that combine motion tracking and projected imagery.
Automated Facial Expression Classification and affect interpretation using infrared measurement of facial skin temperature variations	Machines would require the ability to perceive and adapt to affects for achieving artificial sociability. Most autonomous systems use Automated Facial Expression Classification (AFEC) and Automated Affect Interpretation (AAI) to achieve sociability. Varying lighting conditions, occlusion, and control over physiognomy can influence the real life performance of vision-based AFEC systems. Physiological signals provide complementary information for AFEC and AAI. We employed transient facial thermal features for AFEC and AAI. Infrared thermal images with participants' normal expression and intentional expressions of happiness, sadness, disgust, and fear were captured. Facial points that undergo significant thermal changes with a change in expression termed as Facial Thermal Feature Points (FTFPs) were identified. Discriminant analysis was invoked on principal components derived from the Thermal Intensity Values (TIVs) recorded at the FTFPs. The cross-validation and person-independent classification respectively resulted in 66.28&percnt; and 56.0&percnt; success rates. Classification significance tests suggest that (1) like other physiological cues, facial skin temperature also provides useful information about affective states and their facial expression; (2) patterns of facial skin temperature variation can complement other cues for AFEC and AAI; and (3) infrared thermal imaging may help achieve artificial sociability in robots and autonomous systems.
Going beyond the display: a surface technology with an electronically switchable diffuser	We introduce a new type of interactive surface technology based on a switchable projection screen which can be made diffuse or clear under electronic control. The screen can be continuously switched between these two states so quickly that the change is imperceptible to the human eye. It is then possible to rear-project what is perceived as a stable image onto the display surface, when the screen is in fact transparent for half the time. The clear periods may be used to project a second, different image through the display onto objects held above the surface. At the same time, a camera mounted behind the screen can see out into the environment. We explore some of the possibilities this type of screen technology affords, allowing surface computing interactions to extend 'beyond the display'. We present a single self-contained system that combines these off-screen interactions with more typical multi-touch and tangible surface interactions. We describe the technical challenges in realizing our system, with the aim of allowing others to experiment with these new forms of interactive surfaces.
Interacting with human physiology	We propose a novel system that incorporates physiological monitoring as part of the human-computer interface. The sensing element is a thermal camera that is employed as a computer peripheral. Through bioheat modeling of facial imagery almost the full range of vital signs can be extracted, including localize blood flow, cardiac pulse, and breath rate. This physiological information can then be used to draw inferences about a variety of health symptoms and psychological states. Our research aims to realize the notion of desktop health monitoring and create truly collaborative interactions in which humans and machines are both observing and responding.
O' game, can you feel my frustration?: improving user's gaming experience via stresscam	One of the major challenges of video game design is to have appropriate difficulty levels for users in order to maximize the entertainment value of the game. Game players may lose interests if a game is either too easy or too difficult. This paper presents a novel methodology to improve user's experience in computer games by automatically adjusting the level of the game difficulty. The difficulty level is computed from measurements of the facial physiology of the players at a distance. The measurements are based on the assumption that the players' performance during the game-playing session alters blood flow in the supraorbital region, which is an indirect measurement of increased mental activities. This alters heat dissipation, which can be monitored in a contact-free manner through a thermal imaging-based stress monitoring and analysis system, known as StressCam. In this work, we investigated on two primary objectives: (1) the feasibility of utilizing the facial physiology in automatically adjusting the difficulty level of the game and (2) the capability of the automatic difficulty level adjustment in improving game players' experience. We employed and extended a XNA video game for this study, and performed an in-depth, comparative usability evaluation on it. Our results show that the automatic difficulty adjustable system successfully maintains game players' interests and substantially outperforms traditional fixed-difficulty mode games. Although a number of issues of this preliminary study remain to be investigated further, this research opens a new direction that utilizes non-contact stress measurements for monitoring and further enhancing a variety of user-centric, interactive entertainment activities.
Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays	Recent advances in sensing technology have enabled a new generation of tabletop displays that can sense multiple points of input from several users simultaneously. However, apart from a few demonstration techniques [17], current user interfaces do not take advantage of this increased input bandwidth. We present a variety of multifinger and whole hand gestural interaction techniques for these displays that leverage and extend the types of actions that people perform when interacting on real physical tabletops. Apart from gestural input techniques, we also explore interaction and visualization techniques for supporting shared spaces, awareness, and privacy. These techniques are demonstrated within a prototype room furniture layout application, called RoomPlanner.
Real-Time Fingertip Tracking and Gesture Recognition	The authors introduce a fast and robust method for tracking a user's hands and multiple fingertips. They then demonstrate gesture recognition based on measured fingertip trajectories for augmented desk interface systems. This tracking method is capable of tracking multiple fingertips in a reliable manner even in a complex background under a dynamically changing lighting condition without any markers. The authors' gesture recognition system is particularly advantageous for human-computer interaction (HCI) in that users can achieve interactions based on symbolic gestures at the same time that they perform direct manipulation with their own hands and fingers.
Multiscale Fusion of Visible and Thermal IR Images for Illumination-Invariant Face Recognition	This paper describes a new software-based registration and fusion of visible and thermal infrared (IR) image data for face recognition in challenging operating environments that involve illumination variations. The combined use of visible and thermal IR imaging sensors offers a viable means for improving the performance of face recognition techniques based on a single imaging modality. Despite successes in indoor access control applications, imaging in the visible spectrum demonstrates difficulties in recognizing the faces in varying illumination conditions. Thermal IR sensors measure energy radiations from the object, which is less sensitive to illumination changes, and are even operable in darkness. However, thermal images do not provide high-resolution data. Data fusion of visible and thermal images can produce face images robust to illumination variations. However, thermal face images with eyeglasses may fail to provide useful information around the eyes since glass blocks a large portion of thermal energy. In this paper, eyeglass regions are detected using an ellipse fitting method, and replaced with eye template patterns to preserve the details useful for face recognition in the fused image. Software registration of images replaces a special-purpose imaging sensor assembly and produces co-registered image pairs at a reasonable cost for large-scale deployment. Face recognition techniques using visible, thermal IR, and data-fused visible-thermal images are compared using a commercial face recognition software (FaceIt®) and two visible-thermal face image databases (the NIST/Equinox and the UTK-IRIS databases). The proposed multiscale data-fusion technique improved the recognition accuracy under a wide range of illumination changes. Experimental results showed that the eyeglass replacement increased the number of correct first match subjects by 85% (NIST/Equinox) and 67% (UTK-IRIS).
AnglePose: robust, precise capacitive touch tracking via 3d orientation estimation	We present a finger-tracking system for touch-based interaction which can track 3D finger angle in addition to position, using low-resolution conventional capacitive sensors, therefore compensating for the inaccuracy due to pose variation in conventional touch systems. Probabilistic inference about the pose of the finger is carried out in real-time using a particle filter; this results in an efficient and robust pose estimator which also gives appropriate uncertainty estimates. We show empirically that tracking the full pose of the finger results in greater accuracy in pointing tasks with small targets than competitive techniques. Our model can detect and cope with different finger sizes and the use of either fingers or thumbs, bringing a significant potential for improvement in one-handed interaction with touch devices. In addition to the gain in accuracy we also give examples of how this technique could open up the space of novel interactions.
Detecting and leveraging finger orientation for interaction with direct-touch surfaces	Current interactions on direct-touch interactive surfaces are often modeled based on properties of the input channel that are common in traditional graphical user interfaces (GUI) such as x-y coordinate information. Leveraging additional information available on the surfaces could potentially result in richer and novel interactions. In this paper we specifically explore the role of finger orientation. This property is typically ignored in touch-based interactions partly because of the ambiguity in determining it solely from the contact shape. We present a simple algorithm that unambiguously detects the directed finger orientation vector in real-time from contact information only, by considering the dynamics of the finger landing process. Results of an experimental evaluation show that our algorithm is stable and accurate. We then demonstrate how finger orientation can be leveraged to enable novel interactions and to infer higher-level information such as hand occlusion or user position. We present a set of orientation-aware interaction techniques and widgets for direct-touch surfaces.
MicroRolls: expanding touch-screen input vocabulary by distinguishing rolls vs. slides of the thumb	The input vocabulary for touch-screen interaction on handhelds is dramatically limited, especially when the thumb must be used. To enrich that vocabulary we propose to discriminate, among thumb gestures, those we call MicroRolls, characterized by zero tangential velocity of the skin relative to the screen surface. Combining four categories of thumb gestures, Drags, Swipes, Rubbings and MicroRolls, with other classification dimensions, we show that at least 16 elemental gestures can be automatically recognized. We also report the results of two experiments showing that the roll vs. slide distinction facilitates thumb input in a realistic copy and paste task, relative to existing interaction techniques.
Hover widgets: using the tracking state to extend the capabilities of pen-operated devices	We present Hover Widgets, a new technique for increasing the capabilities of pen-based interfaces. Hover Widgets are implemented by using the pen movements above the display surface, in the tracking state. Short gestures while hovering, followed by a pen down, access the Hover Widgets, which can be used to activate localized interface widgets. By using the tracking state movements, Hover Widgets create a new command layer which is clearly distinct from the input layer of a pen interface. In a formal experiment Hover Widgets were found to be faster than a more traditional command activation technique, and also reduced errors due to divided attention.
In-air typing interface for mobile devices with vibration feedback	Recently the miniaturization of mobile devices has progressed and such devices are difficult to have input interface that has wide operation area on their surface. Conventional input interface on a cell phone, such as a touch panel or keypad, has limited operation area. There has been many approaches to handle this problem, but they require users to wear some physical devices[Harrison and Hudson 2009] or to use in some specific environments[Roeber et al. 2003].
Precise selection techniques for multi-touch screens	The size of human fingers and the lack of sensing precision can make precise touch screen interactions difficult. We present a set of five techniques, called Dual Finger Selections, which leverage the recent development of multi-touch sensitive displays to help users select very small targets. These techniques facilitate pixel-accurate targeting by adjusting the control-display ratio with a secondary finger while the primary finger controls the movement of the cursor. We also contribute a "clicking" technique, called SimPress, which reduces motion errors during clicking and allows us to simulate a hover state on devices unable to sense proximity. We implemented our techniques on a multi-touch tabletop prototype that offers computer vision-based tracking. In our formal user study, we tested the performance of our three most promising techniques (Stretch, X-Menu, and Slider) against our baseline (Offset), on four target sizes and three input noise levels. All three chosen techniques outperformed the control technique in terms of error rate reduction and were preferred by our participants, with Stretch being the overall performance and preference winner.
Separability of spatial manipulations in multi-touch interfaces	Multi-touch interfaces allow users to translate, rotate, and scale digital objects in a single interaction. However, this freedom represents a problem when users intend to perform only a subset of manipulations. A user trying to scale an object in a print layout program, for example, might find that the object was also slightly translated and rotated, interfering with what was already carefully laid out earlier. We implemented and tested interaction techniques that allow users to select a subset of manipulations. Magnitude Filtering eliminates transformations (e.g., rotation) that are small in magnitude. Gesture Matching attempts to classify the user's input into a subset of manipulation gestures. Handles adopts a conventional single-touch handles approach for touch input. Our empirical study showed that these techniques significantly reduce errors in layout, while the Handles technique was slowest. A variation of the Gesture Matching technique presented the best combination of speed and control, and was favored by participants.
Of passwords and people: measuring the effect of password-composition policies	Text-based passwords are the most common mechanism for authenticating humans to computer systems. To prevent users from picking passwords that are too easy for an adversary to guess, system administrators adopt password-composition policies (e.g., requiring passwords to contain symbols and numbers). Unfortunately, little is known about the relationship between password-composition policies and the strength of the resulting passwords, or about the behavior of users (e.g., writing down passwords) in response to different policies. We present a large-scale study that investigates password strength, user behavior, and user sentiment across four password-composition policies. We characterize the predictability of passwords by calculating their entropy, and find that a number of commonly held beliefs about password composition and strength are inaccurate. We correlate our results with user behavior and sentiment to produce several recommendations for password-composition policies that result in strong passwords without unduly burdening users.
A large-scale study of web password habits	We report the results of a large scale study of password use andpassword re-use habits. The study involved half a million users over athree month period. A client component on users' machines recorded a variety of password strength, usage and frequency metrics. This allows us to measure or estimate such quantities as the average number of passwords and average number of accounts each user has, how many passwords she types per day, how often passwords are shared among sites, and how often they are forgotten. We get extremely detailed data on password strength, the types and lengths of passwords chosen, and how they vary by site. The data is the first large scale study of its kind, and yields numerous other insights into the role the passwords play in users' online experience.
Testing metrics for password creation policies by attacking large sets of revealed passwords	In this paper we attempt to determine the effectiveness of using entropy, as defined in NIST SP800-63, as a measurement of the security provided by various password creation policies. This is accomplished by modeling the success rate of current password cracking techniques against real user passwords. These data sets were collected from several different websites, the largest one containing over 32 million passwords. This focus on actual attack methodologies and real user passwords quite possibly makes this one of the largest studies on password security to date. In addition we examine what these results mean for standard password creation policies, such as minimum password length, and character set requirements.
The true cost of unusable password policies: password use in the wild	HCI research published 10 years ago pointed out that many users cannot cope with the number and complexity of passwords, and resort to insecure workarounds as a consequence. We present a study which re-examined password policies and password practice in the workplace today. 32 staff members in two organisations kept a password diary for 1 week, which produced a sample of 196 passwords. The diary was followed by an interview which covered details of each password, in its context of use. We find that users are in general concerned to maintain security, but that existing security policies are too inflexible to match their capabilities, and the tasks and contexts in which they operate. As a result, these password policies can place demands on users which impact negatively on their productivity and, ultimately, that of the organisation. We conclude that, rather than focussing password policies on maximizing password strength and enforcing frequency alone, policies should be designed using HCI principles to help the user to set an appropriately strong password in a specific context of use.
A diary study of password usage in daily life	While past work has examined password usage on a specific computer, web site, or organization, there is little work examining overall password usage in daily life. Through a diary study, we examine all usage of passwords, and offer some new findings based on quantitative analyses regarding how often people log in, where they log in, and how frequently people use foreign computers. Our analysis also confirms or updates existing statistics about password usage patterns. We also discuss some implications for design as well as security education.
A large-scale study of web password habits	We report the results of a large scale study of password use andpassword re-use habits. The study involved half a million users over athree month period. A client component on users' machines recorded a variety of password strength, usage and frequency metrics. This allows us to measure or estimate such quantities as the average number of passwords and average number of accounts each user has, how many passwords she types per day, how often passwords are shared among sites, and how often they are forgotten. We get extremely detailed data on password strength, the types and lengths of passwords chosen, and how they vary by site. The data is the first large scale study of its kind, and yields numerous other insights into the role the passwords play in users' online experience.
Password management strategies for online accounts	Given the widespread use of password authentication in online correspondence, subscription services, and shopping, there is growing concern about identity theft. When people reuse their passwords across multiple accounts, they increase their vulnerability; compromising one password can help an attacker take over several accounts. Our study of 49 undergraduates quantifies how many passwords they had and how often they reused these passwords. The majority of users had three or fewer passwords and passwords were reused twice. Furthermore, over time, password reuse rates increased because people accumulated more accounts but did not create more passwords. Users justified their habits. While they wanted to protect financial data and personal communication, reusing passwords made passwords easier to manage. Users visualized threats from human attackers, particularly viewing those close to them as the most motivated and able attackers; however, participants did not separate the human attackers from their potentially automated tools. They sometimes failed to realize that personalized passwords such as phone numbers can be cracked given a large enough dictionary and enough tries. We discuss how current systems support poor password practices. We also present potential changes in website authentication systems and password managers.
Do strong web passwords accomplish anything?	We find that traditional password advice given to users is somewhat dated. Strong passwords do nothing to protect online users from password stealing attacks such as phishing and keylogging, and yet they place considerable burden on users. Passwords that are too weak of course invite brute-force attacks. However, we find that relatively weak passwords, about 20 bits or so, are sufficient to make brute-force attacks on a single account unrealistic so long as a "three strikes" type rule is in place. Above that minimum it appears that increasing password strength does little to address any real threat. If a larger credential space is needed it appears better to increase the strength of the userID's rather than the passwords. For large institutions this is just as effective in deterring bulk guessing attacks and is a great deal better for users. For small institutions there appears little reason to require strong passwords for online accounts.
The true cost of unusable password policies: password use in the wild	HCI research published 10 years ago pointed out that many users cannot cope with the number and complexity of passwords, and resort to insecure workarounds as a consequence. We present a study which re-examined password policies and password practice in the workplace today. 32 staff members in two organisations kept a password diary for 1 week, which produced a sample of 196 passwords. The diary was followed by an interview which covered details of each password, in its context of use. We find that users are in general concerned to maintain security, but that existing security policies are too inflexible to match their capabilities, and the tasks and contexts in which they operate. As a result, these password policies can place demands on users which impact negatively on their productivity and, ultimately, that of the organisation. We conclude that, rather than focussing password policies on maximizing password strength and enforcing frequency alone, policies should be designed using HCI principles to help the user to set an appropriately strong password in a specific context of use.
Security in the wild: user strategies for managing security as an everyday, practical problem	Ubiquitous and mobile technologies create new challenges for system security. Effective security solutions depend not only on the mathematical and technical properties of those solutions, but also on people’s ability to understand them and use them as part of their work. As a step towards solving this problem, we have been examining how people experience security as a facet of their daily life, and how they routinely answer the question, “is this system secure enough for what I want to do?” We present a number of findings concerning the scope of security, attitudes towards security, and the social and organizational contexts within which security concerns arise, and point towards emerging technical solutions.
Understanding people and animals: the use of a positioning system in ordinary human-canine interaction	Animals are increasingly integrated in interactive contexts depending on digital technologies. The current and future use of such technologies is a relevant topic for HCI research. However, the field is struggling with the inherent problem of 'nteraction' in understanding interaction with animals. We argue for a way forward based on an ethnomethodological perspective on anthropomorphism, with a focus on manifest interaction. Drawing upon a field study of hunters' use of a GPS dog tracking-device, we discuss how interaction between dogs and humans is affected when new technology is introduced. The GPS data is situated and interpreted by the dog handler, and supports the hunter's work of dealing with the dogs' intentions. This opens up for new forms of interactions with the dog. When studying and designing for interaction between humans and animals we should move beyond merely looking at dyadic relationships, and also consider the social organization of the interaction.
Transferring qualities from horseback riding to design	We see more and more attempts to design for bodily experiences with digital technology, but it is a notably challenging design task. What are the possible bodily experiences we may aim to design for, and how can we characterise them? By analysing a horseback riding experience, we came to identify the following themes: (1) how certain kinds of bodily experiences are best understood through experiencing them yourself -- the bodily ways of knowing, (2) how rhythm and balance create for particularly strong physical experiences of this kind, (3) how movement and emotion coincide in these experiences, (4) how the movement between seeing our own bodies as objects vs experiencing in and through our bodies is one of the ways we come to learn the language of expressing and understanding bodily action, and (5) how this in turn lets us describe the sensitive and delicate relationship of wordless signs and signals that represent, in the case described, two bodily agents -- a human and a horse. When the human-horse relationship is really successful, it can be described as rare moments of becoming a centaur. We translate these themes into design considerations for bodily interactions.
Hunting for fun: solitude and attentiveness in collaboration	The design of online collaborative computer games and pervasive games can learn from the everyday practice of deer hunting. We present an ethnographic study revealing how hunters fine-tune their experience through temporal and spatial organization. The hunt is organized in a way that allows the hunters to balance between forms of collaboration ranging from solitude to face-to-face interaction, as well as between attentiveness and relaxation. Thus, the hunters deal with the task -- hunting down the prey -- while managing issues of enjoyment. We argue that understanding these experiential qualities is relevant for collaborative gaming, and adds to our understanding of leisure.
Species-appropriate computer mediated interaction	Given the importance of our non-human companions, do we not want to extend social media to our nonhuman co-species? If "human computer interfaces" should be designed for "Anyone. Anywhere." (the theme of CHI 2001), then why not for all species? Recent pioneering efforts have shown that computer mediated interactions between humans and dogs, cats, chickens, cows, hamsters, and other species are technically possible. These efforts excite the imagination and challenge our understanding the basic nature of computer mediated interaction.
Machine intelligence	Under certain conditions, we appear willing to see and interact with computing machines as though they exhibited intelligence, at least an intelligence of sorts. Using examples from AI and robotics research, as well as a selection of relevant art installations and anthropological fieldwork, this paper reflects on some of our interactions with the kinds of machines we seem ready to treat as intelligent. Broadly, it is suggested that ordinary, everyday ideas of intelligence are not fixed, but rather actively seen and enacted in the world. As such, intelligence does not just belong to the province of the human mind, but can emerge in quite different, unexpected forms in things. For HCI, it is proposed this opens up a new set of possibilities for design; examining the ways intelligence is seen and enacted gives rise to a very different way of thinking about the intersection between human and machine, and thus promotes some radically new types of interactions with computing machines.
Designing sports: a framework for exertion games	Exertion games require investing physical effort. The fact that such games can support physical health is tempered by our limited understanding of how to design for engaging exertion experiences. This paper introduces the Exertion Framework as a way to think and talk about Exertion Games, both for their formative design and summative analysis. Our Exertion Framework is based on the ways in which we can conceive of the body investing in game-directed exertion, supported by four perspectives on the body (the Responding Body, Moving Body, Sensing Body and Relating Body) and three perspectives on gaming (rules, play and context). The paper illustrates how this framework was derived from prior systems and theory, and presents a case study of how it has been used to inspire novel exertion interactions.
Jogging over a distance between Europe and Australia	Exertion activities, such as jogging, require users to invest intense physical effort and are associated with physical and social health benefits. Despite the benefits, our understanding of exertion activities is limited, especially when it comes to social experiences. In order to begin understanding how to design for technologically augmented social exertion experiences, we present "Jogging over a Distance", a system in which spatialized audio based on heart rate allowed runners as far apart as Europe and Australia to run together. Our analysis revealed how certain aspects of the design facilitated a social experience, and consequently we describe a framework for designing augmented exertion activities. We make recommendations as to how designers could use this framework to aid the development of future social systems that aim to utilize the benefits of exertion.
Making sense of group interaction in an ambient intelligent environment for physical play	This paper presents the results of a study on group interaction with a prototype known as socio-ec(h)o. socio-ec(h)o explores the design of sensing and display, user modeling, and interaction in an embedded interaction system utilizing a game structure. Our study involved the playing of our prototype system by thirty-six (36) participants grouped into teams of four (4). Our aim was to determine heuristics that we could use to further design the interaction and user model approaches for group and embodied interaction systems. We analyzed group interaction and performance based on factors of team cohesion and goal focus. We found that with our system, these factors alone could not explain performance. However, when transitions in the degrees of each factor, i.e. high, medium or low are considered, a clearer picture for performance emerges. The significance of the results is that they describe recognizable factors for positive group interaction.
Considerations for the design of exergames	Exergaming is the use of video games in an exercise activity. In this paper we consider game design for successful exergames. To do this, we review the history of exergaming and the current state of research in this field. We find that there exists some research aimed at evaluating the physical and health characteristics of exergames, but research on how to design exercise games is still in the early stages. From an analysis of this information, and drawing on established principles from sports science for the prescription of exercise programs, we then attempt to identify success factors to guide designers of exergaming systems.
Kinesthetic interaction: revealing the bodily potential in interaction design	Within the Human-Computer Interaction community there is a growing interest in designing for the whole body in interaction design. The attempts aimed at addressing the body have very different outcomes spanning from theoretical arguments for understanding the body in the design process, to more practical examples of designing for bodily potential. This paper presents Kinesthetic Interaction as a unifying concept for describing the body in motion as a foundation for designing interactive systems. Based on the theoretical foundation for Kinesthetic Interaction, a conceptual framework is introduced to reveal bodily potential in relation to three design themes --- kinesthetic development, kinesthetic means and kinesthetic disorder; and seven design parameters --- engagement, sociality, movability, explicit motivation, implicit motivation, expressive meaning and kinesthetic empathy. The framework is a tool to be utilized when analyzing existing designs, as well as developing designs exploring new ways of designing kinesthetic interactions.
When clapping data speaks to Wii: physical creativity and performative interaction in playground games and songs	In this paper, we explore how exertion interfaces can promote physical creativity and the role that this might play in performative interaction. In particular, we are interested in exploring how to design and develop devices and applications which use physical interaction, or exertion, to promote performative interaction or the witting transitions between observing, participating and performing. Using the Nintendo Wii as an exertion interface, we are updating, analysing and representing a small selection of clapping games found in the Opie Collection of Children's Games and Songs in the British Library and emerging from ethnographic studies of playgrounds in London and Sheffield, UK. We describe the Physics of clapping and associated technical issues, the design of a low-fi, open source exertion interface and the analysis of a participant study. We suggest guidelines for designing for physical creativity, namely kinesthetic literacy, performative interaction and believability, and conclude with a discussion of future considerations.
How bodies matter: five themes for interaction design	Our physical bodies play a central role in shaping human experience in the world, understandingof the world, and interactions in the world. This paper draws on theories of embodiment - from psychology, sociology, and philosophy - synthesizing five themes we believe are particularly salient for interaction design: thinking through doing, performance, visibility, risk, and thick practice. We intro-duce aspects of human embodied engagement in the world with the goal of inspiring new interaction design ap-proaches and evaluations that better integrate the physical and computational worlds.
The Frame of the Game: Blurring the Boundary between Fiction and Reality in Mobile Experiences	Mobile experiences that take place in public settings such as on city streets create new opportunities for interweaving the fictional world of a performance or game with the everyday physical world. A study of a touring performance reveals how designers generated excitement and dramatic tension by implicating bystanders and encouraging the (apparent) crossing of normal boundaries of behaviour. The study also shows how designers dealt with associated risks through a process of careful orchestration. Consequently, we extend an existing framework for designing spectator interfaces with the concept of performance frames, enabling us to distinguish audience from bystanders. We conclude that using ambiguity to blur the frame can be a powerful design tactic, empowering players to willingly suspend disbelief, so long as a safety-net of orchestration ensures that they do not stray into genuine difficulty.
Cat cat revolution: an interspecies gaming experience	Despite owners' desires to include pets in their everyday activities, pets have yet to be included in the digital gaming experience. The Cat Cat Revolution (CCR) is a digital game of cat and mouse that allows cats to participate in play through a species-appropriate interface. The game applies Human-Computer Interaction (HCI) principles to pets and casts pets as participants in the gaming experience. During the pilot study, pet owners characterized CCR as a mutually positive experience, describing the game as a fun way to play. CCR explores the effects of including pets in the digital gaming experience.
Machine intelligence	Under certain conditions, we appear willing to see and interact with computing machines as though they exhibited intelligence, at least an intelligence of sorts. Using examples from AI and robotics research, as well as a selection of relevant art installations and anthropological fieldwork, this paper reflects on some of our interactions with the kinds of machines we seem ready to treat as intelligent. Broadly, it is suggested that ordinary, everyday ideas of intelligence are not fixed, but rather actively seen and enacted in the world. As such, intelligence does not just belong to the province of the human mind, but can emerge in quite different, unexpected forms in things. For HCI, it is proposed this opens up a new set of possibilities for design; examining the ways intelligence is seen and enacted gives rise to a very different way of thinking about the intersection between human and machine, and thus promotes some radically new types of interactions with computing machines.
Antiquarian answers: book restoration as a resource for design	As technologies age, they experience wear and degradation, sometimes resulting in loss of functionality. In response, parts are replaced and software is updated. Yet restoration - the process of returning something to a previous condition, often regardless of its instrumental value -"is a relatively rare practice with computational technologies. The aim of this paper is to enrich HCI design practices by considering the material qualities of restoration. We consider what makes a technology worth restoring and what constitutes the process of restoration by examining data collected from a three-month apprenticeship-based qualitative study of bookbinding. Building on relevant literatures, we offer antiquarian books -"long-established information technologies - as a lens onto the ways values are enacted through material engagements. We conclude with a discussion of restoration's role in HCI.
Learning from IKEA hacking: i'm not one to decoupage a tabletop and call it a day.	We present a qualitative study based on interviews with nine IKEA Hackers - people who go online to share the process of repurposing IKEA products to create personalized objects. Whether they were making a self-conscious artistic statement or simply modifying a towel rack to fit in a small bathroom, IKEA hackers illuminate an emergent practice that provides insights into contemporary changes in creativity. We discuss the motivations for IKEA hacking and explore the impact of information technology on do-it-yourself culture, design, and HCI.
Empowering products: personal identity through the act of appropriation	This paper explores the relationship between personal identity and the act of appropriating digital objects in the home--specifically do-it-yourself--to inform the design of empowering products. It reports ongoing research and provides a preliminary analysis of the Steampunk movement as a case study for personal appropriation. Appropriation-identity design guidelines are provided as a result of the data analysis.
Interaction with magic lenses: real-world validation of a Fitts' Law model	Rohs and Oulasvirta (2008) proposed a two-component Fitts' law model for target acquisition with magic lenses in mobile augmented reality (AR) with 1) a physical pointing phase, in which the target can be directly observed on the background surface, and 2) a virtual pointing phase, in which the target can only be observed through the device display. The model provides a good fit (R2=0.88) with laboratory data, but it is not known if it generalizes to real-world AR tasks. In the present outdoor study, subjects (N=12) did building-selection tasks in an urban area. The differences in task characteristics to the laboratory study are drastic: targets are three-dimensional and they vary in shape, size, z-distance, and visual context. Nevertheless, the model yielded an R2 of 0.80, and when using effective target width an R2 of 0.88 was achieved.
A probabilistic approach to modeling two-dimensional pointing	We investigate and model two-dimensional pointing where the target distance and size vary as does the angle of movement. We first study the spread of hits in a rapid approximate pointing task at varied distances and movement angles. Consistent with the literature, our results show that the spread of hits along the movement direction deviate more than the spread of hits in the direction perpendicular to movement, and both spreads increase with distance. Based on the distribution of this spread of hits, we propose and validate a new probabilistic model that describes two-dimensional pointing. Unlike previous models, our model accounts for more variables of two-dimensional pointing and can be generalized to any target shape, size, orientation, location, and dimension. In contrast to previous work, which suggests that target height has minimal impact on performance when it is larger than the width, our results show that, even when height is greater than width, it can significantly impact movement time.
Target acquisition with camera phones when used as magic lenses	When camera phones are used as magic lenses in handheld augmented reality applications involving wall maps or posters, pointing can be divided into two phases: (1) an initial coarse physical pointing phase, in which the target can be directly observed on the background surface, and (2) a fine-control virtual pointing phase, in which the target can only be observed through the device display. In two studies, we show that performance cannot be adequately modeled with standard Fitts' law, but can be adequately modeled with a two-component modification. We chart the performance space and analyze users' target acquisition strategies in varying conditions. Moreover, we show that the standard Fitts' law model does hold for dynamic peephole pointing where there is no guiding background surface and hence the physical pointing component of the extended model is not needed. Finally, implications for the design of magic lens interfaces are considered.
Dips and ceilings: understanding and supporting transitions to expertise in user interfaces	Interface guidelines encourage designers to include shortcut mechanisms that enable high levels of expert performance, but prior research has demonstrated that few users switch to using them. To help understand how interfaces can better support a transition to expert performance we develop a framework of the interface and human factors influencing expertise development. We then present a system called Blur that addresses three main problems in promoting the transition: prompting an initial switch to expert techniques, minimising the performance dip arising from the switch, and enabling a high performance ceiling. Blur observes the user's interaction with unaltered desktop applications and uses calm notification to support learning and promote awareness of an alternative hot command interface. An empirical study validates Blur's design, showing that users make an early and sustained switch to hot commands, and that doing so improves their performance and satisfaction.
A survey of software learnability: metrics, methodologies and guidelines	It is well-accepted that learnability is an important aspect of usability, yet there is little agreement as to how learnability should be defined, measured, and evaluated. In this paper, we present a survey of the previous definitions, metrics, and evaluation methodologies which have been used for software learnability. Our survey of evaluation methodologies leads us to a new question-suggestion protocol, which, in a user study, was shown to expose a significantly higher number of learnability issues in comparison to a more traditional think-aloud protocol. Based on the issues identified in our study, we present a classification system of learnability issues, and demonstrate how these categories can lead to guidelines for addressing the associated challenges.
Ambient help	In this paper we present Ambient Help, a system that supports opportunistic learning by providing automatic, context-sensitive learning resources while a user works. Multiple videos and textual help resources are presented ambiently on a secondary display. We define and examine a collection of design consideration for this type of interface. After describing our implementation details, we report on an experiment which shows that Ambient Help supports finding more helpful information, while not having a negative impact on the user's productivity, as compared to a traditional help condition.
Technology-mediated interruption management	Previous research into providing interpersonal technology-mediated interruption management support has predominantly been conducted from a paradigmatic standpoint that focused on modeling the context of the person being interrupted (interruptee) such as his/her mental workload, activity and location as a means to identify opportune/inopportune moments for communication. However, the utility of this approach and the associated design implications are questioned by the interruption value evaluation paradigm, which holds that interpersonal interruption management decisions are often made by people assessing factors such as who the interruption is from and what it is about (the relational context). To assess the validity of the competing assumptions underlining these paradigms about everyday interpersonal interruption management, a field study of interruption management practices in everyday cell phone use was conducted. Analysis of 1201 incoming calls from our experience sampling method study of cell phone use shows that ''who'' is calling is used most of the time (87.4%) by individuals to make deliberate call handling decisions (N=834), in contrast to the interruptee's current local social (34.9%) or cognitive (43%) contexts. Building on these findings, we present a theoretical framework that aids in understanding the design space of interruption management tools that focus on reducing uncertainty of the interruption context to improve interruption management decisions.
Heuristic evaluation of ambient displays	We present a technique for evaluating the usability and effectiveness of ambient displays. Ambient displays are abstract and aesthetic peripheral displays portraying non-critical information on the periphery of a user's attention. Although many innovative displays have been published, little existing work has focused on their evaluation, in part because evaluation of ambient displays is difficult and costly. We adapted a low-cost evaluation technique, heuristic evaluation, for use with ambient displays. With the help of ambient display designers, we defined a modified set of heuristics. We compared the performance of Nielsen's heuristics and our heuristics on two ambient displays. Evaluators using our heuristics found more, severe problems than evaluators using Nielsen's heuristics. Additionally, when using our heuristics, 3-5 evaluators were able to identify 40--60% of known usability issues. This implies that heuristic evaluation is an effective technique for identifying usability issues with ambient displays.
A survey of software learnability: metrics, methodologies and guidelines	It is well-accepted that learnability is an important aspect of usability, yet there is little agreement as to how learnability should be defined, measured, and evaluated. In this paper, we present a survey of the previous definitions, metrics, and evaluation methodologies which have been used for software learnability. Our survey of evaluation methodologies leads us to a new question-suggestion protocol, which, in a user study, was shown to expose a significantly higher number of learnability issues in comparison to a more traditional think-aloud protocol. Based on the issues identified in our study, we present a classification system of learnability issues, and demonstrate how these categories can lead to guidelines for addressing the associated challenges.
Prefab: implementing advanced behaviors using pixel-based reverse engineering of interface structure	Current chasms between applications implemented with different user interface toolkits make it difficult to implement and explore potentially important interaction techniques in new and existing applications, limiting the progress and impact of human-computer interaction research. We examine an approach based in the single most common characteristic of all graphical user interface toolkits, that they ultimately paint pixels to a display. We present Prefab, a system for implementing advanced behaviors through the reverse engineering of the pixels in graphical interfaces. Informed by how user interface toolkits paint interfaces, Prefab features a separation of the modeling of widget layout from the recognition of widget appearance. We validate Prefab in implementations of three applications: target-aware pointing techniques, Phosphor transitions, and Side Views parameter spectrums. Working only from pixels, we demonstrate a single implementation of these enhancements in complex existing applications created in different user interface toolkits running on different windowing systems.
Over the Shoulder Learning: Supporting Brief Informal Learning	The paper reviews work on informal technical help giving between colleagues. It concentrates on the process of how colleagues help each other to use a computer application to achieve a specific work task, contrasting this with the focus of much prior work on surrounding issues like the choice of whom to ask, information re-use and the larger work context of encouragement or otherwise of such learning. By an analysis of the literature and a study of office activity, some strengths and weaknesses of the method are identified. The difficulties of talking about the process of performing graphical user interface actions are explored. Various design implications for functionalities to improve the efficiency of informal help giving are explored. A consideration of informal learning can help in designing more effective, learnable, robust and acceptable CSCW systems. It also provides a different perspective on interface design as an exploration of features to support human---human interaction, using the computer screen as a shared resource to support this. In this way CSCW research may contribute to HCI research, since during such help giving, all computer systems are at least temporarily collaborative applications.
Query-Free News Search	Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast.We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84--91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics.
Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations	Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.
The SATIN Component System-A Metamodel for Engineering Adaptable Mobile Systems	Mobile computing devices, such as personal digital assistants and mobile phones, are becoming increasingly popular, smaller, and more capable. We argue that mobile systems should be able to adapt to changing requirements and execution environments. Adaptation requires the ability to reconfigure the deployed code base on a mobile device. Such reconfiguration is considerably simplified if mobile applications are component-oriented rather than monolithic blocks of code. We present the satin (System Adaptation Targeting Integrated Networks) component metamodel, a lightweight local component metamodel that offers the flexible use of logical mobility primitives to reconfigure the software system by dynamically transferring code. The metamodel is implemented in the satin middleware system, a component-based mobile computing middleware that uses the mobility primitives defined in the metamodel to reconfigure both itself and applications that it hosts. We demonstrate the suitability of satin in terms of lightweightedness, flexibility, and reusability for the creation of adaptable mobile systems by using it to implement, port, and evaluate a number of existing and new applications, including an active network platform developed for satellite communication at the European Space Agency. These applications exhibit different aspects of adaptation and demonstrate the flexibility of the approach and the advantages gained.
Engineering human trust in mobile system collaborations	Rapid advances in wireless networking technologies have enabled mobile devices to be connected anywhere and anytime. While roaming, applications on these devices dynamically discover hosts and services with whom interactions can be started. However, the fear of exposure to risky transactions with potentially unknown entities may seriously hinder collaboration. To minimise this risk, an engineering approach to the development of trust-based collaborations is necessary. This paper introduces hTrust, a human trust management model and framework that facilitates the construction of trust-aware mobile systems and applications. In particular, hTrust supports: reasoning about trust (trust formation), dissemination of trust information in the network (trust dissemination), and derivation of new trust relationships from previously formed ones (trust evolution). The framework views each mobile host as a self-contained unit, carrying along a portfolio of credentials that are used to prove its trustworthiness to other hosts in an ad-hoc mobile environment. Customising functions are defined to capture the natural disposition to trust of the user of the device inside our trust management framework.
Understanding Code Mobility	The technologies, architectures, and methodologies traditionally used to develop distributed applications exhibit a variety of limitations and drawbacks when applied to large scale distributed settings (e.g., the Internet). In particular, they fail in providing the desired degree of configurability, scalability, and customizability. To address these issues, researchers are investigating a variety of innovative approaches. The most promising and intriguing ones are those based on the ability of moving code across the nodes of a network, exploiting the notion of mobile code. As an emerging research field, code mobility is generating a growing body of scientific literature and industrial developments. Nevertheless, the field is still characterized by the lack of a sound and comprehensive body of concepts and terms. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. In turn, this limits our ability to fully exploit them in practice, and to further promote the research work on mobile code. Indeed, a significant symptom of this situation is the lack of a commonly accepted and sound definition of the term "mobile code" itself.This paper presents a conceptual framework for understanding code mobility. The framework is centered around a classification that introduces three dimensions: technologies, design paradigms, and applications. The contribution of the paper is two-fold. First, it provides a set of terms and concepts to understand and compare the approaches based on the notion of mobile code. Second, it introduces criteria and guidelines that support the developer in the identification of the classes of applications that can leverage off of mobile code, in the design of these applications, and, finally, in the selection of the most appropriate implementation technologies. The presentation of the classification is intertwined with a review of state-of-the-art in the field. Finally, the use of the classification is exemplified in a case study.
Developing adaptive groupware applications using a mobile component framework	A need exists to develop groupware systems that adapt to available resources and support user mobility. This paper presents DACIA, a system that provides mechanisms for building such groupware applications. Using DACIA, components of a groupware application can be moved to different hosts during execution, while maintaining communication connectivity with groupware services and other users. DACIA provides mechanisms that simplify building groupware for domains where users are mobile. New collaboration features can be also more easily implemented. DACIA is also applicable to non-mobile environments. We show its applicability to building groupware applications that can be reconfigured at run-time to adapt to changing user demands and resource constraints, for example, by relocating services or introducing new services. This paper describes the architecture of DACIA and its use in building adaptable groupware systems.
Architecture-Level Support for Software Component Deployment in Resource Constrained Environments	Software deployment comprises activities for installing or updating an already implemented software system. These activities include (1) deployment of a system onto a new host, (2) component upgrade in an existing system, (3) static analysis of the proposed system configuration, and (4) dynamic analysis of the configuration after the deployment. In this paper, we describe an approach that supports all four of these activities. The approach is specifically intended to support software deployment onto networks of distributed, mobile, highly resource constrained devices. Our approach is based on the principles of software architectures. In particular, we leverage our lightweight architectural implementation infrastructure to natively support deployment in resource constrained environments.
Prototyping dynamics: sharing multiple designs improves exploration, group rapport, and results	Prototypes ground group communication and facilitate decision making. However, overly investing in a single design idea can lead to fixation and impede the collaborative process. Does sharing multiple designs improve collaboration? In a study, participants created advertisements individually and then met with a partner. In the Share Multiple condition, participants designed and shared three ads. In the Share Best condition, participants designed three ads and selected one to share. In the Share One condition, participants designed and shared one ad. Sharing multiple designs improved outcome, exploration, sharing, and group rapport. These participants integrated more of their partner's ideas into their own subsequent designs, explored a more divergent set of ideas, and provided more productive critiques of their partner's designs. Furthermore, their ads were rated more highly and garnered a higher click-through rate when hosted online.
Design as exploration: creating interface alternatives through parallel authoring and runtime tuning	Creating multiple prototypes facilitates comparative reasoning, grounds team discussion, and enables situated exploration. However, current interface design tools focus on creating single artifacts. This paper introduces the Juxtapose code editor and runtime environment for designing multiple alternatives of both application logic and interface parameters. For rapidly comparing code alternatives, Juxtapose introduces selectively parallel source editing and execution. To explore parameter variations, Juxtapose automatically creates control interfaces for "tuning" application variables at runtime. This paper describes techniques to support design exploration for desktop, mobile, and physical interfaces, and situates this work in a larger design space of tools for explorative programming. A summative study of Juxtapose with 18 participants demonstrated that parallel editing and execution are accessible to interaction designers and that designers can leverage these techniques to survey more options, faster.
Authenticated Encryption: Relations among Notions and Analysis of the Generic Composition Paradigm	We consider two possible notions of authenticity for symmetric encryption schemes, namely integrity of plaintexts and integrity of ciphertexts, and relate them to the standard notions of privacy for symmetric encryption schemes by presenting implications and separations between all notions considered. We then analyze the security of authenticated encryption schemes designed by "generic composition," meaning making black-box use of a given symmetric encryption scheme and a given MAC. Three composition methods are considered, namely Encrypt-and-MAC plaintext, MAC-then-encrypt, and Encrypt-then-MAC. For each of these, and for each notion of security, we indicate whether or not the resulting scheme meets the notion in question assuming the given symmetric encryption scheme is secure against chosen-plaintext attack and the given MAC is unforgeable under chosen-message attack. We provide proofs for the cases where the answer is "yes" and counter-examples for the cases where the answer is "no."
Authenticated Encryption: Relations among Notions and Analysis of the Generic Composition Paradigm	We consider two possible notions of authenticity for symmetric encryption schemes, namely integrity of plaintexts and integrity of ciphertexts, and relate them to the standard notions of privacy for symmetric encryption schemes by presenting implications and separations between all notions considered. We then analyze the security of authenticated encryption schemes designed by "generic composition," meaning making black-box use of a given symmetric encryption scheme and a given MAC. Three composition methods are considered, namely Encrypt-and-MAC plaintext, MAC-then-encrypt, and Encrypt-then-MAC. For each of these, and for each notion of security, we indicate whether or not the resulting scheme meets the notion in question assuming the given symmetric encryption scheme is secure against chosen-plaintext attack and the given MAC is unforgeable under chosen-message attack. We provide proofs for the cases where the answer is "yes" and counter-examples for the cases where the answer is "no."
Encode-Then-Encipher Encryption: How to Exploit Nonces or Redundancy in Plaintexts for Efficient Cryptography	We investigate the following approach to symmetric encryption: first encode the message via some keyless transform, and then encipher the encoded message, meaning apply a permutation FK based on a shared key K. We provide conditions on the encoding functions and the cipher which ensure that the resulting encryption scheme meets strong privacy (eg. semantic security) and/or authenticity goals. The encoding can either be implemented in a simple way (eg. prepend a counter and append a checksum) or viewed as modeling existing redundancy or entropy already present in the messages, whereby encode-then-encipher encryption provides a way to exploit structured message spaces to achieve compact ciphertexts.
UMAC: Fast and Secure Message Authentication	We describe a message authentication algorithm, UMAC, which can authenticate messages (in software, on contemporary machines) roughly an order of magnitude faster than current practice (e.g., HMAC-SHA1), and about twice as fast as times previously reported for the universal hash-function family MMH. To achieve such speeds, UMAC uses a new universal hash-function family, NH, and a design which allows effective exploitation of SIMD parallelism. The "cryptographic" work of UMAC is done using standard primitives of the user's choice, such as a block cipher or cryptographic hash function; no new heuristic primitives are developed here. Instead, the security of UMAC is rigorously proven, in the sense of giving exact and quantitatively strong results which demonstrate an inability to forge UMAC-authenticated messages assuming an inability to break the underlying cryptographic primitive. Unlike conventional, inherently serial MACs, UMAC is parallelizable, and will have ever-faster implementation speeds as machines offer up increasing amounts of parallelism. We envision UMAC as a practical algorithm for next-generation message authentication.
Encode-Then-Encipher Encryption: How to Exploit Nonces or Redundancy in Plaintexts for Efficient Cryptography	We investigate the following approach to symmetric encryption: first encode the message via some keyless transform, and then encipher the encoded message, meaning apply a permutation FK based on a shared key K. We provide conditions on the encoding functions and the cipher which ensure that the resulting encryption scheme meets strong privacy (eg. semantic security) and/or authenticity goals. The encoding can either be implemented in a simple way (eg. prepend a counter and append a checksum) or viewed as modeling existing redundancy or entropy already present in the messages, whereby encode-then-encipher encryption provides a way to exploit structured message spaces to achieve compact ciphertexts.
Authenticated Encryption: Relations among Notions and Analysis of the Generic Composition Paradigm	We consider two possible notions of authenticity for symmetric encryption schemes, namely integrity of plaintexts and integrity of ciphertexts, and relate them to the standard notions of privacy for symmetric encryption schemes by presenting implications and separations between all notions considered. We then analyze the security of authenticated encryption schemes designed by "generic composition," meaning making black-box use of a given symmetric encryption scheme and a given MAC. Three composition methods are considered, namely Encrypt-and-MAC plaintext, MAC-then-encrypt, and Encrypt-then-MAC. For each of these, and for each notion of security, we indicate whether or not the resulting scheme meets the notion in question assuming the given symmetric encryption scheme is secure against chosen-plaintext attack and the given MAC is unforgeable under chosen-message attack. We provide proofs for the cases where the answer is "yes" and counter-examples for the cases where the answer is "no."
Encode-Then-Encipher Encryption: How to Exploit Nonces or Redundancy in Plaintexts for Efficient Cryptography	We investigate the following approach to symmetric encryption: first encode the message via some keyless transform, and then encipher the encoded message, meaning apply a permutation FK based on a shared key K. We provide conditions on the encoding functions and the cipher which ensure that the resulting encryption scheme meets strong privacy (eg. semantic security) and/or authenticity goals. The encoding can either be implemented in a simple way (eg. prepend a counter and append a checksum) or viewed as modeling existing redundancy or entropy already present in the messages, whereby encode-then-encipher encryption provides a way to exploit structured message spaces to achieve compact ciphertexts.
The Modular Inversion Hidden Number Problem	We study a class of problems called Modular Inverse Hidden Number Problems (MIHNPs). The basic problem in this class is the following: Given many pairs 〈xi, MSBk((α + xi)-1 mod p)〉 for random xi ∈ Zp the problem is to find α ∈ Zp (here MSBk(x) refers to the k most significant bits of x). We describe an algorithm for this problem when k (log2 p)/3 and conjecture that the problem is hard whenever k 2 p)/3. We show that assuming hardness of some variants of this MIHNP problem leads to very efficient algebraic PRNGs and MACs.
Signature schemes based on the strong RSA assumption	We describe and analyze a new digital signature scheme. The new scheme is quite efficient, does not require the the signer to maintain any state, and can be proven secure against adaptive chosen message attack under a reasonable intractability assumption, the so-called strong RSA assumption. Moreover, a hash function can be incorporated into the scheme in such a way that it is also secure in the random oracle model under the standard RSA assumption.
Secure hash-and-sign signatures without the random oracle	We present a new signature scheme which is existentially unforgeable under chosen message attacks, assuming some variant of the RSA conjecture. This scheme is not based on "signature trees", and instead it uses the so called "hash-and-sign" paradigm. It is unique in that the assumptions made on the cryptographic hash function in use are well defined and reasonable (although non-standard). In particular, we do not model this function as a random oracle. We construct our proof of security in steps. First we describe and prove a construction which operates in the random oracle model. Then we show that the random oracle in this construction can be replaced by a hash function which satisfies some strong (but well defined!) computational assumptions. Finally, we demonstrate that these assumptions are reasonable, by proving that a function satisfying them exists under standard intractability assumptions.
Self-Blindable Credential Certificates from the Weil Pairing	We describe two simple, efficient and effective credential pseudonymous certificate systems, which also support anonymity without the need for a trusted third party. The second system provides cryptographic protection against the forgery and transfer of credentials. Both systems are based on a new paradigm, called self-blindable certificates. Such certificates can be constructed using the Weil pairing in supersingular elliptic curves.
Provably Secure and Practical Identification Schemes and Corresponding Signature Schemes	This paper presents a three-move interactive identification scheme and proves it to be as secure as the discrete logarithm problem. This provably secure scheme is almost as efficient as the Schnorr identification scheme, while the Schnorr scheme is not provably secure. This paper also presents another practical identification scheme which is proven to be as secure as the factoring problem and is almost as efficient as the Guillou-Quisquater identification scheme: the Guillou-Quisquater scheme is not provably secure. We also propose practical digital signature schemes based on these identification schemes. The signature schemes are almost as efficient as the Schnorr and Guillou-Quisquater signature schemes, while the security assumptions of our signature schemes are weaker than those of the Schnorr and Guillou-Quisquater. signature schemes. This paper also gives a theoretically generalized result: a three-move identification scheme can be constructed which is as secure as the random-self-reducible problem. Moreover, this paper proposes a variant which is proven to be as secuie as the difficulty of solving both the discrete logarithm problem and the specific factoring problem simultaneously. Some other variants such as an identity-based variant and an elliptic curve variant are also proposed.
Evidence that XTR Is More Secure than Supersingular Elliptic Curve Cryptosystems	We show that finding an efficiently computable injective homomorphism from the XTR subgroup into the group of points over GF(p2) of a particular type of supersingular elliptic curve is at least as hard as solving the Diffie-Hellman problem in the XTR subgroup. This provides strong evidence for a negative answer to the question posed by S. Vanstone and A. Menezes at the Crypto 2000 Rump Session on the possibility of efficiently inverting the MOV embedding into the XTR subgroup. As a side result we show that the Decision Diffie-Hellman problem in the group of points on this type of supersingular elliptic curves is efficiently computable, which provides an example of a group where the Decision Diffie-Hellman problem is simple, while the Diffie-Hellman and discrete logarithm problem are presumably not. The cryptanalytical tools we use also lead to cryptographic applications of independent interest. These applications are an improvement of Joux's one round protocol for tripartite Diffie-Hellman key exchange and a non refutable digital signature scheme that supports escrowable encryption. We also discuss the applicability of our methods to general elliptic curves defined over finite fields.
An Efficient System for Non-transferable Anonymous Credentials with Optional Anonymity Revocation	A credential system is a system in which users can obtain credentials from organizations and demonstrate possession of these credentials. Such a system is anonymous when transactions carried out by the same user cannot be linked. An anonymous credential system is of significant practical relevance because it is the best means of providing privacy for users. In this paper we propose a practical anonymous credential system that is based on the strong RSA assumption and the decisional Diffie-Hellman assumption modulo a safe prime product and is considerably superior to existing ones: (1) We give the first practical solution that allows a user to unlinkably demonstrate possession of a credential as many times as necessary without involving the issuing organization. (2) To prevent misuse of anonymity, our scheme is the first to offer optional anonymity revocation for particular transactions. (3) Our scheme offers separability: all organizations can choose their cryptographic keys independently of each other. Moreover, we suggest more effective means of preventing users from sharing their credentials, by introducing all-or-nothing sharing: a user who allows a friend to use one of her credentials once, gives him the ability to use all of her credentials, i.e., taking over her identity. This is implemented by a new primitive, called circular encryption, which is of independent interest, and can be realized from any semantically secure cryptosystem in the random oracle model.
Efficient Zero-Knowledge Authentication Based on a Linear Algebra Problem MinRank	A Zero-knowledge protocol provides provably secure entity authentication based on a hard computational problem. Among many schemes proposed since 1984, the most practical rely on factoring and discrete log, but still they are practical schemes based on NP-hard problems. Among them, the problem SD of decoding linear codes is in spite of some 30y ears of research effort, still exponential. We study a more general problem called MinRank that generalizes SD and contains also other well known hard problems. MinRank is also used in cryptanalysis of several public key cryptosystems such as birational schemes (Crypto'93), HFE (Crypto'99), GPT cryptosystem (Eurocrypt'91), TTM (Asiacrypt'2000) and Chen's authentication scheme (1996). We propose a new Zero-knowledge scheme based on MinRank. We prove it to be Zero-knowledge by black-box simulation. An adversary able to fraud for a given MinRank instance is either able to solve it, or is able to compute a collision on a given hash function. MinRank is one of the most efficient schemes based on NP-complete problems. It can be used to prove in Zero-knowledge a solution to any problem described by multivariate equations. We also present a version with a public key shared by a few users, that allows anonymous group signatures (a.k.a. ring signatures).
How to Leak a Secret	In this paper we formalize the notion of a ring signature, which makes it possible to specify a set of possible signers without revealing which member actually produced the signature. Unlike group signatures, ring signatures have no group managers, no setup procedures, no revocation procedures, and no coordination: any user can choose any set of possible signers that includes himself, and sign any message by using his secret key and the others' public keys, without getting their approval or assistance. Ring signatures provide an elegant way to leak authoritative secrets in an anonymous way, to sign casual email in a way which can only be verified by its intended recipient, and to solve other problems in multiparty computations. The main contribution of this paper is a new construction of such signatures which is unconditionally signer-ambiguous, provably secure in the random oracle model, and exceptionally efficient: adding each ring member increases the cost of signing or verifying by a single modular multiplication and a single symmetric encryption.
The evaluation of text editors: methodology and empirical results.	This paper presents a methodology for evaluating text editors on several dimensions: the time it takes experts to perform basic editing tasks, the time experts spend making and correcting errors, the rate at which novices learn to perform basic editing tasks, and the functionality of editors over more complex tasks. Time, errors, and learning are measured experimentally; functionality is measured analytically; time is also calculated analytically. The methodology has thus far been used to evaluate nine diverse text editors, producing an initial database of performance results. The database is used to tell us not only about the editors but also about the users—the magnitude of individual differences and the factors affecting novice learning.
A simple guide to five normal forms in relational database theory	The concepts behind the five principal normal forms in relational database theory are presented in simple terms.
Limitations of record-based information models	Record structures are generally efficient, familiar, and easy to use for most current data processing applications. But they are not complete in their ability to represent information, nor are they fully self-describing.
A complete axiomatization for a large class of dependencies in relational datatbases	Relational database theory has discovered complete axiomatizations for functional and multivalued dependencies. However, a database design system that makes use of dependencies declared by the user must deal with some more general kinds of dependencies than these— at least with embedded multivalued dependencies. Yet no axiomatization for embedded multivalued dependencies is known. In this paper, we define a more general class of dependencies, called “template dependencies” and give a complete axiomatization for these. We then discuss the interaction between functional dependencies and template dependencies.
A complete axiomatization for functional and multivalued dependencies in database relations	We investigate the inference rules that can be applied to functional and multivalued dependencies that exist in a database relation. Three types of rules are discussed. First, we list the well known rules for functional dependencies. Then we investigate the rules for multivalued dependencies. It is shown that for each rule for functional dependencies the same rule or a similar rule holds for multivalued dependencies. There is, however, one additional rule for multivalued dependencies that has no parallel among the rules for functional dependencies. Finally, we present rules that involve functional and multivalued dependencies together. The main result of the paper is that the rules presented are complete for the family of functional and multivalued dependencies.
How to Leak a Secret	In this paper we formalize the notion of a ring signature, which makes it possible to specify a set of possible signers without revealing which member actually produced the signature. Unlike group signatures, ring signatures have no group managers, no setup procedures, no revocation procedures, and no coordination: any user can choose any set of possible signers that includes himself, and sign any message by using his secret key and the others' public keys, without getting their approval or assistance. Ring signatures provide an elegant way to leak authoritative secrets in an anonymous way, to sign casual email in a way which can only be verified by its intended recipient, and to solve other problems in multiparty computations. The main contribution of this paper is a new construction of such signatures which is unconditionally signer-ambiguous, provably secure in the random oracle model, and exceptionally efficient: adding each ring member increases the cost of signing or verifying by a single modular multiplication and a single symmetric encryption.
Designated verifier proofs and their applications	For many proofs of knowledge it is important that only the verifier designated by the confirmer can obtain any conviction of the correctness of the proof. A good example of such a situation is for undeniable signatures, where the confirmer of a signature wants to make sure that only the intended verifier(s) in fact can be convinced about the validity or invalidity of the signature. Generally, authentication of messages and off-the-record messages are in conflict with each other. We show how, using designation of verifiers, these notions can be combined, allowing authenticated but private conversations to take place. Our solution guarantees that only the specified verifier can be convinced by t,he proof, even if he shares all his secret information with entities that want to get convinced. Our solution is based on trap-door conim.itments [4], allowing the designated verifier to open up commitments in any way he wants. We demonstrate how a trap-door commitment scheme can be uscd to construct designated verifier proofs, both interactive and non-interactive. We examplify the verifier designation method for the confirmation protocol for undeniable signatures.
Threshold Cryptosystems Secure against Chosen-Ciphertext Attacks	Semantic security against chosen-ciphertext attacks (INDCCA) is widely believed as the correct security level for public-key encryption scheme. On the other hand, it is often dangerous to give to only one people the power of decryption. Therefore, threshold cryptosystems aimed at distributing the decryption ability. However, only two efficient such schemes have been proposed so far for achieving IND-CCA. Both are El Gamal-like schemes and thus are based on the same intractability assumption, namely the Decisional Diffie-Hellman problem. In this article we rehabilitate the twin-encryption paradigm proposed by Naor and Yung to present generic conversions from a large family of (threshold) IND-CPA scheme into a (threshold) IND-CCA one in the random oracle model. An efficient instantiation is also proposed, which is based on the Paillier cryptosystem. This new construction provides the first example of threshold cryptosystem secure against chosen-ciphertext attacks based on the factorization problem. Moreover, this construction provides a scheme where the "homomorphic properties" of the original scheme still hold. This is rather cumbersome because homomorphic cryptosystems are known to be malleable and therefore not to be CCA secure. However, we do not build a "homomorphic cryptosystem", but just keep the homomorphic properties.
Sharing Decryption in the Context of Voting or Lotteries	Several public key cryptosystems with additional homomorphic properties have been proposed so far. They allow to perform computation with encrypted data without the knowledge of any secret information. In many applications, the ability to perform decryption, i.e. the knowledge of the secret key, gives a huge power. A classical way to reduce the trust in such a secret owner, and consequently to increase the security, is to share the secret between many entities in such a way that cooperation between them is necessary to decrypt. In this paper, we propose a distributed version of the Paillier cryptosystem presented at Eurocrypt '99. This shared scheme can for example be used in an electronic voting scheme or in a lottery where a random number related to the winning ticket has to be jointly chosen by all participants.
Extended Notions of Security for Multicast Public Key Cryptosystems	In this paper we introduce two notions of security: multi-user indistinguishability and multi-user non-malleability. We believe that they encompass the correct requirements for public key encryption schemes in the context of multicast communications. A precise and non-trivial analysis proves that they are equivalent to the former single-user notions, provided the number of participants is polynomial. We also introduce a new definition for non-malleability which is simpler than those currently in use. We believe that our results are of practical significance: especially they support the use of PKCS#1 v.2 based on OAEP in the multicast setting.
Practical multi-candidate election system	The aim of electronic voting schemes is to provide a set of protocols that allow voters to cast ballots while a group of authorities collect the votes and output the final tally. In this paper we describe a practical multi-candidate election scheme that guarantees privacy of voters, public verifiability, and robustness against a coalition of malicious authorities. Furthermore, we address the problem of receipt-freeness and incoercibility of voters. Our new scheme is based on the Paillier cryptosystem and on some related zero-knowledge proof techniques. The voting schemes are very practical and can be efficiently implemented in a real system.
Fully Distributed Threshold RSA under Standard Assumptions	The aim of this article is to propose a fully distributed environment for the RSA scheme. What we have in mind is highly sensitive applications and even if we are ready to pay a price in terms of efficiency, we do not want any compromise of the security assumptions that we make. Recently Shoup proposed a practical RSA threshold signature scheme that allows to share the ability to sign between a set of players. This scheme can be used for decryption as well. However, Shoup's protocol assumes a trusted dealer to generate and distribute the keys. This comes from the fact that the scheme needs a special assumption on the RSA modulus and this kind of RSA moduli cannot be easily generated in an efficient way with many players. Of course, it is still possible to call theoretical results on multiparty computation, but we cannot hope to design efficient protocols. The only practical result to generate RSA moduli in a distributive manner is Boneh and Franklin's protocol but it seems difficult to modify it in order to generate the kind of RSA moduli that Shoup's protocol requires. The present work takes a different path by proposing a method to enhance the key generation with some additional properties and revisits Shoup's protocol to work with the resulting RSA moduli. Both of these enhancements decrease the performance of the basic protocols. However, we think that in the applications we target, these enhancements provide practical solutions. Indeed, the key generation protocol is usually run only once and the number of players used to sign or decrypt is not very large. Moreover, these players have time to perform their task so that the communication or time complexity are not overly important.
Robust and Efficient Sharing of RSA Functions	We present two efficient protocols which implement robust threshold RSA signature schemes, where the power to sign is shared by N players such that any subset of T or more signers can collaborate to produce a valid RSA signature on any given message, but no subset of fewer than T corrupted players can forge a signature. Our protocols are robust in the sense that the correct signature is computed even if up to T - 1 players behave in arbitrarily malicious way during the signature protocol. This in particular includes the cases of players that refuse to participate or that generate incorrect partial signatures. Our robust protocols achieve optimal resiliency as they can tolerate up to (N - 1)/2 faults, and their efficiency is comparable to the efficiency of the underlying threshold RSA signature scheme. Robust threshold signature schemes have very important applications, since they provide increased security and availability for a signing server (e.g. a certification authority or an electronic cash provider). Solutions for the case of the RSA signature scheme are especially important because of its widespread use. In addition, these techniques apply to shared RSA decryption as well, thus leading to efficient key escrow schemes for RSA. Our schemes are based on some interesting extensions that we devised for the information checking protocol of T. Rabin and Ben-Or [Rab94, RB89], and the undeniable signature work initiated by Chaum and van Antwerpen [CA90]. These extensions have some attractive properties, and hence are of independent interest.
REACT: Rapid Enhanced-Security Asymmetric Cryptosystem Transform	Seven years after the optimal asymmetric encryption padding (OAEP) which makes chosen-ciphertext secure encryption scheme from any trapdoor one-way permutation (but whose unique application is RSA), this paper presents REACT, a new conversion which applies to any weakly secure cryptosystem, in the random oracle model: it is optimal from both the computational and the security points of view. Indeed, the overload is negligible, since it just consists of two more hashings for both encryption and decryption, and the reduction is very tight. Furthermore, advantages of REACT beyond OAEP are numerous: 1. it is more general since it applies to any partially trapdoor one-way function (a.k.a. weakly secure public-key encryption scheme) and therefore provides security relative to RSA but also to the Diffie-Hellman problem or the factorization; 2. it is possible to integrate symmetric encryption (block and stream ciphers) to reach very high speed rates; 3. it provides a key distribution with session key encryption, whose overall scheme achieves chosen-ciphertext security even with weakly secure symmetric scheme.Therefore, REACT could become a new alternative to OAEP, and even reach security relative to factorization, while allowing symmetric integration.
Fully Distributed Threshold RSA under Standard Assumptions	The aim of this article is to propose a fully distributed environment for the RSA scheme. What we have in mind is highly sensitive applications and even if we are ready to pay a price in terms of efficiency, we do not want any compromise of the security assumptions that we make. Recently Shoup proposed a practical RSA threshold signature scheme that allows to share the ability to sign between a set of players. This scheme can be used for decryption as well. However, Shoup's protocol assumes a trusted dealer to generate and distribute the keys. This comes from the fact that the scheme needs a special assumption on the RSA modulus and this kind of RSA moduli cannot be easily generated in an efficient way with many players. Of course, it is still possible to call theoretical results on multiparty computation, but we cannot hope to design efficient protocols. The only practical result to generate RSA moduli in a distributive manner is Boneh and Franklin's protocol but it seems difficult to modify it in order to generate the kind of RSA moduli that Shoup's protocol requires. The present work takes a different path by proposing a method to enhance the key generation with some additional properties and revisits Shoup's protocol to work with the resulting RSA moduli. Both of these enhancements decrease the performance of the basic protocols. However, we think that in the applications we target, these enhancements provide practical solutions. Indeed, the key generation protocol is usually run only once and the number of players used to sign or decrypt is not very large. Moreover, these players have time to perform their task so that the communication or time complexity are not overly important.
Practical Threshold RSA Signatures without a Trusted Dealer	We propose a threshold RSA scheme which is as efficient as the fastest previous threshold RSA scheme (by Shoup), but where two assumptions needed in Shoup's and in previous schemes can be dropped, namely that the modulus must be a product of safe primes and that a trusted dealer generates the keys. The robustness (but not the unforgeability) of our scheme depends on a new intractability assumption, in addition to security of the underlying standard RSA scheme.
Robust and Efficient Sharing of RSA Functions	We present two efficient protocols which implement robust threshold RSA signature schemes, where the power to sign is shared by N players such that any subset of T or more signers can collaborate to produce a valid RSA signature on any given message, but no subset of fewer than T corrupted players can forge a signature. Our protocols are robust in the sense that the correct signature is computed even if up to T - 1 players behave in arbitrarily malicious way during the signature protocol. This in particular includes the cases of players that refuse to participate or that generate incorrect partial signatures. Our robust protocols achieve optimal resiliency as they can tolerate up to (N - 1)/2 faults, and their efficiency is comparable to the efficiency of the underlying threshold RSA signature scheme. Robust threshold signature schemes have very important applications, since they provide increased security and availability for a signing server (e.g. a certification authority or an electronic cash provider). Solutions for the case of the RSA signature scheme are especially important because of its widespread use. In addition, these techniques apply to shared RSA decryption as well, thus leading to efficient key escrow schemes for RSA. Our schemes are based on some interesting extensions that we devised for the information checking protocol of T. Rabin and Ben-Or [Rab94, RB89], and the undeniable signature work initiated by Chaum and van Antwerpen [CA90]. These extensions have some attractive properties, and hence are of independent interest.
Sharing Decryption in the Context of Voting or Lotteries	Several public key cryptosystems with additional homomorphic properties have been proposed so far. They allow to perform computation with encrypted data without the knowledge of any secret information. In many applications, the ability to perform decryption, i.e. the knowledge of the secret key, gives a huge power. A classical way to reduce the trust in such a secret owner, and consequently to increase the security, is to share the secret between many entities in such a way that cooperation between them is necessary to decrypt. In this paper, we propose a distributed version of the Paillier cryptosystem presented at Eurocrypt '99. This shared scheme can for example be used in an electronic voting scheme or in a lottery where a random number related to the winning ticket has to be jointly chosen by all participants.
Practical multi-candidate election system	The aim of electronic voting schemes is to provide a set of protocols that allow voters to cast ballots while a group of authorities collect the votes and output the final tally. In this paper we describe a practical multi-candidate election scheme that guarantees privacy of voters, public verifiability, and robustness against a coalition of malicious authorities. Furthermore, we address the problem of receipt-freeness and incoercibility of voters. Our new scheme is based on the Paillier cryptosystem and on some related zero-knowledge proof techniques. The voting schemes are very practical and can be efficiently implemented in a real system.
Generation of Shared RSA Keys by Two Parties	At Crypto'97 Boneh and Franklin proposed a protocol to efficiently generate shared RSA keys. In the case of two parties, the drawback of their scheme is the need of an independent third party. Furthermore, the security is guaranteed only if the three players follow the protocol. In this paper, we propose a protocol that enables two parties to evaluate any algebraic expression, including an RSA modulus, along the same lines as in the Boneh-Franklin protocol. Our solution does not need the help of a third party and the only assumption we make is the existence of an oblivious transfer protocol. Furthermore, it remains robust even if one of the two players deviates from the protocol.
Robot introspection through learned hidden Markov models	In this paper we describe a machine learning approach for acquiring a model of a robot behaviour from raw sensor data. We are interested in automating the acquisition of behavioural models to provide a robot with an introspective capability. We assume that the behaviour of a robot in achieving a task can be modelled as a finite stochastic state transition system.Beginning with data recorded by a robot in the execution of a task, we use unsupervised learning techniques to estimate a hidden Markov model (HMM) that can be used both for predicting and explaining the behaviour of the robot in subsequent executions of the task. We demonstrate that it is feasible to automate the entire process of learning a high quality HMM from the data recorded by the robot during execution of its task.The learned HMM can be used both for monitoring and controlling the behaviour of the robot. The ultimate purpose of our work is to learn models for the full set of tasks associated with a given problem domain, and to integrate these models with a generative task planner. We want to show that these models can be used successfully in controlling the execution of a plan. However, this paper does not develop the planning and control aspects of our work, focussing instead on the learning methodology and the evaluation of a learned model. The essential property of the models we seek to construct is that the most probable trajectory through a model, given the observations made by the robot, accurately diagnoses, or explains, the behaviour that the robot actually performed when making these observations. In the work reported here we consider a navigation task. We explain the learning process, the experimental setup and the structure of the resulting learned behavioural models. We then evaluate the extent to which explanations proposed by the learned models accord with a human observer's interpretation of the behaviour exhibited by the robot in its execution of the task.
Facial expression recognition from video sequences: temporal and static modeling	The most expressive way humans display emotions is through facial expressions. In this work we report on several advances we have made in building a system for classification of facial expressions from continuous video input. We introduce and test different Bayesian network classifiers for classifying expressions from video, focusing on changes in distribution assumptions, and feature dependency structures. In particular we use Naive-Bayes classifiers and change the distribution from Gaussian to Cauchy, and use Gaussian Tree-Augmented Naive Bayes (TAN) classifiers to learn the dependencies among different facial motion features. We also introduce a facial expression recognition from live video input using temporal cues. We exploit the existing methods and propose a new architecture of hidden Markov models (HMMs) for automatically segmenting and recognizing human facial expression from video sequences. The architecture performs both segmentation and recognition of the facial expressions automatically using a multi-level architecture composed of an HMM layer and a Markov model layer. We explore both person-dependent and person-independent recognition of expressions and compare the different methods.
Shared substance: developing flexible multi-surface applications	This paper presents a novel middleware for developing flexible interactive multi-surface applications. Using a scenario-based approach, we identify the requirements for this type of applications. We then introduce Substance, a data-oriented framework that decouples functionality from data, and Shared Substance, a middleware implemented in Substance that provides powerful sharing abstractions. We describe our implementation of two applications with Shared Substance and discuss the insights gained from these experiments. Our finding is that the combination of a data-oriented programming model with middleware support for sharing data and functionality provides a flexible, robust solution with low viscosity at both design-time and run-time.
Experiences with recombinant computing: Exploring ad hoc interoperability in evolving digital networks	This article describes an infrastructure that supports the creation of interoperable systems while requiring only limited prior agreements about the specific forms of communication between these systems. Conceptually, our approach uses a set of “meta-interfaces”—agreements on how to exchange new behaviors necessary to achieve compatibility at runtime, rather than requiring that communication specifics be built in at development time—to allow devices on the network to interact with one another. While this approach to interoperability can remove many of the system-imposed constraints that prevent fluid, ad hoc use of devices now, it imposes its own limitations on the user experience of systems that use it. Most importantly, since devices may be expected to work with peers about which they have no detailed semantic knowledge, it is impossible to achieve the sort of tight semantic integration that can be obtained using other approaches today, despite the fact that these other approaches limit interoperability. Instead, under our model, users must be tasked with performing the sense-making and semantic arbitration necessary to determine how any set of devices will be used together. This article describes the motivation and details of our infrastructure, its implications on the user experience, and our experience in creating, deploying, and using applications built with it over a period of several years.
The BEACH application model and software framework for synchronous collaboration in ubiquitous computing environments	In this paper, a conceptual model for synchronous applications in ubiquitous computing environments is proposed. To test its applicability, it was used to structure the architecture of the BEACH software framework that is the basis for the software infrastructure of i-LAND (the ubiquitous computing environment at FhG-IPSI). The BEACH framework provides the functionality for synchronous cooperation and interaction with roomware components, i.e. room elements with integrated information technology. To show how the BEACH model and framework can be applied, the design of a sample application is explained. Also, the BEACH model is positioned against related work. In conclusion, we provide our experiences with the current implementation.
Gaia: enabling active spaces	Ubiquitous computing promotes physical spaces with hundreds of specialized embedded devices that increase our productivity, alleviate some specific everyday tasks and provide new ways of interacting with the computational environment. Because the computational environment is spread across the physical space, personal computers lose the focus of attention. Therefore, the users' view of the computational environment is finally extended beyond the physical limits of the computer. Physical spaces become computer systems, or in other terms, Active Spaces. However, these Active Spaces require novel system software capable of seamlessly coordinating their hidden complexity. Our goal is to extend the model provided by current computer systems to allow interaction with physical spaces and their contained entities (physical and virtual) by means of a single abstraction called Active Space.
Interweaving mobile games with everyday life	We introduce a location--based game called Feeding Yoshi that provides an example of seamful design, in which key characteristics of its underlying technologies-the coverage and security characteristics of WiFi-are exposed as a core element of gameplay. Feeding Yoshi is also a long--term, wide--area game, being played over a week between three different cities during an initial user study. The study, drawing on participant diaries and interviews, supported by observation and analysis of system logs, reveals players' reactions to the game. We see the different ways in which they embedded play into the patterns of their daily lives, augmenting existing practices and creating new ones, and observe the impact of varying location on both the ease and feel of play. We identify potential design extensions to Feeding Yoshi and conclude that seamful design provides a route to creating engaging experiences that are well adapted to their underlying technologies.
Connectables: dynamic coupling of displays for the flexible creation of shared workspaces	We present the ConnecTable, a new mobile, networked and context-aware information appliance that provides affordances for pen-based individual and cooperative work as well as for the seamless transition between the two. In order to dynamically enlarge an interaction area for the purpose of shared use, a flexible coupling of displays has been realized that overcomes the restrictions of display sizes and borders. Two ConnecTable displays dynamically form a homogeneous display area when moved close to each other. The appropriate triggering signal comes from built-in sensors allowing users to temporally combine their individual displays to a larger shared one by a simple physical movement in space. Connected ConnecTables allow their users to work in parallel on an ad-hoc created shared workspace as well as exchanging information by simply shuffling objects from one display to the other. We discuss the user interface and related issues as well as the software architecture. We also present the physical realization of the ConnecTables.
A taxonomy for and analysis of multi-person-display ecosystems	Interactive displays are increasingly being distributed in a broad spectrum of everyday life environments: they have very diverse form factors and portability characteristics, support a variety of interaction techniques, and can be used by a variable number of people. The coupling of multiple displays creates an interactive "ecosystem of displays". Such an ecosystem is suitable for particular social contexts, which in turn generates novel settings for communication and performance and challenges in ownership. This paper aims at providing a design space that can inform the designers of such ecosystems. To this end, we provide a taxonomy that builds on the size of the ecosystem and on the degree of individual engagement as dimensions. We recognize areas where physical constraints imply certain kinds of social engagement, versus other areas where further work on interaction techniques for coupling displays can open new design spaces.
Normative influences on thoughtful online participation	We describe two experiments on whether individual thoughtful effort during online commenting is shaped by situational norms derived from the behavior of social others and the design of the environment, respectively. By measuring the length of participants' comments on a news website, the time taken to write them, and the number of issue-relevant thoughts they contain, we demonstrate that participants conform to high vs. low norms of thoughtfulness manifested through either the apparent behavior of other users or through visual, textual and interactional design features conceptually associated with thoughtfulness. Theoretical and applied insights for designing online participatory environments are discussed.
Slash(dot) and burn: distributed moderation in a large online conversation space	Can a system of distributed moderation quickly and consistently separate high and low quality comments in an online conversation? Analysis of the site Slashdot.org suggests that the answer is a qualified yes, but that important challenges remain for designers of such systems. Thousands of users act as moderators. Final scores for comments are reasonably dispersed and the community generally agrees that moderations are fair. On the other hand, much of a conversation can pass before the best and worst comments are identified. Of those moderations that were judged unfair, only about half were subsequently counterbalanced by a moderation in the other direction. And comments with low scores, not at top-level, or posted late in a conversation were more likely to be overlooked by moderators.
Computers can't give credit: how automatic attribution falls short in an online remixing community	In this paper, we explore the role that attribution plays in shaping user reactions to content reuse, or remixing, in a large user-generated content community. We present two studies using data from the Scratch online community - a social media platform where hundreds of thousands of young people share and remix animations and video games. First, we present a quantitative analysis that examines the effects of a technological design intervention introducing automated attribution of remixes on users' reactions to being remixed. We compare this analysis to a parallel examination of "manual" credit-giving. Second, we present a qualitative analysis of twelve in-depth, semi-structured, interviews with Scratch participants on the subject of remixing and attribution. Results from both studies suggest that automatic attribution done by technological systems (i.e., the listing of names of contributors) plays a role that is distinct from, and less valuable than, credit which may superficially involve identical information but takes on new meaning when it is given by a human remixer. We discuss the implications of these findings for the designers of online communities and social media platforms.
What's mine is mine: territoriality in collaborative authoring	Territoriality, the expression of ownership towards an object, can emerge when social actors occupy a shared social space. In the case of Wikipedia, the prevailing cultural norm is one that warns against ownership of one's work. However, we observe the emergence of territoriality in online space with respect to a subset of articles that have been tagged with the Maintained template through a qualitative study of 15 editors who have self-designated as Maintainers. Our participants communicated ownership, demarcated boundaries and asserted their control over artifacts for the sake of quality by appropriating existing features of Wikipedia. We then suggest design strategies to support these behaviors in the proper context within collaborative authoring systems more generally.
An analysis of the social structure of remix culture	We present findings from our study of a music sharing and remixing community in an effort to quantify and understand the structural characteristics of commons-based peer production for products of aesthetic/cultural or entertainment value. We also provide a normative perspective on the strategies that such communities should employ with respect to the use of 'remixing contests', which are popular means of attracting new user-creators to the community and boosting its creative output. Until now research has shied away from the quantitative study of what lies at the heart of this 'remix culture', i.e. remixing, presumably because of the difficulties inherent in attaining relevant large datasets amenable to numerical analysis and an early focus of research efforts on communities whose products serve a more functional purpose (e.g., open source software), rather than aiming at entertainment or personal and artistic expression. This paper contributes to the literature of social network analysis of online communities, the literature on commons-based peer production, and the research agenda of cultural analytics.
Edits & credits: exploring integration and attribution in online creative collaboration	Attribution allows online reputations to be formed and motivates many contributions to online creative collaboration. Yet, we know little about attribution practices in online creative collaboration and the technologies that shape them. This paper describes a study of online collaborative animation projects, focused on the practices surrounding integration and attribution. We found that both tasks are closely related and often completed by a single person, a process we call "cr-editing." We also identify frustrations with existing practices and systems and propose design considerations for alleviating them. Our findings offer insights into the growing space of online remixing, mashups, and creativity.
Identifying shared leadership in Wikipedia	In this paper, we introduce a method to measure shared leadership in Wikipedia as a step in developing a new model of online leadership. We show that editors with varying degrees of engagement and from peripheral as well as central roles all act like leaders, but that core and peripheral editors show different profiles of leadership behavior. Specifically, we developed machine learning models to automatically identify four types of leadership behaviors from 4 million messages sent between Wikipedia editors. We found strong evidence of shared leadership in Wikipedia, with editors in peripheral roles producing a large proportion of leadership behaviors.
Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis	Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation). However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word's prior polarity. Positive words are used in phrases expressing negative sentiments, or vice versa. Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment. The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task. Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity. The evaluation includes assessing the performance of features across multiple machine learning algorithms. For all learning algorithms except one, the combination of all features together gives the best performance. Another facet of the evaluation considers how the presence of neutral instances affects the performance of features for distinguishing between positive and negative polarity. These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system's ability to identify when an instance is neutral.
The WEKA data mining software: an update	More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.
Socialization tactics in wikipedia and their effects	Socialization of newcomers is critical both for conventional groups. It helps groups perform effectively and the newcomers develop commitment. However, little empirical research has investigated the impact of specific socialization tactics on newcomers' commitment to online groups. We examined WikiProjects, subgroups in Wikipedia organized around working on common topics or tasks. In study 1, we identified the seven socialization tactics used most frequently: invitations to join, welcome messages, requests to work on project-related tasks, offers of assistance, positive feedback on a new member's work, constructive criticism, and personal-related comments. In study 2, we examined their impact on newcomers' commitment to the project. Whereas most newcomers contributed fewer edits over time, the declines were slowed or reversed for those socialized with welcome messages, assistance, and constructive criticism. In contrast, invitations led to steeper declines in edits. These results suggest that different socialization tactics play different roles in socializing new members in online groups compared to offline ones.
Am I wasting my time organizing email?: a study of email refinding	We all spend time every day looking for information in our email, yet we know little about this refinding process. Some users expend considerable preparatory effort creating complex folder structures to promote effective refinding. However modern email clients provide alternative opportunistic methods for access, such as search and threading, that promise to reduce the need to manually prepare. To compare these different refinding strategies, we instrumented a modern email client that supports search, folders, tagging and threading. We carried out a field study of 345 long-term users who conducted over 85,000 refinding actions. Our data support opportunistic access. People who create complex folders indeed rely on these for retrieval, but these preparatory behaviors are inefficient and do not improve retrieval success. In contrast, both search and threading promote more effective finding. We present design implications: current search-based clients ignore scrolling, the most prevalent refinding behavior, and threading approaches need to be extended.
ContactMap: Organizing communication in a social desktop	Modern work is a highly social process, offering many cues for people to organize communication and access information. Shared physical workplaces provide natural support for tasks such as (a) social reminding about communication commitments and keeping track of collaborators and friends, and (b) social data mining of local expertise for advice and information. However, many people now collaborate remotely using tools such as email and voicemail. Our field studies show that these tools do not provide the social cues needed for group work processes. In part, this is because the tools are organized around messages, rather than people. In response to this problem, we created ContactMap, a system that makes people the primary unit of interaction. ContactMap provides a structured social desktop representation of users' important contacts that directly supports social reminding and social data mining. We conducted an empirical evaluation of ContactMap, comparing it with traditional email systems, on tasks suggested by our fieldwork. Users performed better with ContactMap and preferred ContactMap for the majority of these tasks. We discuss future enhancements of our system and the implications of these results for future communication interfaces and for theories of mediated communication.
Understanding sequence and reply relationships within email conversations: a mixed-model visualization	It has been proposed that email clients could be improved if they presented messages grouped into conversations. An email conversation is the tree of related messages that arises from the use of the reply operation. We propose two models of conversation. The first model characterizes a conversation as a chronological sequence of messages; the second as a tree based on the reply relationship. We show how existing email clients and prior research projects implicitly support each model to a greater or lesser degree depending on their design, but none fully supports both models simultaneously. We present a mixed-model visualization that simultaneously presents sequence and reply relationships among the messages of a conversation, making both visible at a glance. We describe the integration of the visualization into a working prototype email client. A usability study indicates that the system meets our usability goals and verifies that the visualization fully conveys both types of relationships within the messages of an email conversation.
Automatically classifying emails into activities	Email-based activity management systems promise to give users better tools for managing increasing volumes of email, by organizing email according to a user's activities. Current activity management systems do not automatically classify incoming messages by the activity to which they belong, instead relying on simple heuristics (such as message threads), or asking the user to manually classify incoming messages as belonging to an activity. This paper presents several algorithms for automatically recognizing emails as part of an ongoing activity. Our baseline methods are the use of message reply-to threads to determine activity membership and a naïve Bayes classifier. Our SimSubset and SimOverlap algorithms compare the people involved in an activity against the recipients of each incoming message. Our SimContent algorithm uses IRR (a variant of latent semantic indexing) to classify emails into activities using similarity based on message contents. An empirical evaluation shows that each of these methods provide a significant improvement to the baseline methods. In addition, we show that a combined approach that votes the predictions of the individual methods performs better than each individual method alone.
Quality versus quantity: e-mail-centric task management and its relation with overload	It is widely acknowledged that many professionals suffer from "e-mail overload." This article presents findings from in-depth fieldwork that examined this phenomenon, uncovering six key challenges of taskmanagement in e-mail. Analysis of qualitative and quantitative data suggests that it is not simply the quantity but also the collaborative quality of e-mail task and project management that causes this overload. We describe how e-mail becomes especially overwhelming when people use it for tasks that involve participation of others; tasks cannot be completed until a response is obtained and so they are interleaved. Interleaving means that the email user must somehow simultaneously keep track of multiple incomplete tasks, often with the only reminder for each one being an e-mail message somewhere in the inbox or a folder. This and other insights from our fieldwork led us to a new design philosophy for e-mail in which resources for task and project management are embedded directly within an e-mail client as opposed to being added on as separate components of the application. A client, TaskMaster, embodying these ideas, was developed and tested by users in managing their real e-mail over an extended period. The design of the client and results of its evaluation are also reported.
E-mail research: targeting the enterprise	The research program at IBM's® Collaborative User Experience (CUE) group supports an e-mail system used by millions of people. We present three lessons learned from working with real-world enterprise e-mail solutions. First, a pragmatic, system-level approach reveals that e-mail programs are generally used idiosyncratically, often for many different goals at once. This fact has strong implications for both the design and assessment of new features. Second, we discuss how viewing e-mail as an element of corporate collaboration--not just communication--provides insights into problems with current systems as well as potential solutions. Third, we describe constraints imposed by the realities of software development and how they shape the space of feasible new designs. Finally, we illustrate these lessons with an overview of CUE research strategies in the context of an extended case study of one specific new technology: Thread Arcs. Although not all researchers work with an enterprise-level product team, we believe the experiences described here will be useful to anyone wishing to see their ideas ultimately implemented on a broad scale.
Taking email to task: the design and evaluation of a task management centered email tool	Email has come to play a central role in task management, yet email tool features have remained relatively static in recent years, lagging behind users? evolving practices. The Taskmaster system narrows this gap by recasting email as task management and embedding task-centric resources directly in the client. In this paper, we describe the field research that inspired Taskmaster and the principles behind its design. We then describe how user studies conducted with ?live? email data over a two-week period revealed the value of a task-centric approach to email system design and its potential benefits for overloaded users.
Improved search engines and navigation preference in personal information management	Traditionally users access their personal files mainly by usingfolder navigation. We evaluate whether recent improvements indesktop search have changed this fundamental aspect of PersonalInformation Management (PIM). We tested this in two studies usingthe same questionnaire: (a) The Windows Studya longitudinalcomparison of Google Desktop and Windows XP SearchCompanion, and (b) The Mac Studya large scale comparison of MacSpotlight and Sherlock. There were few effects forimproved search. First, regardless of search engine, there was astrong navigation preference: on average, users estimated that theyused navigation for 56-68% of file retrieval events but searchedfor only 4-15% of events. Second, the effect of improving thequality of the search engine on search usage was limited andinconsistent. Third, search was used mainly as a last resort whenusers could not remember file location. Finally, there was noevidence that using improved desktop search engines leads people tochange their filing habits to become less reliant on hierarchicalfile organization. We conclude by offering theoretical explanationsfor navigation preference, relating to differences between PIM andInternet retrieval, and suggest alternative design directions forPIM systems.
Exploring memory in email refinding	Human memory plays an important role in personal information management (PIM). Several scholars have noted that people refind information based on what they remember and it has been shown that people adapt their management strategies to compensate for the limitations of memory. Nevertheless, little is known about what people tend to remember about their personal information and how they use their memories to refind. The aim of this article is to increase our understanding of the role that memory plays in the process of refinding personal information. Concentrating on email re-finding, we report on a user study that investigates what attributes of email messages participants remember when trying to refind. We look at how the attributes change in different scenarios and examine the factors which impact on what is remembered.
Understanding email use: predicting action on a message	Email consumes significant time and attention in the workplace. We conducted an organizational survey to understand how and why people attend to incoming email messages. We examined people's ratings of message importance and the actions they took on specific email messages, based on message characteristics and characteristics of receivers and senders. Respondents kept half of their new messages in the inbox and replied to about a third of them. They rated messages as important if they were about work and required action. Importance, in turn, had a modest impact on whether people replied to their incoming messages and whether they saved them. The results indicate that factors other than message importance (e.g., their social nature) also determine how people handle email. Overall, email usage reflects attentional differences due both to personal propensities and to work demands and relationships.
UX research: what theoretical roots do we build on -- if any?	User Experience (UX) research focusing on the emotional and experiential aspects of system usage is of highly recognized relevance for the CHI community. A lot of work has been conducted with different goals: investigating a common definition and understanding of UX, creating appropriate concepts, frameworks and models for supporting design and development processes, and developing methods and techniques for evaluating UX. However, there is still a lack of in-depth discussions on the theoretical roots and foundations for all of these UX activities in academia and industry. In this SIG we will explore the state of the art in the theory of UX research in order to lay the fundament for further advancements of the UX field. We will also discuss how the theoretical viewpoints can benefit, and be influenced by the UX practitioners' work.
User experience evaluation methods: current state and development needs	The recent shift of emphasis to user experience (UX) has rendered it a central focus of product design and evaluation. A multitude of methods for UX design and evaluation exist, but a clear overview of the current state of the available UX evaluation methods is missing. This is partly due to a lack of agreement on the essential characteristics of UX. In this paper, we present the results of our multi-year effort of collecting UX evaluation methods from academia and industry with different approaches such as literature review, workshops, Special Interest Groups sessions and an online survey. We have collected 96 methods and analyzed them, among other criteria, based on the product development phase and the studied period of experience. Our analysis reveals development needs for UX evaluation methods, such as early-stage methods, methods for social and collaborative UX evaluation, establishing practicability and scientific quality, and a deeper understanding of UX.
A discriminative HMM/N-gram-based retrieval approach for mandarin spoken documents	In recent years, statistical modeling approaches have steadily gained in popularity in the field of information retrieval. This article presents an HMM/N-gram-based retrieval approach for Mandarin spoken documents. The underlying characteristics and the various structures of this approach were extensively investigated and analyzed. The retrieval capabilities were verified by tests with word- and syllable-level indexing features and comparisons to the conventional vector-space model approach. To further improve the discrimination capabilities of the HMMs, both the expectation-maximization (EM) and minimum classification error (MCE) training algorithms were introduced in training. Fusion of information via indexing word- and syllable-level features was also investigated. The spoken document retrieval experiments were performed on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). Very encouraging retrieval performance was obtained.
A study of smoothing methods for language models applied to Ad Hoc information retrieval	Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections.
Autonomous classifiers with understandable rule using multi-objective genetic algorithms	This paper presents a method for designing autonomous classifiers via multi-objective genetic algorithms. The paper also proposes a novel objective measure to quantify the understandability of the classifiers. The other objectives of the classifiers are classification accuracy and average support value. We experimentally evaluate our approach on five different medical dataset and demonstrate that our algorithm encourages us to improve and apply this strategy in many real-world applications.
Genetic algorithm-based strategy for identifying association rules without specifying actual minimum support	We design a genetic algorithm-based strategy for identifying association rules without specifying actual minimum support. In this approach, an elaborate encoding method is developed, and the relative confidence is used as the fitness function. With genetic algorithm, a global search can be performed and system automation is implemented, because our model does not require the user-specified threshold of minimum support. Furthermore, we expand this strategy to cover quantitative association rule discovery. For efficiency, we design a generalized FP-tree to implement this algorithm. We experimentally evaluate our approach, and demonstrate that our algorithms significantly reduce the computation costs and generate interesting association rules only.
Oblivious Polynomial Evaluation and Oblivious Neural Learning	We study the problem of Oblivious Polynomial Evaluation (OPE). There are two parties, Alice who has a polynomial P, and Bob who has an input x. The goal is for Bob to compute P(x) in such way that Alice learns nothing about x and Bob learns only what can be inferred from P(x). Previously existing protocols are based on some intractability assumptions that have not been well studied [15,14], and these protocols are only applicable for polynomials over finite fields. In this paper, we propose efficient OPE protocols which are based on Oblivious Transfer only. Unlike that of [15], slight modifications to our protocols immediately give protocols to handle multi-variate polynomials and polynomials over floating-point numbers. Many important real-world applications deal with floating-point numbers, instead of integers or arbitrary finite fields, and our protocols have the advantage of operating directly on floating-point numbers, instead of going through finite field simulation as that of [14]. As an example, we give a protocol for the problem of Oblivious Neural Learning, where one party has a neural network and the other, with some training set, wants to train the neural network in an oblivious way.
Non-Interactive CryptoComputing For NC1	The area of "computing with encrypted data" has been studied by numerous authors in the past twenty years since it is fundamental to understanding properties of encryption and it has many practical applications. The related fundamental area of "secure function evaluation" has been studied since the mid 80's. In its basic two-party case, two parties (Alice and Bob) evaluate a known circuit over private inputs (or a private input and a private circuit). Much attention has been paid to the important issue of minimizing rounds of computation in this model. Namely, the number of communication rounds in which Alice and Bob need to engage in to evaluate a circuit on encrypted data securely. Advancements in these areas have been recognized as open problems and have remained open for a number of years. In this paper we give a one round, and thus round optimal, protocol for secure evaluation of circuits which is in polynomial-time for NC1 circuits. The protocol involves an input party sending encrypted input to a second party, a cryptocomputer, which evaluates the circuit (or a known circuit over its additional private input) non-interactively, securely and obliviously, and provides the output to the input party without learning it.This improves on previous (general) results that are specialized to the case of NC1 circuits and require a constant number of communication rounds. We further suggest applications to network and mobile computing. The scenario also coincides with computing with encrypted data when the input is transformed into an output while remaining encrypted throughout the computation.New techniques are required for our highly constrained non-interactive setting. Naturally, some of these techniques are related to special properties of encryption schemes (we in fact, need probabilistic encryption schemes which are random self-reducible). Homomorphic encryption schemes are closely related to and useful in secure circuit evaluation. They have been associated with computations with encrypted data (as well as with many other cryptographic applications). Surprisingly, the known homomorphic schemes have been limited to a small number of algebraic structures, e.g. all the schemes we are aware of are homomorphic over groups. We also give a new provably secure public key scheme that allows the computation of the logical AND operation using encrypted data. This scheme is homomorphic over a semigroup (instead of a group) and thus also expands the range of algebraic structures which can be encrypted "homomorphically".
Privacy Preserving Data Mining	In this paper we introduce the concept of privacy preserving data mining. In our model, two parties owning confidential databases wish to run a data mining algorithm on the union of their databases, without revealing any unnecessary information. This problem has many practical and important applications, such as in medical research with confidential patient records. Data mining algorithms are usually complex, especially as the size of the input is measured in megabytes, if not gigabytes. A generic secure multi-party computation solution, based on evaluation of a circuit computing the algorithm on the entire input, is therefore of no practical use. We focus on the problem of decision tree learning and use ID3, a popular and widely used algorithm for this problem. We present a solution that is considerably more efficient than generic solutions. It demands very few rounds of communication and reasonable bandwidth. In our solution, each party performs by itself a computation of the same order as computing the ID3 algorithm for its own database. The results are then combined using efficient cryptographic protocols, whose overhead is only logarithmic in the number of transactions in the databases. We feel that our result is a substantial contribution, demonstrating that secure multi-party computation can be made practical, even for complex problems and large inputs.
Multiparty unconditionally secure protocols	Under the assumption that each pair of participants can communicate secretly, we show that any reasonable multiparty protocol can be achieved if at least 2n/3 of the participants are honest. The secrecy achieved is unconditional. It does not rely on any assumption about computational intractability.
The Hardness of Hensel Lifting: The Case of RSA and Discrete Logarithm	At ACM CCS '01, Catalano et al. proposed a mix of the RSA cryptosystem with the Paillier cryptosystem from Eurocrypt '99. The resulting scheme, which we call RSAP, is a probabilistic cryptosystem which is both semantically secure under an appropriate decisional assumption and as efficient as RSA, but without the homomorphic property of the Paillier scheme. Interestingly, Sakurai and Takagi presented at PKC '02 a proof that the one-wayness of RSAP was equivalent to the RSA assumption. However, we notice in this paper that the above proof is not completely correct (it works only in the case when a perfect oracle - i.e. an oracle that always provides correct answers - is given). We fix the proof by presenting a new proof based on low-dimensional lattices. The new proof, inspired by the work of Sakurai and Takagi, is somewhat related to Hensel lifting and the N-adic decomposition of integer exponentiation. Roughly speaking, we consider the problem of computing f(x) mod Ml given f(x) mod M and an exponent l 1. By studying the case f(x) = xe and M is an RSA-modulus, we deduce that the one-wayness of RSAP is indeed equivalent to the RSA assumption, and we are led to conjecture that the one-wayness of the original Paillier scheme may not be equivalent to the RSA assumption with exponent N. By analogy, we also study the discrete logarithm case, namely when f(x) = gx and M is a prime, and we show that the corresponding problem is curiously equivalent to the discrete logarithm problem in the subgroup spanned by g.
The Two Faces of Lattices in Cryptology	Lattices are regular arrangements of points in n-dimensional space, whose study appeared in the 19th century in both number theory and crystallography. Since the appearance of the celebrated Lenstra-Lenstra-Lovász lattice basis reduction algorithm twenty years ago, lattices have had surprising applications in cryptology. Until recently, the applications of lattices to cryptology were only negative, as lattices were used to break various cryptographic schemes. Paradoxically, several positive cryptographic applications of lattices have emerged in the past five years: there now exist public-key cryptosystems based on the hardness of lattice problems, and lattices play a crucial rôle in a few security proofs. We survey the main examples of the two faces of lattices in cryptology.
Threshold Cryptosystems Based on Factoring	We consider threshold cryptosystems over a composite modulus N where the factors of N are shared among the participants as the secret key. This is a new paradigm for threshold cryptosystems based on a composite modulus, differing from the typical treatment of RSA-based systems where a "decryption exponent" is shared among the participants. Our approach yields solutions to some open problems in threshold cryptography; in particular, we obtain the following: 1. Threshold Homomorphic Encryption. A number of applications (e.g., electronic voting or efficient multi-party computation) require threshold homomorphic encryption schemes. We present a protocol for threshold decryption of the homomorphic Goldwasser-Micali encryption scheme [34], answering an open question of [21]. 2. Threshold Cryptosystems as Secure as Factoring. We describe a threshold version of a variant of the signature standards ISO 9796-2 and PKCS#1 v1.5 (cf. [39, Section 11.3.4]), thus giving the first threshold signature scheme whose security (in the random oracle model) is equivalent to the hardness of factoring [12]. Our techniques may be adapted to distribute the Rabin encryption scheme [44] whose semantic security may be reduced to the hardness of factoring. 3. Efficient Threshold Schemes without a Trusted Dealer. Because our schemes only require sharing of N - which furthermore need not be a product of strong primes - our schemes are very efficient (compared to previous schemes) when a trusted dealer is not assumed and key generation is done in a distributed manner.Extensions to achieve robustness and proactivation are also possible with our schemes.
Cryptographic protocols provably secure against dynamic adversaries	We introduce new techniques for generating and reasoning about protocols. These techniques are based on protocol transformations that depend on the nature of the adversaries under consideration. We propose a set of definitions that, captures and unifies the intuitive notions of correctness, privacy, and robustness, and enables us to give concise and modular proofs that our protocols possess these desirable properties. Using these techniques, whose major purpose is to greatly simplify the design and verification of cryptographic protocols, we show how to construct a multiparty cryptographic protocol to compute any given feasible function of the parties' inputs. We prove that our protocol is secure against the malicious actions of any adversary, limited to feasible computation, but with the power to eavesdrop on all messages and to corrupt any dynamically chosen minority of the parties. This is the first proof of security against dynamic adversaries in the "cryptographic" model of multiparty protocols. We assume the existeuce of a one-way function and allow the participants to erase small portions of memory. Our result combines the superior resilience of the cryptographic setting of [GMW87] with the stronger (dynamic) fault pattern of the "non-cryptographic" setting of [BGW88, CCD88].
Efficient Computation Modulo a Shared Secret with Application to the Generation of Shared Safe-Prime Products	We present a new protocol for efficient distributed computation modulo a shared secret. We further present a protocol to distributively generate a random shared prime or safe prime that is much more efficient than previously known methods. This allows one to distributively compute shared RSA keys, where the modulus is the product of two safe primes, much more efficiently than was previously known.
Sharing Decryption in the Context of Voting or Lotteries	Several public key cryptosystems with additional homomorphic properties have been proposed so far. They allow to perform computation with encrypted data without the knowledge of any secret information. In many applications, the ability to perform decryption, i.e. the knowledge of the secret key, gives a huge power. A classical way to reduce the trust in such a secret owner, and consequently to increase the security, is to share the secret between many entities in such a way that cooperation between them is necessary to decrypt. In this paper, we propose a distributed version of the Paillier cryptosystem presented at Eurocrypt '99. This shared scheme can for example be used in an electronic voting scheme or in a lottery where a random number related to the winning ticket has to be jointly chosen by all participants.
Threshold Cryptosystems Secure against Chosen-Ciphertext Attacks	Semantic security against chosen-ciphertext attacks (INDCCA) is widely believed as the correct security level for public-key encryption scheme. On the other hand, it is often dangerous to give to only one people the power of decryption. Therefore, threshold cryptosystems aimed at distributing the decryption ability. However, only two efficient such schemes have been proposed so far for achieving IND-CCA. Both are El Gamal-like schemes and thus are based on the same intractability assumption, namely the Decisional Diffie-Hellman problem. In this article we rehabilitate the twin-encryption paradigm proposed by Naor and Yung to present generic conversions from a large family of (threshold) IND-CPA scheme into a (threshold) IND-CCA one in the random oracle model. An efficient instantiation is also proposed, which is based on the Paillier cryptosystem. This new construction provides the first example of threshold cryptosystem secure against chosen-ciphertext attacks based on the factorization problem. Moreover, this construction provides a scheme where the "homomorphic properties" of the original scheme still hold. This is rather cumbersome because homomorphic cryptosystems are known to be malleable and therefore not to be CCA secure. However, we do not build a "homomorphic cryptosystem", but just keep the homomorphic properties.
Fully Distributed Threshold RSA under Standard Assumptions	The aim of this article is to propose a fully distributed environment for the RSA scheme. What we have in mind is highly sensitive applications and even if we are ready to pay a price in terms of efficiency, we do not want any compromise of the security assumptions that we make. Recently Shoup proposed a practical RSA threshold signature scheme that allows to share the ability to sign between a set of players. This scheme can be used for decryption as well. However, Shoup's protocol assumes a trusted dealer to generate and distribute the keys. This comes from the fact that the scheme needs a special assumption on the RSA modulus and this kind of RSA moduli cannot be easily generated in an efficient way with many players. Of course, it is still possible to call theoretical results on multiparty computation, but we cannot hope to design efficient protocols. The only practical result to generate RSA moduli in a distributive manner is Boneh and Franklin's protocol but it seems difficult to modify it in order to generate the kind of RSA moduli that Shoup's protocol requires. The present work takes a different path by proposing a method to enhance the key generation with some additional properties and revisits Shoup's protocol to work with the resulting RSA moduli. Both of these enhancements decrease the performance of the basic protocols. However, we think that in the applications we target, these enhancements provide practical solutions. Indeed, the key generation protocol is usually run only once and the number of players used to sign or decrypt is not very large. Moreover, these players have time to perform their task so that the communication or time complexity are not overly important.
Practical Threshold RSA Signatures without a Trusted Dealer	We propose a threshold RSA scheme which is as efficient as the fastest previous threshold RSA scheme (by Shoup), but where two assumptions needed in Shoup's and in previous schemes can be dropped, namely that the modulus must be a product of safe primes and that a trusted dealer generates the keys. The robustness (but not the unforgeability) of our scheme depends on a new intractability assumption, in addition to security of the underlying standard RSA scheme.
Cryptographic Counters and Applications to Electronic Voting	We formalize the notion of a cryptographic counter, which allows a group of participants to increment and decrement a cryptographic representation of a (hidden) numerical value privately and robustly. The value of the counter can only be determined by a trusted authority (or group of authorities, which may include participants themselves), and participants cannot determine any information about the increment/decrement operations performed by other parties. Previous efficient implementations of such counters have relied on fully-homomorphic encryption schemes; this is a relatively strong requirement which not all encryption schemes satisfy. We provide an alternate approach, starting with any encryption scheme homomorphic over the additive group Z2 (i.e., 1-bit xor). As our main result, we show a general and efficient reduction from any such encryption scheme to a general cryptographic counter. Our main reduction does not use additional assumptions, is efficient, and gives a novel implementation of a general counter. The result can also be viewed as an efficient construction of a general n-bit cryptographic counter from any 1-bit counter which has the additional property that counters can be added securely. As an example of the applicability of our construction, we present a cryptographic counter based on the quadratic residuosity assumption and use it to construct an efficient voting scheme which satisfies universal verifiability, privacy, and robustness.
Multiparty Computation from Threshold Homomorphic Encryption	We introduce a new approach to multiparty computation (MPC) basing it on homomorphic threshold crypto-systems. We show that given keys for any sufficiently efficient system of this type, general MPC protocols for n parties can be devised which are secure against an active adversary that corrupts any minority of the parties. The total number of bits broadcast is O(nk|C|), where k is the security parameter and |C| is the size of a (Boolean) circuit computing the function to be securely evaluated. An earlier proposal by Franklin and Haber with the same complexity was only secure for passive adversaries, while all earlier protocols with active security had complexity at least quadratic in n. We give two examples of threshold cryptosystems that can support our construction and lead to the claimed complexities.
Adaptive Security for Threshold Cryptosystems	We present adaptively-secure efficient solutions to several central problems in the area of threshold cryptography. We prove these solutions to withstand adaptive attackers that choose parties for corruption at any time during the run of the protocol. In contrast, all previously known efficient protocols for these problems were proven secure only against less realistic static adversaries that choose and fix the subset of corrupted parties before the start of the protocol run. Specifically, we provide adaptively-secure solutions for distributed key generation in discrete-log based cryptosystems, and for the problem of distributed generation of DSS signatures (threshold DSS). We also show how to transform existent static solutions for threshold RSA and proactive schemes to withstand the stronger adaptive attackers. In doing so, we introduce several techniques for the design and analysis of adaptively-secure protocols that may well find further applications.
Group Diffie-Hellman Key Exchange Secure against Dictionary Attacks	Group Diffie-Hellman schemes for password-based key exchange are designed to provide a pool of players communicating over a public network, and sharing just a human-memorable password, with a session key (e.g, the key is used for multicast data integrity and confidentiality). The fundamental security goal to achieve in this scenario is security against dictionary attacks. While solutions have been proposed to solve this problem no formal treatment has ever been suggested. In this paper, we define a security model and then present a protocol with its security proof in both the random oracle model and the ideal-cipher model.
Authenticated key exchange secure against dictionary attacks	Password-based protocols for authenticated key exchange (AKE) are designed to work despite the use of passwords drawn from a space so small that an adversary might well enumerate, off line, all possible passwords. While several such protocols have been suggested, the underlying theory has been lagging. We begin by defining a model for this problem, one rich enough to deal with password guessing, forward secrecy, server compromise, and loss of session keys. The one model can be used to define various goals. We take AKE (with "implicit" authentication) as the "basic" goal, and we give definitions for it, and for entity-authentication goals as well. Then we prove correctness for the idea at the center of the Encrypted Key-Exchange (EKE) protocol of Bellovin and Merritt: we prove security, in an ideal-cipher model, of the two-flow protocol at the core of EKE.
Provably secure password-authenticated key exchange using Diffie-Hellman	When designing password-authenticated key exchange protocols (as opposed to key exchange protocols authenticated using cryptographically secure keys), one must not allow any information to be leaked that would allow verification of the password (a weak shared key), since an attacker who obtains this information may be able to run an off-line dictionary attack to determine the correct password. We present a new protocol called PAK which is the first Diffie-Hellman-based password-authenticated key exchange protocol to provide a formal proof of security (in the random oracle model) against both passive and active adversaries. In addition to the PAK protocol that provides mutual explicit authentication, we also show a more efficient protocol called PPK that is provably secure in the implicit -authentication model. We then extend PAK to a protocol called PAK-X, in which one side (the client) stores a plaintext version of the password, while the other side (the server) only stores a verifier for the password. We formally prove security of PAK-X, even when the server is compromised. Our formal model for password-authenticated key exchange is new, and may be of independent interest.
Provably authenticated group Diffie-Hellman key exchange	Group Diffie-Hellman protocols for Authenticated Key Exchange (AKE) are designed to provide a pool of players with a shared secret key which may later be used, for example, to achieve multicast message integrity. Over the years, several schemes have been offered. However, no formal treatment for this cryptographic problem has ever been suggested. In this paper, we present a security model for this problem and use it to precisely define AKE (with "implicit" authentication) as the fundamental goal, and the entity-authentication goal as well. We then define in this model the execution of an authenticated group Diffie-Hellman scheme and prove its security.
OAEP Reconsidered	The OAEP encryption scheme was introduced by Bellare and Rogaway at Eurocrypt '94. It converts any trapdoor permutation scheme into a public-key encryption scheme. OAEP is widely believed to provide resistance against adaptive chosen ciphertext attack. The main justification for this belief is a supposed proof of security in the random oracle model, assuming the underlying trapdoor permutation scheme is one way. This paper shows conclusively that this justification is invalid. First, it observes that there appears to be a non-trivial gap in the OAEP security proof. Second, it proves that this gap cannot be filled, in the sense that there can be no standard "black box" security reduction for OAEP. This is done by proving that there exists an oracle relative to which the general OAEP scheme is insecure. The paper also presents a new scheme OAEP+, along with a complete proof of security in the random oracle model. OAEP+ is essentially just as efficient as OAEP, and even has a tighter security reduction. It should be stressed that these results do not imply that a particular instantiation of OAEP, such as RSA-OAEP, is insecure. They simply undermine the original justification for its security. In fact, it turns out-- essentially by accident, rather than by design--that RSA-OAEP is secure in the random oracle model; however, this fact relies on special algebraic properties of the RSA function, and not on the security of the general OAEP scheme.
Group Diffie-Hellman Key Exchange Secure against Dictionary Attacks	Group Diffie-Hellman schemes for password-based key exchange are designed to provide a pool of players communicating over a public network, and sharing just a human-memorable password, with a session key (e.g, the key is used for multicast data integrity and confidentiality). The fundamental security goal to achieve in this scenario is security against dictionary attacks. While solutions have been proposed to solve this problem no formal treatment has ever been suggested. In this paper, we define a security model and then present a protocol with its security proof in both the random oracle model and the ideal-cipher model.
The Provable Security of Graph-Based One-Time Signatures and Extensions to Algebraic Signature Schemes	Essentially all known one-time signature schemes can be described as special instances of a general scheme suggested by Bleichenbacher and Maurer based on "graphs of one-way functions". Bleichenbacher and Maurer thoroughly analyze graph based signatures from a combinatorial point of view, studying the graphs that result in the most efficient schemes (with respect to various efficiency measures, but focusing mostly on key generation time). However, they do not give a proof of security of their generic construction, and they leave open the problem of determining under what assumption security can be formally proved. In this paper we analyze graph based signatures from a security point of view and give sufficient conditions that allow to prove the security of the signature scheme in the standard complexity model (no random oracles). The techniques used to prove the security of graph based one-time signatures are then applied to the construction of a new class of algebraic signature schemes, i.e., schemes where signatures can be combined with a restricted set of operations.
Transitive Signatures Based on Factoring and RSA	We present novel realizations of the transitive signature primitive introduced by Micali and Rivest [12], and also provide an answer to an open question they raise regarding the security of their RSA based scheme. Our schemes provide performance improvements over the scheme of [12].
Homomorphic Signature Schemes	Privacy homomorphisms, encryption schemes that are also homomorphisms relative to some binary operation, have been studied for some time, but one may also consider the analogous problem of homomorphic signature schemes. In this paper we introduce basic definitions of security for homomorphic signature systems, motivate the inquiry with example applications, and describe several schemes that are homomorphic with respect to useful binary operations. In particular, we describe a scheme that allows a signature holder to construct the signature on an arbitrarily redacted submessage of the originally signed message. We present another scheme for signing sets that is homomorphic with respect to both union and taking subsets. Finally, we show that any signature scheme that is homomorphic with respect to integer addition must be insecure.
A Forward-Secure Digital Signature Scheme	We describe a digital signature scheme in which the public key is fixed but the secret signing key is updated at regular intervals so as to provide a forward security property: compromise of the current secret key does not enable an adversary to forge signatures pertaining to the past. This can be useful to mitigate the damage caused by key exposure without requiring distribution of keys. Our construction uses ideas from the Fiat-Shamir and Ong-Schnorr identification and signature schemes, and is proven to be forward secure based on the hardness of factoring, in the random oracle model. The construction is also quite efficient.
Transitive Signatures Based on Factoring and RSA	We present novel realizations of the transitive signature primitive introduced by Micali and Rivest [12], and also provide an answer to an open question they raise regarding the security of their RSA based scheme. Our schemes provide performance improvements over the scheme of [12].
Homomorphic Signature Schemes	Privacy homomorphisms, encryption schemes that are also homomorphisms relative to some binary operation, have been studied for some time, but one may also consider the analogous problem of homomorphic signature schemes. In this paper we introduce basic definitions of security for homomorphic signature systems, motivate the inquiry with example applications, and describe several schemes that are homomorphic with respect to useful binary operations. In particular, we describe a scheme that allows a signature holder to construct the signature on an arbitrarily redacted submessage of the originally signed message. We present another scheme for signing sets that is homomorphic with respect to both union and taking subsets. Finally, we show that any signature scheme that is homomorphic with respect to integer addition must be insecure.
Regular and algebraic words and ordinals	We solve fixed point equations over finite and infinite words by initiality. By considering equations without and with parameters, two families of words arise, the regular and the algebraic words. Regular words were introduced by Courcelle in the late 1970's. We provide a summary of results on regular words, some of which have been obtained very recently, and include some new results for algebraic words.
Parallel structured duplicate detection	We describe a novel approach to parallelizing graph search using structured duplicate detection. Structured duplicate detection was originally developed as an approach to external-memory graph search that reduces the number of expensive disk I/O operations needed to check stored nodes for duplicates, by using an abstraction of the search graph to localize memory references. In this paper, we show that this approach can also be used to reduce the number of slow synchronization operations needed in parallel graph search. In addition, we describe several techniques for integrating parallel and external-memory graph search in an efficient way. We demonstrate the effectiveness of these techniques in a graph-search algorithm for domain-independent STRIPS planning.
Domain-independent structured duplicate detection	The scalability of graph-search algorithms can be greatly extended by using external memory, such as disk, to store generated nodes. We consider structured duplicate detection, an approach to external-memory graph search that limits the number of slow disk I/O operations needed to access search nodes stored on disk by using an abstract representation of the graph to localize memory references. For graphs with sufficient locality, structured duplicate detection outperforms other approaches to external-memory graph search. We develop an automatic method for creating an abstract representation that reveals the local structure of a graph. We then integrate this approach into a domain-independent STRIPS planner and show that it dramatically improves scalability for a wide range of planning problems. The success of this approach strongly suggests that similar local structure can be found in many other graph-search problems.
Edge partitioning in external-memory graph search	There is currently much interest in using external memory, such as disk storage, to scale up graph-search algorithms. Recent work shows that the local structure of a graph can be leveraged to substantially improve the efficiency of external-memory graph search. This paper introduces a technique, called edge partitioning, which exploits a form of local structure that has not been considered in previous work. The new technique improves the scalability of structured approaches to external-memory graph search, and also guarantees the applicability of these approaches to any graph-search problem. We show its effectiveness in an external-memory graph-search algorithm for domain-independent STRIPS planning.
Evolving open and independent ontologies	In this paper, we address the problem of evolving open and independent ontologies, by proposing a novel ontology evolution methodology called H-CHANGE and a set of related techniques. After providing a formal definition of distributed concept, we describe; change detection techniques based on semantic matchmaking for determining the semantics of change; assimilation techniques for evolving ontology metadata according to new incoming external knowledge at different integration levels, ranging from concept merging to concept alignment. Examples of evolving OWL ontologies according to H-CHANGE are also provided.
A model for compound type changes encountered in schema evolution	Schema evolution is a problem that is faced by long-lived data. When a schema changes, existing persistent data can become inaccessible unless the database system provides mechanisms to access data created with previous versions of the schema. Most existing systems that support schema evolution focus on changes local to individual types within the schema, thereby limiting the changes that the database maintainer can perform. We have developed a model of type changes involving multiple types. The model describes both type changes and their impact on data by defining derivation rules to initialize new data based on the existing data. The derivation rules can describe local and nonlocal changes to types to capture the intent of a large class of type change operations. We have built a system called Tess (Type Evolution Software System) that uses this model to recognize type changes by comparing schemas and then produces a transformer that can update data in a database to correspond to a newer version of the schema.
Evolving ontology evolution	One of the crucial tasks towards the realization of the Semantic Web vision is the efficient encoding of human knowledge in ontologies. Thus, the proper maintenance of these, usually large, structures and, in particular, their adaptation to new knowledge (ontology evolution) is one of the most challenging problems in the current Semantic Web research. In this paper, we uncover a certain gap in the current research area of ontology evolution and propose a research direction based on belief revision. We present some results in this direction and argue that our approach introduces an interesting new dimension to the problem that is likely to find important applications in the future.
Ontologies for Enterprise Knowledge Management	Ontologies are a key technology for enabling semantics-driven knowledge processing, and it is widely accepted that the next generation of knowledge management system will rely on conceptual models in the form of ontologies. Unfortunately, the development of real-world enterprise-wide ontology-based knowledge management systems is still in an early stage. The authors present an integrated enterprise knowledge management architecture developed within the Ontologging project dealing with several challenges related to applying ontologies in real-world environments. They focus on two important ontology management problemsýnamely, supporting multiple ontologies and managing ontology evolution.
Design space optimization of embedded memory systems via data remapping	In this paper, we provide a novel compile-time data remapping algorithm that runs in linear time. This remapping algorithm is the first fully automatic approach applicable to pointer-intensive dynamic applications. We show that data remapping can be used to significantly reduce the energy consumed as well as the memory size needed to meet a user-specified performance goal (i.e., execution time) -- relative to the same application executing without being remapped. These twin advantages afforded by a remapped program -- reduced cache size and energy needs -- constitute a key step in a framework for design space exploration: for any given performance goal, remapping allows the user to reduce the primary and secondary cache size by 50%, yielding a concomitant energy savings of 57%. Additionally, viewed as a compiler optimization for a fixed processor, we show that remapping improves the energy consumed by the cache subsystem by 25%. All of the above savings are in the context of the cache subsystem in isolation. We also show that remapping yields an average 20% energy saving for an ARM-like processor and cache subsystem. All of our improvements are achieved in the context of DIS, Olden and SPEC2000 pointer-centric benchmarks.
Data and memory optimization techniques for embedded systems	We present a survey of the state-of-the-art techniques used in performing data and memory-related optimizations in embedded systems. The optimizations are targeted directly or indirectly at the memory subsystem, and impact one or more out of three important cost metrics: area, performance, and power dissipation of the resulting implementation.We first examine architecture-independent optimizations in the form of code transoformations. We next cover a broad spectrum of optimization techniques that address memory architectures at varying levels of granularity, ranging from register files to on-chip memory, data caches, and dynamic memory (DRAM). We end with memory addressing related issues.
Automated data-member layout of heap objects to improve memory-hierarchy performance	We present and evaluate a simple, yet efficient optimization technique that improves memory-hierarchy performance for pointer-centric applications by up to 24% and reduces cache misses by up to 35%. This is achieved by selecting an improved ordering for the data members of pointer-based data structures. Our optimization is applicable to all type-safe programming languages that completely abstract from physical storage layout; examples of such languages are Java and Oberon. Our technique does not involve programmers in the optimization process, but runs fully automatically, guided by dynamic profiling information that captures which paths through the program are taken with that frequencey. The algorithm first strives to cluster data members that are accessed closely after one another onto the same cache line, increasing spatial locality. Then, the data members that have been mapped to a particular cache line are ordered to minimize load latency in case of a cache miss.
Cache-conscious data placement	As the gap between memory and processor speeds continues to widen, cache eficiency is an increasingly important component of processor performance. Compiler techniques have been used to improve instruction cache pet$ormance by mapping code with temporal locality to different cache blocks in the virtual address space eliminating cache conflicts. These code placement techniques can be applied directly to the problem of placing data for improved data cache pedormance.In this paper we present a general framework for Cache Conscious Data Placement. This is a compiler directed approach that creates an address placement for the stack (local variables), global variables, heap objects, and constants in order to reduce data cache misses. The placement of data objects is guided by a temporal relationship graph between objects generated via profiling. Our results show that profile driven data placement significantly reduces the data miss rate by 24% on average.
Cache-conscious structure definition	A program's cache performance can be improved by changing the organization and layout of its data---even complex, pointer-based data structures. Previous techniques improved the cache performance of these structures by arranging distinct instances to increase reference locality. These techniques produced significant performance improvements, but worked best for small structures that could be packed into a cache block.This paper extends that work by concentrating on the internal organization of fields in a data structure. It describes two techniques---structure splitting and field reordering---that improve the cache behavior of structures larger than a cache block. For structures comparable in size to a cache block, structure splitting can increase the number of hot fields that can be placed in a cache block. In five Java programs, structure splitting reduced cache miss rates 10--27% and improved performance 6--18% beyond the benefits of previously described cache-conscious reorganization techniques.For large structures, which span many cache blocks, reordering fields, to place those with high temporal affinity in the same cache block can also improve cache utilization. This paper describes bbcache, a tool that recommends C structure field reorderings. Preliminary measurements indicate that reordering fields in 5 active structures improves the performance of Microsoft SQL Server 7.0 2--3%.
Uncertainty-aware and coverage-oriented deployment for sensor networks	We consider the sensor deployment problem in the context of uncertainty in sensor locations subsequent to airdropping. Sensor deployment in such scenarios is inherently non-deterministic and there is a certain degree of randomness associated with the location of a sensor in the sensor field. We present two algorithms for the efficient placement of sensors in a sensor field when the exact locations of the sensors are not known. The proposed approach is aimed at optimizing the number of sensors and determining their placement to support distributed sensor networks. These algorithms are targeted at average coverage as well as at maximizing the coverage of the most vulnerable regions in the sensor field. Experimental results for an example sensor field demonstrate the application of our approach.
Exposure in wireless Ad-Hoc sensor networks	Wireless ad-hoc sensor networks will provide one of the missing connections between the Internet and the physical world. One of the fundamental problems in sensor networks is the calculation of coverage. Exposure is directly related to coverage in that it is a measure of how well an object, moving on an arbitrary path, can be observed by the sensor network over a period of time.In addition to the informal definition, we formally define exposure and study its properties. We have developed an efficient and effective algorithm for exposure calculation in sensor networks, specifically for finding minimal exposure paths. The minimal exposure path provides valuable information about the worst case exposure-based coverage in sensor networks. The algorithm works for any given distribution of sensors, sensor and intensity models, and characteristics of the network. It provides an unbounded level of accuracy as a function of run time and storage. We provide an extensive collection of experimental results and study the scaling behavior of exposure and the proposed algorithm for its calculation.
A dual-space approach to tracking and sensor management in wireless sensor networks	Wireless ad hoc sensor networks have the advantage of spanning a large geographical region and being able to collaboratively detect and track non-local spatio-temporal events. This paper presents a dual-space approach to event tracking and sensor resource management in sensor networks. The dual-space transformation maps a non-local phenomenon, e.g., the edge of a half-plane shadow, to a single point in the dual space, and maps locations of distributed sensor nodes to a set of lines that partitions the dual space. The detection problem becomes finding and tracking the cell that contains the point in the arrangement defined by these lines. This mechanism can be effectively used for power management of the sensor network - nodes that will not be immediately visited by an event can be turned off to save energy required for sensing, processing, and communication. The approach has been successfully demonstrated on a laboratory testbed built using the UC Berkeley motes sensors. An implemented application of detecting and tracking light shadow edges moving over a sensor field is described.
Sensor deployment strategy for target detection	In order to monitor a region for traffic traversal, sensors can be deployed to perform collaborative target detection. Such a sensor network achieves a certain level of detection performance with an associated cost of deployment. This paper addresses this problem by proposing path exposure as a measure of the goodness of a deployment and presents an approach for sequential deployment in steps. It illustrates that the cost of deployment can be minimized to achieve the desired detection performance by appropriately choosing the number of sensors deployed in each step.
Analysis of resource transfers in peer-to-peer file sharing applications using fluid models	This paper proposes a stochastic fluid flow model to compute the transfer time distribution of resources in peer-to-peer file sharing applications. The amount of bytes transferred among peers is represented by a continuous quantity (the fluid level) whose flow rate is modulated by a set of discrete states representing the concurrent upload and download operations on the peers participating to the transfer. A transient solution of the model is then performed to compute the probability that a peer can download a given resource in less than t units of time as a function of several system parameters. In particular, the impact of file popularity, bandwidth characteristics, concurrent downloads and uploads, cooperation level among peers, and user behavior are included in our model specification.We also provide numerical results aiming at proving the potentialities of the approach we adopted as well as to investigate interesting issues related to the effect of incentive mechanisms on the user cooperation.
Measurement, modeling, and analysis of a peer-to-peer file-sharing workload	Peer-to-peer (P2P) file sharing accounts for an astonishing volume of current Internet traffic. This paper probes deeply into modern P2P file sharing systems and the forces that drive them. By doing so, we seek to increase our understanding of P2P file sharing workloads and their implications for future multimedia workloads. Our research uses a three-tiered approach. First, we analyze a 200-day trace of over 20 terabytes of Kazaa P2P traffic collected at the University of Washington. Second, we develop a model of multimedia workloads that lets us isolate, vary, and explore the impact of key system parameters. Our model, which we parameterize with statistics from our trace, lets us confirm various hypotheses about file-sharing behavior observed in the trace. Third, we explore the potential impact of locality-awareness in Kazaa.Our results reveal dramatic differences between P2P file sharing and Web traffic. For example, we show how the immutability of Kazaa's multimedia objects leads clients to fetch objects at most once; in contrast, a World-Wide Web client may fetch a popular page (e.g., CNN or Google) thousands of times. Moreover, we demonstrate that: (1) this "fetch-at-most-once" behavior causes the Kazaa popularity distribution to deviate substantially from Zipf curves we see for the Web, and (2) this deviation has significant implications for the performance of multimedia file-sharing systems. Unlike the Web, whose workload is driven by document change, we demonstrate that clients' fetch-at-most-once behavior, the creation of new objects, and the addition of new clients to the system are the primary forces that drive multimedia workloads such as Kazaa. We also show that there is substantial untapped locality in the Kazaa workload. Finally, we quantify the potential bandwidth savings that locality-aware P2P file-sharing architectures would achieve.
An analysis of internet content delivery systems	In the span of only a few years, the Internet has experienced an astronomical increase in the use of specialized content delivery systems, such as content delivery networks and peer-to-peer file sharing systems. Therefore, an understanding of content delivery on the Internet now requires a detailed understanding of how these systems are used in practice.This paper examines content delivery from the point of view of four content delivery systems: HTTP web traffic, the Akamai content delivery network, and Kazaa and Gnutella peer-to-peer file sharing traffic. We collected a trace of all incoming and outgoing network traffic at the University of Washington, a large university with over 60,000 students, faculty, and staff. From this trace, we isolated and characterized traffic belonging to each of these four delivery classes. Our results (1) quantify the rapidly increasing importance of new content delivery systems, particularly peer-to-peer networks, (2) characterize the behavior of these systems from the perspectives of clients, objects, and servers, and (3) derive implications for caching in these systems.
The maude formal tool environment	This paper describes the main features of several tools concerned with the analysis of either Maude specifications, or of extensions of such specifications: the ITP, MTT, CRC, ChC, and SCC tools, and Real-Time Maude for real-time systems. These tools, together with Maude itself and its searching and model-checking capabilities, constitute Maude's formal environment.
Semantics and pragmatics of Real-Time Maude	At present, designers of real-time systems face a dilemma between expressiveness and automatic verification: if they can specify some aspects of their system in some automaton-based formalism, then automatic verification is possible; but more complex system components may be hard or impossible to express in such decidable formalisms. These more complex components may still be simulated; but there is then little support for their formal analysis. The main goal of Real-Time Maude is to provide a way out of this dilemma, while complementing both decision procedures and simulation tools. Real-Time Maude emphasizes ease and generality of specification, including support for distributed real-time object-based systems. Because of its generality, falling outside of decidable system classes, the formal analyses supported--including symbolic simulation, breadth-first search for failures of safety properties, and model checking of time-bounded temporal logic properties--are in general incomplete (although they are complete for discrete time). These analysis techniques have been shown useful in finding subtle bugs of complex systems, clearly outside the scope of current decision procedures. This paper describes both the semantics of Real-Time Maude specifications, and of the formal analyses supported by the tool. It also explains the tool's pragmatics, both in the use of its features, and in its application to concrete examples.
Proving operational termination of membership equational programs	Reasoning about the termination of equational programs in sophisticated equational languages such as Elan, Maude, OBJ, CafeOBJ, Haskell, and so on, requires support for advanced features such as evaluation strategies, rewriting modulo, use of extra variables in conditions, partiality, and expressive type systems (possibly including polymorphism and higher-order). However, many of those features are, at best, only partially supported by current term rewriting termination tools (for instance mu-term, C i ME, AProVE, TTT, Termptation, etc.) while they may be essential to ensure termination. We present a sequence of theory transformations that can be used to bridge the gap between expressive membership equational programs and such termination tools, and prove the correctness of such transformations. We also discuss a prototype tool performing the transformations on Maude equational programs and sending the resulting transformed theories to some of the aforementioned standard termination tools.
Recompose: direct and gestural interaction with an actuated surface	In this paper we present Recompose, a new system for manipulation of an actuated surface. By collectively utilizing the body as a tool for direct manipulation alongside gestural input for functional manipulation, we show how a user is afforded unprecedented control over an actuated surface. We describe a number of interaction techniques exploring the shared space of direct and gestural input, demonstrating how their combined use can greatly enhance creation and manipulation beyond unaided human capability.
Illuminating clay: a 3-D tangible interface for landscape analysis	This paper describes a novel system for the real-time computational analysis of landscape models. Users of the system - called Illuminating Clay - alter the topography of a clay landscape model while the changing geometry is captured in real-time by a ceiling-mounted laser scanner. A depth image of the model serves as an input to a library of landscape analysis functions. The results of this analysis are projected back into the workspace and registered with the surfaces of the model.We describe a scenario for which this kind of tool has been developed and we review past work that has taken a similar approach. We describe our system architecture and highlight specific technical issues in its implementation.We conclude with a discussion of the benefits of the system in combining the tangible immediacy of physical models with the dynamic capabilities of computational simulations.
Collaborative interaction with volumetric displays	Volumetric displays possess a number of unique properties which potentially make them particularly suitable for collaborative 3D applications. Because such displays have only recently become available, interaction techniques for collaborative usage have yet to be explored. In this paper, we initiate this exploration. We present a prototype collaborative 3D model viewing application, which served as a platform for our explorations. We outline three design goals, discuss the key interaction issues which were encountered, and describe a suite of new techniques in detail. In initial user observation sessions, we found that our techniques allowed users to successfully complete a variety of 3D tasks. Furthermore, interviews with experts in potential usage domains indicated that the techniques we developed can serve as a baseline for future collaborative applications for volumetric displays.
Relief: a scalable actuated shape display	Relief is an actuated tabletop display, which is able to render and animate three-dimensional shapes with a malleable surface. It allows users to experience and form digital models like geographical terrain in an intuitive manner. The tabletop surface is actuated by an array of 120 motorized pins, which are controlled with a low-cost, scalable platform built upon open-source hardware and software tools. Each pin can be addressed individually and senses user input like pulling and pushing.
Sensor-centric energy-constrained reliable query routing for wireless sensor networks	Standard wireless sensor network models emphasize energy efficiency and distributed decision-making by considering untethered and unattended sensors. To this we add two constraints--the possibility of sensor failure and the fact that each sensor must tradeoff its own resource consumption with overall network objectives. In this paper, we develop an analytical model of energy-constrained, reliable, data-centric information routing in sensor networks under all the above constraints. Unlike existing techniques, we use game theory to model intelligent sensors thereby making our approach sensor-centric. Sensors behave as rational players in an N-player routing game, where they tradeoff individual communication and other costs with network wide benefits. The outcome of the sensor behavior is a sequence of communication link establishments, resulting in routing paths from reporting to querying sensors. We show that the optimal routing architecture is the Nash equilibrium of the N-player routing game and that computing the optimal paths (which maximizes payoffs of the individual sensors) is NP-Hard with and without data-aggregation. We develop a game-theoretic metric called path weakness to measure the qualitative performance of different routing mechanisms. This sensor-centric concept which is based on the contribution of individual sensors to the overall routing objective is used to define the quality of routing (QoR) paths. Analytical results on computing paths of bounded weakness are derived and game-theoretic heuristics for finding approximately optimal paths are presented. Simulation results are used to compare the QoR of different routing paths derived using various energy-constrained routing algorithms.
Directed diffusion: a scalable and robust communication paradigm for sensor networks	Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is datacentric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
Denial of Service in Sensor Networks	Sensor networks hold the promise off acilitating large-scale, real-time data processing in complex environments, environments, helping to protect and monitor military, environmental, safety-critical, or domestic infrastructures and resources. Denial-of-service attacks against such networks, however, may permit real world damage to public health and safety. Without proper security mechanisms, networks will be confined to limited, controlled environments, negating much of the promise they hold. The limited ability ofindividual sensor nodes to thwart failure or attack makes ensuring network availability more difficult.To identify denial-of-service vulnerabilities, the authors analyzed two effective sensor network protocols that did not initially consider security. These examples demonstrate that consideration of security at design time is the best way to ensure successful network deployment.
Wireless sensor networks: a survey	This paper describes the concept of sensor networks which has been made viable by the convergence of micro-electro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are also discussed.
A Survey of Energy Efficient Network Protocols for Wireless Networks	Wireless networking has witnessed an explosion of interest from consumers in recent years for its applications in mobile and personal communications. As wireless networks become an integral component of the modern communication infrastructure, energy efficiency will be an important design consideration due to the limited battery life of mobile terminals. Power conservation techniques are commonly used in the hardware design of such systems. Since the network interface is a significant consumer of power, considerable research has been devoted to low-power design of the entire network protocol stack of wireless networks in an effort to enhance energy efficiency. This paper presents a comprehensive summary of recent work addressing energy efficient and low-power design within all layers of the wireless network protocol stack.
A model of consumer acceptance of mobile payment	One promising area of mobile commerce (m-commerce) that is receiving growing attention globally is mobile payment (m-payment). m-payment refers to making payments using mobile devices. Understanding the determinants of consumer acceptance of m-payment will provide important theoretical contributions to the field and lead to the development of more effective m-payment devices and systems. By expanding the Technology Acceptance Model (TAM) and the Innovation Diffusion Theory (IDT), this study proposes a research model that examines the factors which determine consumer acceptance of m-payment. Significant support for the model was found in the data collected from a survey of 299 potential m-payment users. Implications for practice and suggestions for future research are provided.
Exploring m-commerce in terms of viability, growth and challenges	The growth and use of m-commerce as an emerging technology has the potential to dramatically change the way consumers do business; hence m-commerce has enormous potential to be the dominant form of transactions. The benefits will be gradually introduced and will first be adopted by large companies, and early adopters look for value-added operation efficiencies and convenience, as proposed and empirically tested in this study. The industry needs to develop standard networks, platform and devices and also create reliability in the medium through which m-commerce will take place. From the empirical section of this paper, m-commerce has become more available and user friendly and is considered a pull market as reflected in increasingly positive attitudes towards the implication of such technology into routine decision-making tasks.
CheMO: mixed object instruments and interactions for tangible chemistry experiments	In this paper, we present CheMO, a system for tangible chemistry experiments where users can interact with Mixed Object (MO) instruments that consist of a graspable physical part in the real world and a digital part in a virtual world. When used for an experiment, MO instruments enable users to employ tangible interaction methods inherited from real experience and to be given digital information similar to a physical expression arising from an actual experiment. The goal of our research is to enhance the sense of reality in a virtual experiment and to enable users to learn experimental procedures effectively and easily.
IncreTable, a mixed reality tabletop game experience	IncreTable is a mixed reality tabletop game inspired by The Incredible Machine. Users can combine real and virtual game pieces in order to solve puzzles in the game. Game actions include placing virtual domino blocks with digital pens, controlling a virtual car by modifying the virtual terrain through a depth camera interface or controlling real robots to topple over real and virtual dominoes.
Emerging frameworks for tangible user interfaces	We present steps toward a conceptual framework for tangible user interfaces. We introduce the MCRpd interaction model for tangible interfaces, which relates the role of physical and digital representations, physical control, and underlying digital models. This model serves as a foundation for identifying and discussing several key characteristics of tangible user interfaces. We identify a number of systems exhibiting these characteristics, and situate these within 12 application domains. Finally, we discuss tangible interfaces in the context of related research themes, both within and outside of the human-computer interaction domain.
Mixed reality: a model of mixed interaction	Mixed reality systems seek to smoothly link the physical and data processing (digital) environments. Although mixed reality systems are becoming more prevalent, we still do not have a clear understanding of this interaction paradigm. Addressing this problem, this article introduces a new interaction model called Mixed Interaction model. It adopts a unified point of view on mixed reality systems by considering the interaction modalities and forms of multimodality that are involved for defining mixed environments. This article presents the model and its foundations. We then study its unifying and descriptive power by comparing it with existing classification schemes. We finally focus on the generative and evaluative power of the Mixed Interaction model by applying it to design and compare alternative interaction techniques in the context of RAZZLE, a mobile mixed reality game for which the goal of the mobile player is to collect digital jigsaw pieces localized in space.
Constructing virtual 3D models with physical building blocks	Constructing virtual 3D models typically requires specialized desktop modeling tools (e.g., CAD tools), which, while very powerful, tend to require a lot of precision, time, and expertise from the user. We present StereoBlocks, a system that combines a Kinect depth camera with 3D stereoscopic projector to allow the user to build complex virtual 3D models from available physical objects. By treating the camera information as a continuous 3D digitizer, we are able to capture the details of the real world and re-project virtual objects side-by-side to real objects. The user is able to visualize such mixed reality model through stereoscopic projected imagery tightly aligned with the real world. In our system, it is literally possible to build the entire virtual castle, using only a single physical brick piece. We discuss our prototype implementation and report on early feedback from the four users that evaluated our system.
Illuminating clay: a 3-D tangible interface for landscape analysis	This paper describes a novel system for the real-time computational analysis of landscape models. Users of the system - called Illuminating Clay - alter the topography of a clay landscape model while the changing geometry is captured in real-time by a ceiling-mounted laser scanner. A depth image of the model serves as an input to a library of landscape analysis functions. The results of this analysis are projected back into the workspace and registered with the surfaces of the model.We describe a scenario for which this kind of tool has been developed and we review past work that has taken a similar approach. We describe our system architecture and highlight specific technical issues in its implementation.We conclude with a discussion of the benefits of the system in combining the tangible immediacy of physical models with the dynamic capabilities of computational simulations.
An interface for creating and manipulating curves using a high degree-of-freedom curve input device	Current interfaces for manipulating curves typically use a standard point cursor to indirectly adjust curve parameters. We present an interface for far more direct manipulation of curves using a specialized high degree-of-freedom curve input device, called ShapeTape. This device allows us to directly control the shape and position of a virtual curve widget. We describe the design and implementation of a variety of interaction techniques that use this curve widget to create and manipulate other virtual curves in 2D and 3D space. The input device is also used to sense a set of user gestures for invoking commands and tools. The result is an effective alternate user interface for curve manipulation that can be used in 2D and 3D graphics applications.
Identifying the differences between stationary office support and mobile work support: a conceptual framework	The rapid development of mobile technologies provides great potential to support mobile work that was not supported by traditional stationary information systems. To realise this great potential, it is important for us to fully understand the nature of mobile work in order to develop efficient and effective support for mobile workers. In this paper, we propose a conceptual framework and use this framework to analyse four fundamental aspects of mobile work: mobile workers, mobile tasks, mobile context and mobile technology. The key differences between office work support and mobile work support are highlighted. The conceptual framework can be used to identify research issues and provide guidelines for the development of effective mobile work support systems.
“Making place” to make IT work: empirical explorations of HCI for mobile CSCW	This paper addresses issues of user interface design, relating to ease of use, of handheld CSCW. In particular, we are concerned with the requirements that arise from situations in which a traditionally designed mobile computer with a small keyboard and screen, may not be easily used. This applies to many mobile use contexts, such as inspection work and engineering in the field. By examining two such settings, we assert that what is usually pointed to as severe shortcomings of mobile computing today, for example: awkward keyboard, small display and unreliable networks, are really implications from a conceptual HCI design that emphasise unstructured, unlimited input; a rich, continuous visual feedback channel and marginal use of sound. We introduce MOTILE, a small prototype that demonstrates some alternative ideas about HCI for mobile devices. We suggest that identifying complementing user interface paradigms for handheld CSCW may enhance our understanding not only of mobile computing or handheld CSCW, but the CSCW field as a whole.
Exploring m-commerce in terms of viability, growth and challenges	The growth and use of m-commerce as an emerging technology has the potential to dramatically change the way consumers do business; hence m-commerce has enormous potential to be the dominant form of transactions. The benefits will be gradually introduced and will first be adopted by large companies, and early adopters look for value-added operation efficiencies and convenience, as proposed and empirically tested in this study. The industry needs to develop standard networks, platform and devices and also create reliability in the medium through which m-commerce will take place. From the empirical section of this paper, m-commerce has become more available and user friendly and is considered a pull market as reflected in increasingly positive attitudes towards the implication of such technology into routine decision-making tasks.
Exploring the inherent benefits of RFID and automated self-serve checkouts in a B2C environment	Automated identification services such as RFID and self-serve checkouts that require many different technological components in order to successfully operate and be accepted in a B2C (Business-to-Customer) environment. In theory, using self-checkouts as a proxy for RFID applications allows for an investigation of potential consumers' acceptance of self-service technology to be determined when applied to a retail environment. In terms of factor analysis results, three independent constructs were found from the interval Likert-type and binary discrete variables from the questionnaire data. The three major constructs that were generated from the factor loadings, renamed based on the variables that loaded with loadings equal to greater than 0.5, included: positive experience, privacy and demographics, and acceptability of technology. The independent constructs of positive experience (t = 6.296, p = 0.000) and acceptability of technology (t = 2.478, p = 0.016) were the most important factors in predicting the frequency of use of such automated technologies in a retail grocery setting.
Hoarding content for mobile learning	M-learning is currently a rapidly expanding domain. Provoked by the fast advances of mobile technologies, different applications and systems are developed continuously. Here we address the hoarding problem, which is weakly explored before but is a particularly important issue in the mobile domain, and a solution should be included in every system with a large quantity of data. Hoarding is the process of automatically selecting learning content, which is to be prepared and prefetched on the mobile device's local memory for the following offline session. We describe the hoarding problem and the strategy to solve it with the goal of providing an efficient hoarding solution.
UbiData: ubiquitous mobile file service	One of the most challenging objectives of mobile data management is the ubiquitous, any time, anywhere access. This objective is very difficult to meet due to several network and mobile device limitations. Optimistic data replication is a generally agreed upon approach to alleviating the difficulty of data access in the adverse mobile environment. However, the two currently most popular models, both Client/Server and Peer-to-Peer models, do not adequately meet the ubiquity objectives. In our views, mobile data management should adequately support access to any data source, from any mobile device. It should also eliminate user involvement by automating data selection, hoarding, and synchronization, regardless of the mobile device chosen by the user. In this paper, we present UbiData: an application-transparent, double-middleware architecture that addresses these challenges. UbiData supports access and update to data from heterogeneous sources (e.g. files belonging to different file systems). It provides for the automatic and device-independent selection, hoarding, and synchronization of data. We present the UbiData architecture and system component, and evaluate the effectiveness of UbiData's automatic data selection and hoarding mechanisms.
A survey of web caching schemes for the Internet	The World Wide Web can be considered as a large distributed information system that provides access to shared data objects. As one of the most popular applications currently running on the Internet, the World Wide Web is of an exponential growth in size, which results in network congestion and server overloading. Web caching has been recognized as one of the effective schemes to alleviate the service bottleneck and reduce the network traffic, thereby minimize the user access latency. In this paper, we first describe the elements of a Web caching system and its desirable properties. Then, we survey the state-of-art techniques which have been used in Web caching systems. Finally, we discuss the research frontier in Web caching.
A UML2 profile for service modeling	In this article we provide an embedding of an interaction-based service notion into UML2. Such an embedding is needed, because to this date, UML2 has only limited support for services - they are certainly not first-class modeling elements of the notation. This is despite the ever increasing importance of services as an integration paradigm for ultra large scale systems. The embedding we provide rests on two observations: (i) services are fundamentally defined by component collaborations; (ii) to support a seamless development process, the service notion must span both logical and deployment architecture. To satisfy (i) and (ii) we introduce modifications to the UML that focus on interaction modeling, and the mapping from logical to deployment service architectures. The result is a novel and comprehensive UML2 profile for service-oriented systems.
A formal model of services	Service-oriented software systems rapidly gain importance across application domains: They emphasize functionality (services), rather structural entities (components), as the basic building block for system composition. More specifically, services coordinate the interplay of components to accomplish specific tasks. In this article, we establish a foundation of service orientation: Based on the Focus theory of distributed systems (see Broy and Stølen [2001]), we introduce a theory and formal model of services. In Focus, systems are composed of interacting components. A component is a total behavior. We introduce a formal model of services where, in contrast, a service is a partial behavior. For services and components, we work out foundational specification techniques and outline methodological development steps. We show how services can be structured and how software architectures can be composed of services and components. Although our emphasis is on a theoretical foundation of the notion of services, we demonstrate utility of the concepts we introduce by means of a running example from the automotive domain.
Modeling crosscutting services with UML sequence diagrams	Current software systems increasingly consist of distributed interacting components. The use of web services and similar middleware technologies strongly fosters such architectures. The complexity resulting from a high degree of interaction between distributed components – that we face with web service orchestration for example – poses severe problems. A promising approach to handle this intricacy is service-oriented development; in particular with a do-main-unspecific service notion based on interaction patterns. Here, a service is defined by the interplay of distributed system entities, which can be modeled using UML Sequence Diagrams. However, we often face functionality that affects or is spanned across the behavior of other services; a similar concept to aspects in Aspect-Oriented Programming. In the service-oriented world, such aspects form crosscutting services. In this paper we show how to model those; we introduce aspect-oriented modeling techniques for UML Sequence Dia-grams and show their usefulness by means of a running example.
Research areas and challenges for mobile information systems	This paper explores new challenges and possible approaches for developing mobile information systems, with an emphasis on model-based approaches on the conceptual and logical levels. Over the last few years, we have experienced these new challenges through our involvement in several research and industrial projects on mobile solutions, usability and model-based approaches. We summarise the main challenges of how model-based approaches can support the development of mobile information systems that are to be used together with other types of systems in a primarily professional setting and indicate upcoming research issues in this very dynamic area.
Interaction as a framework for flexible workflow modelling	There are a number of approaches to making workflow management systems more flexible. Most follow conventional notions of workflow models as formally complete and consistent, and look at how change can be handled by migrating instances from one stable state to another. This paper argues that interaction should be pursued more vigorously as an approach to enactment. In this framework, interpretation is not fully automated. Involving users in situated model interpretation, interactive enactment allows inconsistent and incomplete models to emerge, better matching the contingencies of real work. This reassessment of the concept of workflow models is illustrated by the Workware prototype and modelling language, showing that the interaction perspective can inform design of simple and flexible workflow architectures. A case from an interorganisational project further illustrates this.
Task Modelling for Context-Sensitive User Interfaces	With the explosion of devices, computing platforms, contextual conditions, user interfaces become more confronted to a need to be adapted to multiple configurations of the context of use. In the past, many techniques were developed to perform a task analysis for obtaining a single user interface that is adapted for a single context of use. As this user interface may become unusable for other contexts of use, there emerges a need for modelling tasks which can be supported in multiple contexts of use, considering multiple combinations of the contextual conditions. For this purpose, the concept of unit task is exploited to identify a point where traditional task models can break into two parts: a contextinsensitive part and a context-sensitive part. A widespread task model notation is then used to examine, discuss, and criticise possible configurations for modelling a context-sensitive task as a whole. One particular form is selected that attempts to achieve a clear separation of concern between the context-insensitive part, the context-sensitive part, and a new decision tree which branches to context-sensitive tasks, depending on contextual conditions. The questions of factoring out possible elements that are common across multiple contexts of use and representation of the resulting task model are discussed.
Conceptual and operational definition of system complexity in the domain of Wireless Internet via Mobile Technology	Acceptance of new information technologies by their intended users endures as an important issue for researchers and practitioners of information systems. Research has shown that the complexity of technology is an important factor influencing user acceptance, but few studies have been done in conjunction with Wireless Internet via Mobile Technology (WIMT). This study defines and explores four aspects of the System Complexity construct in the WIMT domain (Efficiency of Data Transfer, System Functionality, Interface Design and Capacity of Mobile Devices). A measurement instrument is developed and validated for this construct. Implications for theory and practice are discussed with suggestions for future research in this area.
Determinants of User Acceptance of Digital Libraries: An Empirical Examination of Individual Differences and System Characteristics	The explosion in Internet usage and huge government funding initiatives in digital libraries have drawn attention to research on digital libraries. Whereas the traditional focus of digital library research has been on the technological development, there is now a call for user-focused research. Although millions of dollars have been spent on building usable systems, research on digital libraries has shown that potential users may not use the systems in spite of their availability. There is a need for research to identify the factors that determine users' adoption of digital libraries. Using the technology acceptance model (TAM) as a theoretical framework, this study investigates the effect of a set of individual differences (computer self-efficacy and knowledge of search domain) and system characteristics (relevance, terminology, and screen design) on intention to use digital libraries. Based on a sample of 585 users of a university's award-winning digital library, the results strongly support the utilization of TAM in predicting users' intention to adopt digital libraries, and demonstrate the effects of critical external variables on behavior intention through perceived ease of use and perceived usefulness. All of the individual differences and system characteristics have significant effects on perceived ease of use of digital libraries. In addition, relevance has the strongest effect on perceived usefulness of digital libraries.
Modeling and Distributed Simulation of a Broadband-ISDN Network	A distributed approach to communication network simulation using a network of workstations configured as a loosely coupled parallel processor to model and simulate the broadband integrated services digital network (B-ISDN) is proposed. In a loosely coupled parallel processor system, a number of concurrently executable processors communicate asynchronously using explicit messages over high-speed links. Since this architecture is similar to that of B-ISDN networks, it constitutes a realistic testbed for their modeling and simulation. The authors describe an implementation of this approach on 50 Sun workstations at Brown University. Performance results, based on representative B-ISDN networks and realistic traffic models, indicate that the distributed approach is efficient and accurate.
Virtual time	Virtual time is a new paradigm for organizing and synchronizing distributed systems which can be applied to such problems as distributed discrete event simulation and distributed database concurrency control. Virtual time provides a flexible abstraction of real time in much the same way that virtual memory provides an abstraction of real memory. It is implemented using the Time Warp mechanism, a synchronization protocol distinguished by its reliance on lookahead-rollback, and by its implementation of rollback via antimessages.
Combining social-based and information-based approaches for personalised recommendation on sequencing learning activities	Lifelong learners who select learning activities to attain certain learning goals need to know which are suitable and in which sequence they should be performed. Learners need support in this way-finding process, and we argue that this could be provided by using Personalised Recommender Systems (PRSs). To enable personalisation, collaborative filtering could use information about learners and learning activities, since their alignment contributes to learning efficiency. A model for way-finding presents personalised recommendations in relation to information about learning goals, learning activities and learners. A PRS has been developed according to this model, and recommends to learners the best next learning activities. Both model and system combine social-based (i.e., completion data from other learners) and information-based (i.e., metadata from learner profiles and learning activities) approaches to recommend the best next learning activity to be completed.
Evaluating collaborative filtering recommender systems	Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.
Expressions for batched searching of sequential and hierarchical files	Batching yields significant savings in access costs in sequential, tree structured, and random files. A direct and simple expression is developed for computing the average number of records/pages accessed to satisfy a batched query of a sequential tile. The advantages of batching for sequential and random files are discussed. A direct equation is provided for the number of nodes accessed in unhatched queries of hierarchical files. An exact recursive expression is developed for node accesses in batched queries of hierarchical files. In addition to the recursive relationship, good, closed-form upper- and lower-bound approximations are provided for the case of batched queries of hierarchical files.
An attribute based model for database access cost analysis	A generalized model for physical database organizations is presented. Existing database organizations are shown to fit easily into the model as special cases. Generalized access algorithms and cost equations associated with the model are developed and analyzed. The model provides a general design framework in which the distinguishing properties of database organizations are made explicit and their performances can be compared.
Compiling and securing cryptographic protocols	Protocol narrations are widely used in security as semi-formal notations to specify conversations between roles. We define a translation from a protocol narration to the sequences of operations to be performed by each role. Unlike previous works, we reduce this compilation process to well-known decision problems in formal protocol analysis. This allows one to define a natural notion of prudent translation and to reuse many known results from the literature in order to cover more crypto-primitives. In particular this work is the first one to show how to compile protocols parameterised by the properties of the available operations.
Deciding knowledge in security protocols for monoidal equational theories	In formal approaches, messages sent over a network are usually modeled by terms together with an equational theory, axiomatizing the properties of the cryptographic functions (encryption, exclusive or, ...). The analysis of cryptographic protocols requires a precise understanding of the attacker knowledge. Two standard notions are usually used: deducibility and indistinguishability. Only few results have been obtained (in an ad-hoc way) for equational theories with associative and commutative properties, especially in the case of static equivalence. The main contribution of this paper is to propose a general setting for solving deducibility and indistinguishability for an important class (called monoidal) of these theories. Our setting relies on the correspondence between a monoidal theory E and a semiring SE which allows us to give an algebraic characterization of the deducibility and indistinguishability problems. As a consequence we recover easily existing decidability results and obtain several new ones.
Deciding knowledge in security protocols under equational theories	The analysis of security protocols requires precise formulations of the knowledge of protocol participants and attackers. In formal approaches this knowledge is often treated in terms of message deducibility and indistinguishability relations. In this paper we study the decidability of these two relations. The messages in question may employ functions (encryption, decryption, etc.) axiomatized in an equational theory. One of our main positive results says that deducibility and indistinguishability are both decidable in polynomial time for a large class of equational theories. This class of equational theories is defined syntactically and includes, for example, theories for encryption, decryption, and digital signatures. We also establish general decidability theorems for an even larger class of theories. These theorems require only loose, abstract conditions, and apply to many other useful theories, for example with blind digital signatures, homomorphic encryption, XOR, and other associative-commutative functions.
Intruders with caps	In the analysis of cryptographic protocols, a treacherous set of terms is one from which an intruder can get access to what was intended to be secret, by adding on to the top of a sequence of elements of this set, a cap formed of symbols legally part of his/her knowledge. In this paper, we give sufficient conditions on the rewrite system modeling the intruder's abilities, such as using encryption and decryption functions, to ensure that it is decidable if such caps exist. The following classes of intruder systems are studied: linear, dwindling, Δ-strong, and optimally reducing; and depending on the class considered, the cap problem ("find a cap for a given set of terms") is shown respectively to be in P, NP-complete, decidable, and undecidable.
Extreme model simplification for forest rendering	Models of large forest scenes are of a geometric complexity that surpasses even the capabilities of current high end graphics hardware. We propose an extreme simplification method which allows us to render such scenes in realtime. Our work is an extension of the image based-simplification method of Billboard Clouds. We automatically generate tree model representations of 15-50 textured polygons. In this paper, we focus on the algorithmic details to improve the simplification process for foliage. We use the simplified models as static levels-of-detail in the medium to far field and demonstrate how our approach yields real-time rendering of dense forest scenes for walkthroughs and flyovers.
Image-driven simplification	We introduce the notion of image-driven simplification, a framework that uses images to decide which portions of a model to simplify. This is a departure from approaches that make polygonal simplification decisions based on geometry. As with many methods, we use the edge collapse operator to make incremental changes to a model. Unique to our approach, however, is the use at comparisons between images of the original model against those of a simplified model to determine the cost of an ease collapse. We use common graphics rendering hardware to accelerate the creation of the required images. As expected, this method produces models that are close to the original model according to image differences. Perhaps more surprising, however, is that the method yields models that have high geometric fidelity as well. Our approach also solves the quandary of how to weight the geometric distance versus appearance properties such as normals, color, and texture. All of these trade-offs are balanced by the image metric. Benefits of this approach include high fidelity silhouettes, extreme simplification of hidden portions of a model, attention to shading interpolation effects, and simplification that is sensitive to the content of a texture. In order to better preserve the appearance of textured models, we introduce a novel technique for assigning texture coordinates to the new vertices of the mesh. This method is based on a geometric heuristic that can be integrated with any edge collapse algorithm to produce high quality textured surfaces.
Transactions are back---but are they the same?	Transactions are back in the spotlight! They are emerging in concurrent programming languages under the name of transactional memory (TM). Their new role? Concurrency control on new multi-core processors. From afar they look the same as good ol' database transactions. But are they really? In this position paper, we reflect about the distinguishing features of these memory transactions with respect to their database cousins. Disclaimer: By its very nature, this position paper does not try to avoid subjectivity.
Semantics of transactional memory and automatic mutual exclusion	Software Transactional Memory (STM) is an attractive basis for the development of language features for concurrent programming. However, the semantics of these features can be delicate and problematic. In this paper we explore the tradeoffs between semantic simplicity, the viability of efficient implementation strategies, and the flexibilityof language constructs. Specifically, we develop semantics and type systems for the constructs of the Automatic Mutual Exclusion (AME) programming model; our results apply also to other constructs, such as atomic blocks. With this semantics as a point of reference, we study several implementation strategies. We model STM systems that use in-place update, optimistic concurrency, lazy conflict detection, and roll-back. These strategies are correct only under non-trivial assumptions that we identify and analyze. One important source of errors is that some efficient implementations create dangerous 'zombie' computations where a transaction keeps running after experiencing a conflict; the assumptions confine the effects of these computations.
On the correctness of transactional memory	Transactional memory (TM) is perceived as an appealing alternative to critical sections for general purpose concurrent programming. Despite the large amount of recent work on TM implementations, however, very little effort has been devoted to precisely defining what guarantees these implementations should provide. A formal description of such guarantees is necessary in order to check the correctness of TM systems, as well as to establish TM optimality results and inherent trade-offs. This paper presents opacity, a candidate correctness criterion for TM implementations. We define opacity as a property of concurrent transaction histories and give its graph theoretical interpretation. Opacity captures precisely the correctness requirements that have been intuitively described by many TM designers. Most TM systems we know of do ensure opacity. At a very first approximation, opacity can be viewed as an extension of the classical database serializability property with the additional requirement that even non-committed transactions are prevented from accessing inconsistent states. Capturing this requirement precisely, in the context of general objects, and without precluding pragmatic strategies that are often used by modern TM implementations, such as versioning, invisible reads, lazy updates, and open nesting, is not trivial. As a use case of opacity, we prove the first lower bound on the complexity of TM implementations. Basically, we show that every single-version TM system that uses invisible reads and does not abort non-conflicting transactions requires, in the worst case, ?(k) steps for an operation to terminate, where k is the total number of objects shared by transactions. This (tight) bound precisely captures an inherent trade-off in the design of TM systems. The bound also highlights a fundamental gap between systems in which transactions can be fully isolated from the outside environment, e.g., databases or certain specialized transactional languages, and systems that lack such isolation capabilities, e.g., general TM frameworks.
Adaptive software transactional memory	Software Transactional Memory (STM) is a generic synchronization construct that enables automatic conversion of correct sequential objects into correct nonblocking concurrent objects. Recent STM systems, though significantly more practical than their predecessors, display inconsistent performance: differing design decisions cause different systems to perform best in different circumstances, often by dramatic margins. In this paper we consider four dimensions of the STM design space: (i) when concurrent objects are acquired by transactions for modification; (ii) how they are acquired; (iii) what they look like when not acquired; and (iv) the non-blocking semantics for transactions (lock-freedom vs. obstruction-freedom). In this 4-dimensional space we highlight the locations of two leading STM systems: the DSTM of Herlihy et al. and the OSTM of Fraser and Harris. Drawing motivation from the performance of a series of application benchmarks, we then present a new Adaptive STM (ASTM) system that adjusts to the offered workload, allowing it to match the performance of the best known existing system on every tested workload.
A lazy snapshot algorithm with eager validation	Most high-performance software transactional memories (STM) use optimistic invisible reads. Consequently, a transaction might have an inconsistent view of the objects it accesses unless the consistency of the view is validated whenever the view changes. Although all STMs usually detect inconsistencies at commit time, a transaction might never reach this point because an inconsistent view can provoke arbitrary behavior in the application (e.g., enter an infinite loop). In this paper, we formally introduce a lazy snapshot algorithm that verifies at each object access that the view observed by a transaction is consistent. Validating previously accessed objects is not necessary for that, however, it can be used on-demand to prolong the view's validity. We demonstrate both formally and by measurements that the performance of our approach is quite competitive by comparing other STMs with an STM that uses our algorithm.
Automatic Detection and Masking of Nonatomic Exception Handling	The development of robust software is a difficult undertaking and is becoming increasingly more important as applications grow larger and more complex. Although modern programming languages such as C++ and Java provide sophisticated exception handling mechanisms to detect and correct runtime error conditions, exception handling code must still be programmed with care to preserve application consistency. In particular, exception handling is only effective if the premature termination of a method due to an exception does not leave an object in an inconsistent state. We address this issue by introducing the notion of failure atomicity in the context of exceptions. We propose practical techniques to automatically detect and mask the nonatomic exception handling situations encountered during program execution. These techniques can be applied to applications written in various programming languages that support exceptions. We perform experimental evaluation on both C++ and Java applications to demonstrate the effectiveness of our techniques and measure the overhead that they introduce.
Software transactional memory for dynamic-sized data structures	We propose a new form of software transactional memory (STM) designed to support dynamic-sized data structures, and we describe a novel non-blocking implementation. The non-blocking property we consider is obstruction-freedom. Obstruction-freedom is weaker than lock-freedom; as a result, it admits substantially simpler and more efficient implementations. A novel feature of our obstruction-free STM implementation is its use of modular contention managers to ensure progress in practice. We illustrate the utility of our dynamic STM with a straightforward implementation of an obstruction-free red-black tree, thereby demonstrating a sophisticated non-blocking dynamic data structure that would be difficult to implement by other means. We also present the results of simple preliminary performance experiments that demonstrate that an "early release" feature of our STM is useful for reducing contention, and that our STM lends itself to the effective use of modular contention managers.
Advanced contention management for dynamic software transactional memory	The obstruction-free Dynamic Software Transactional Memory (DSTM) system of Herlihy et al@. allows only one transaction at a time to acquire an object for writing. Should a second require an object currently in use, a contention manager must determine which may proceed and which must wait or abort.We analyze both new and existing policies for this contention management problem, using experimental results from a 16-processor SunFire machine. We consider both visible and invisible versions of read access, and benchmarks that vary in complexity, level of contention, tendency toward circular dependence, and mix of reads and writes. We present fair proportional-share prioritized versions of several policies, and identify a candidate default policy: one that provides, for the first time, good performance in every case we test. The tradeoff between visible and invisible reads remains application-specific: visible reads reduce the overhead for incremental validation when opening new objects, but the requisite bookkeeping exacerbates contention for the memory interconnect.
Distributed computing and the multicore revolution	Changes in technology can have far-reaching effects on theory. For example, while Turing's work on computability predated the first electronic computers, complexity theory flowered only after computers became a reality. After all, an algorithm's complexity may not matter much in a mathematics journal, but matters quite a bit in a FORTRAN program. We argue that something similar is going on with parallel and concurrent computation: after decades of being respected but not taken seriously, research on multiprocessor algorithms and data structures is going mainstream.
High-level small-step operational semantics for transactions	Software transactions have received significant attention as a way to simplify shared-memory concurrent programming, but insufficient focus has been given to the precise meaning of software transactions or their interaction with other language features. This work begins to rectify that situation by presenting a family of formal languages that model a wide variety of behaviors for software transactions. These languages abstract away implementation details of transactional memory, providing high-level definitions suitable for programming languages. We use small-step semantics in order to represent explicitly the interleaved execution of threads that is necessary to investigate pertinent issues. We demonstrate the value of our core approach to modeling transactions by investigating two issues in depth. First, we consider parallel nesting, in which parallelism and transactions can nest arbitrarily. Second, we present multiple models for weak isolation, in which nontransactional code can violate the isolation of a transaction. For both, type-and-effect systems let us soundly and statically restrict what computation can occur inside or outside a transaction. We prove some key language-equivalence theorems to confirm that under sufficient static restrictions, in particular that each mutable memory location is used outside transactions or inside transactions (but not both), no program can determine whether the language implementation uses weak isolation or strong isolation.
Advanced contention management for dynamic software transactional memory	The obstruction-free Dynamic Software Transactional Memory (DSTM) system of Herlihy et al@. allows only one transaction at a time to acquire an object for writing. Should a second require an object currently in use, a contention manager must determine which may proceed and which must wait or abort.We analyze both new and existing policies for this contention management problem, using experimental results from a 16-processor SunFire machine. We consider both visible and invisible versions of read access, and benchmarks that vary in complexity, level of contention, tendency toward circular dependence, and mix of reads and writes. We present fair proportional-share prioritized versions of several policies, and identify a candidate default policy: one that provides, for the first time, good performance in every case we test. The tradeoff between visible and invisible reads remains application-specific: visible reads reduce the overhead for incremental validation when opening new objects, but the requisite bookkeeping exacerbates contention for the memory interconnect.
Split-ordered lists: Lock-free extensible hash tables	We present the first lock-free implementation of an extensible hash table running on current architectures. Our algorithm provides concurrent insert, delete, and find operations with an expected O(1) cost. It consists of very simple code, easily implementable using only load, store, and compare-and-swap operations. The new mathematical structure at the core of our algorithm is recursive split-ordering, a way of ordering elements in a linked list so that they can be repeatedly “split” using a single compare-and-swap operation. Metaphorically speaking, our algorithm differs from prior known algorithms in that extensibility is derived by “moving the buckets among the items” rather than “the items among the buckets.” Though lock-free algorithms are expected to work best in multiprogrammed environments, empirical tests we conducted on a large shared memory multiprocessor show that even in non-multiprogrammed environments, the new algorithm performs as well as the most efficient known lock-based resizable hash-table algorithm, and in high load cases it significantly outperforms it.
Using elimination to implement scalable and lock-free FIFO queues	This paper shows for the first time that elimination, a scaling technique formerly applied only to counters and LIFO structures, can be applied to FIFO data structures, specifically, to linearizable FIFO queues. We show how to transform existing nonscalable FIFO queue implementations into scalable implementations using the elimination technique, while preserving lock-freedom and linearizablity.We apply our transformation to the FIFO queue algorithm of Michael and Scott, which is included in the Java™ Concurrency Package. Empirical evaluation on a state-of-the-art CMT multiprocessor chip shows that by using elimination as a backoff technique for the Michael and Scott queue algorithm, we can achieve comparable performance at low loads, and improved scalability as load increases.
Obstruction-Free Synchronization: Double-Ended Queues as an Example	We introduce obstruction-freedom, a new nonblockingproperty for shared data structure implementations. Thisproperty is strong enough to avoid the problems associatedwith locks, but it is weaker than previous nonblockingproperties-specifically lock-freedom and wait-freedom-allowing greater flexibility in the design of efficient implementations.Obstruction-freedom admits substantially simplerimplementations, and we believe that in practice itprovides the benefits of wait-free and lock-free implementations.To illustrate the benefits of obstruction-freedom, wepresent two obstruction-free CAS-based implementations ofdouble-ended queues (deques); the first is implemented on alinear array, the second on a circular array. To our knowledge,all previous nonblocking deque implementations arebased on unrealistic assumptions about hardware supportfor synchronization, have restricted functionality, or haveoperations that interfere with operations at the opposite endof the deque even when the deque has many elements init. Our obstruction-free implementations have none of thesedrawbacks, and thus suggest that it is much easier to designobstruction-free implementations than lock-free and wait-freeones. We also briefly discuss other obstruction-freedata structures and operations that we have implemented.
Software transactional memory for dynamic-sized data structures	We propose a new form of software transactional memory (STM) designed to support dynamic-sized data structures, and we describe a novel non-blocking implementation. The non-blocking property we consider is obstruction-freedom. Obstruction-freedom is weaker than lock-freedom; as a result, it admits substantially simpler and more efficient implementations. A novel feature of our obstruction-free STM implementation is its use of modular contention managers to ensure progress in practice. We illustrate the utility of our dynamic STM with a straightforward implementation of an obstruction-free red-black tree, thereby demonstrating a sophisticated non-blocking dynamic data structure that would be difficult to implement by other means. We also present the results of simple preliminary performance experiments that demonstrate that an "early release" feature of our STM is useful for reducing contention, and that our STM lends itself to the effective use of modular contention managers.
Finding paths through the world's photos	When a scene is photographed many times by different people, the viewpoints often cluster along certain paths. These paths are largely specific to the scene being photographed, and follow interesting regions and viewpoints. We seek to discover a range of such paths and turn them into controls for image-based rendering. Our approach takes as input a large set of community or personal photos, reconstructs camera viewpoints, and automatically computes orbits, panoramas, canonical views, and optimal paths between views. The scene can then be interactively browsed in 3D using these controls or with six degree-of-freedom free-viewpoint control. As the user browses the scene, nearby views are continuously selected and transformed, using control-adaptive reprojection techniques.
Hierarchical photo organization using geo-relevance	We present a novel framework for organizing large collections of images in a hierarchical way, based on scene semantics. Rather than score images directly, we use them to score the scene in order to identify typical views and important locations which we term Geo-Relevance. This is done by relating each image with its viewing frustum which can be readily computed for huge collections of images nowadays. The frustum contains much more information than only camera position that has been used so far. For example, it distinguishes between a photo of the Eiffel Tower and a photo of a garbage bin taken from the exact same place. The proposed framework enables a summarized display of the information and facilitates efficient browsing.
Photo tourism: exploring photo collections in 3D	We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.
Distinctive Image Features from Scale-Invariant Keypoints	This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.
Exploration and virtual camera control in virtual three dimensional environments	This paper evaluates three distinct metaphors for exploration and virtual camera control in virtual environments using a six degree of freedom input device. The metaphors are "eyeball in hand", "scene in hand", and "flying vehicle control". These metaphors have been implemented and evaluated using an IRIS workstation and a Polhemus 3Space. The system has the capability to record the motion path followed during an exploration session and this can be recorded and played back to create a movie. Evaluation is through intensive structured interview sessions wherein subjects are required to complete a number of tasks involving three different "toy" environments. None of the metaphors is judged the best in all situations, rather the different metaphors each have advantages and disadvantages depending on the particular task. For example, "scene in hand" is judged to be good for manipulating closed objects, but is not good for moving through an interior; whereas "flying vehicle control" is judged the best for navigating through the interior, but is poor for moving around a closed object.
Improved seam carving for video retargeting	Video, like images, should support content aware resizing. We present video retargeting using an improved seam carving operator. Instead of removing 1D seams from 2D images we remove 2D seam manifolds from 3D space-time volumes. To achieve this we replace the dynamic programming method of seam carving with graph cuts that are suitable for 3D volumes. In the new formulation, a seam is given by a minimal cut in the graph and we show how to construct a graph such that the resulting cut is a valid seam. That is, the cut is monotonic and connected. In addition, we present a novel energy criterion that improves the visual quality of the retargeted images and videos. The original seam carving operator is focused on removing seams with the least amount of energy, ignoring energy that is introduced into the images and video by applying the operator. To counter this, the new criterion is looking forward in time - removing seams that introduce the least amount of energy into the retargeted result. We show how to encode the improved criterion into graph cuts (for images and video) as well as dynamic programming (for images). We apply our technique to images and videos and present results of various applications.
Dynamosaicing: Mosaicing of Dynamic Scenes	This paper explores the manipulation of time in video editing, enabling to control the chronological time of events. These time manipulations include slowing down (or postponing) some dynamic events while speeding up (or advancing) others. When a video camera scans a scene, aligning all the events to a single time interval will result in a panoramic movie. Time manipulations are obtained by first constructing an aligned space-time volume from the input video, and then sweeping a continuous 2D slice (time front) through that volume, generating a new sequence of images. For dynamic scenes, aligning the input video frames poses an important challenge. We propose to align dynamic scenes using a new notion of "dynamics constancy", which is more appropriate for this task than the traditional assumption of "brightness constancy".Another challenge is to avoid visual seams inside moving objects and other visual artifacts resulting from sweeping the space-time volumes with time fronts of arbitrary geometry. To avoid such artifacts, we formulate the problem of finding optimal time front geometry as one of finding a minimal cut in a 4D graph, and solve it using max-flow methods.
Dynamic Graph Cuts for Efficient Inference in Markov Random Fields	Abstract—In this paper we present a fast new fully dynamic algorithm for the st-mincut/max-flow problem. We show how this algorithm can be used to efficiently compute MAP solutions for certain dynamically changing MRF models in computer vision such as image segmentation. Specifically, given the solution of the max-flow problem on a graph, the dynamic algorithm efficiently computes the maximum flow in a modified version of the graph. The time taken by it is roughly proportional to the total amount of change in the edge weights of the graph. Our experiments show that, when the number of changes in the graph is small, the dynamic algorithm is significantly faster than the best known static graph cut algorithm. We test the performance of our algorithm on one particular problem: the object-background segmentation problem for video. It should be noted that the application of our algorithm is not limited to the above problem, the algorithm is generic and can be used to yield similar improvements in many other cases that involve dynamic change.
Interactive video cutout	We present an interactive system for efficiently extracting foreground objects from a video. We extend previous min-cut based image segmentation techniques to the domain of video with four new contributions. We provide a novel painting-based user interface that allows users to easily indicate the foreground object across space and time. We introduce a hierarchical mean-shift preprocess in order to minimize the number of nodes that min-cut must operate on. Within the min-cut we also define new local cost functions to augment the global costs defined in earlier work. Finally, we extend 2D alpha matting methods designed for images to work with 3D video volumes. We demonstrate that our matting approach preserves smoothness across both space and time. Our interactive video cutout system allows users to quickly extract foreground objects from video sequences for use in a variety of applications including compositing onto new backgrounds and NPR cartoon style rendering.
Larrabee: a many-core x86 architecture for visual computing	This paper presents a many-core visual computing architecture code named Larrabee, a new software rendering pipeline, a manycore programming model, and performance analysis for several applications. Larrabee uses multiple in-order x86 CPU cores that are augmented by a wide vector processor unit, as well as some fixed function logic blocks. This provides dramatically higher performance per watt and per unit of area than out-of-order CPUs on highly parallel workloads. It also greatly increases the flexibility and programmability of the architecture as compared to standard GPUs. A coherent on-die 2nd level cache allows efficient inter-processor communication and high-bandwidth local data access by CPU cores. Task scheduling is performed entirely with software in Larrabee, rather than in fixed function logic. The customizable software graphics rendering pipeline for this architecture uses binning in order to reduce required memory bandwidth, minimize lock contention, and increase opportunities for parallelism relative to standard GPUs. The Larrabee native programming model supports a variety of highly parallel applications that use irregular data structures. Performance analysis on those applications demonstrates Larrabee's potential for a broad range of parallel computation.
Brook for GPUs: stream computing on graphics hardware	In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.
Physical simulation for animation and visual effects: parallelization and characterization for chip multiprocessors	We explore the emerging application area of physics-based simulation for computer animation and visual special effects. In particular, we examine its parallelization potential and characterize its behavior on a chip multiprocessor (CMP). Applications in this domain model and simulate natural phenomena, and often direct visual components of motion pictures. We study a set of three workloads that exemplify the span and complexity of physical simulation applications used in a production environment: fluid dynamics, facial animation, and cloth simulation. They are computationally demanding, requiring from a few seconds to several minutes to simulate a single frame; therefore, they can benefit greatly from the acceleration possible with large scale CMPs. Starting with serial versions of these applications, we parallelize code accounting for at least 96% of the serial execution time, targeting a large number of threads.We then study the most expensive modules using a simulated 64-core CMP. For the code representing key modules, we achieve parallel scaling of 45x, 50x, and 30x for fluid, face, and cloth simulations, respectively. The modules have a spectrum of parallel task granularity and locking behavior, and all but one are dominated by loop-level parallelism. Many modules operate on streams of data. In some cases, modules iterate over their data, leading to significant temporal locality. This streaming behavior leads to very high on-die and main memory bandwidth requirements. Finally, most modules have little inter-thread communication since they are data-parallel, but a few require heavy communication between data-parallel operations.
Multi-level ray tracing algorithm	We propose new approaches to ray tracing that greatly reduce the required number of operations while strictly preserving the geometrical correctness of the solution. A hierarchical "beam" structure serves as a proxy for a collection of rays. It is tested against a kd-tree representing the overall scene in order to discard from consideration the sub-set of the kd-tree (and hence the scene) that is guaranteed not to intersect with any possible ray inside the beam. This allows for all the rays inside the beam to start traversing the tree from some node deep inside thus eliminating unnecessary operations. The original beam can be further sub-divided, and we can either continue looking for new optimal entry points for the sub-beams, or we can decompose the beam into individual rays. This is a hierarchical process that can be adapted to the geometrical complexity of a particular view direction allowing for efficient geometric anti-aliasing. By amortizing the cost of partially traversing the tree for all the rays in a beam, up to an order of magnitude performance improvement can be achieved enabling interactivity for complex scenes on ordinary desktop machines.
A Sorting Classification of Parallel Rendering	We describe a classification scheme that we believe provides a more structured framework for reasoning about parallel rendering. The scheme is based on where the sort from object coordinates to screen coordinates occurs, which we believe is fundamental whenever both geometry processing and rasterization are performed in parallel. This classification scheme supports the analysis of computational and communication costs, and encompasses the bulk of current and proposed highly parallel renderers - both hardware and software. We begin by reviewing the standard feed-forward rendering pipeline, showing how different ways of parallelizing it lead to three classes of rendering algorithms. Next, we consider each of these classes in detail, analyzing their aggregate processing and communication costs, possible variations, and constraints they may impose on rendering applications. Finally, we use these analyses to compare the classes and identify when each is likely to be preferable.
Imagine: Media Processing with Streams	The power-efficient imagine stream processor achieves performance densities comparable to those of special-purpose embedded processors. Executing programs mapped to streams and kernels, a single imagine processor is expected to have a peak performance of 20 gflops and sustain 18.3 gops on mpeg-2 encoding.
BSGP: bulk-synchronous GPU programming	We present BSGP, a new programming language for general purpose computation on the GPU. A BSGP program looks much the same as a sequential C program. Programmers only need to supply a bare minimum of extra information to describe parallel processing on GPUs. As a result, BSGP programs are easy to read, write, and maintain. Moreover, the ease of programming does not come at the cost of performance. A well-designed BSGP compiler converts BSGP programs to kernels and combines them using optimally allocated temporary streams. In our benchmark, BSGP programs achieve similar or better performance than well-optimized CUDA programs, while the source code complexity and programming time are significantly reduced. To test BSGP's code efficiency and ease of programming, we implemented a variety of GPU applications, including a highly sophisticated X3D parser that would be extremely difficult to develop with existing GPU programming languages.
Brook for GPUs: stream computing on graphics hardware	In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.
Shader algebra	An algebra consists of a set of objects and a set of operators that act on those objects. We treat shader programs as first-class objects and define two operators: connection and combination. Connection is functional composition: the outputs of one shader are fed into the inputs of another. Combination concatenates the input channels, output channels, and computations of two shaders. Similar operators can be used to manipulate streams and apply computational kernels expressed as shaders to streams. Connecting a shader program to a stream applies that program to all elements of the stream; combining streams concatenates the record definitions of those streams.In conjunction with an optimizing compiler, these operators can manipulate shader programs in many useful ways, including specialization, without modifying the original source code. We demonstrate these operators in Sh, a metaprogramming shading language embedded in C++.
KD-tree acceleration structures for a GPU raytracer	Modern graphics hardware architectures excel at compute-intensive tasks such as ray-triangle intersection, making them attractive target platforms for raytracing. To date, most GPU-based raytracers have relied upon uniform grid acceleration structures. In contrast, the kd-tree has gained widespread use in CPU-based raytracers and is regarded as the best general-purpose acceleration structure. We demonstrate two kd-tree traversal algorithms suitable for GPU implementation and integrate them into a streaming raytracer. We show that for scenes with many objects at different scales, our kd-tree algorithms are up to 8 times faster than a uniform grid. In addition, we identify load balancing and input data recirculation as two fundamental sources of inefficiency when raytracing on current graphics hardware.
Interactive k-d tree GPU raytracing	Over the past few years, the powerful computation rates and high memory bandwidth of GPUs have attracted efforts to run raytracing on GPUs. Our work extends Foley et al.'s GPU k-d tree research. We port their kd-restart algorithm from multi-pass, using CPU load balancing, to single pass, using current GPUs' branching and looping abilities. We introduce three optimizations: a packetized formulation, a technique for restarting partially down the tree instead of at the root, and a small, fixed-size stack that is checked before resorting to restart. Our optimized implementation achieves 15 - 18 million primary rays per second and 16 - 27 million shadow rays per second on our test scenes. Our system also takes advantage of GPUs' strengths at rasterization and shading to offer a mode where rasterization replaces eye ray scene intersection, and primary hits and local shading are produced with standard Direct3D code. For 1024x1024 renderings of our scenes with shadows and Phong shading, we achieve 12-18 frames per second. Finally, we investigate the efficiency of our implementation relative to the computational resources of our GPUs and also compare it against conventional CPUs and the Cell processor, which both have been shown to raytrace well.
Parallel Poisson disk sampling	Sampling is important for a variety of graphics applications include rendering, imaging, and geometry processing. However, producing sample sets with desired efficiency and blue noise statistics has been a major challenge, as existing methods are either sequential with limited speed, or are parallel but only through pre-computed datasets and thus fall short in producing samples with blue noise statistics. We present a Poisson disk sampling algorithm that runs in parallel and produces all samples on the fly with desired blue noise properties. Our main idea is to subdivide the sample domain into grid cells and we draw samples concurrently from multiple cells that are sufficiently far apart so that their samples cannot conflict one another. We present a parallel implementation of our algorithm running on a GPU with constant cost per sample and constant number of computation passes for a target number of samples. Our algorithm also works in arbitrary dimension, and allows adaptive sampling from a user-specified importance field. Furthermore, our algorithm is simple and easy to implement, and runs faster than existing techniques.
A procedural object distribution function	In this article, we present a procedural object distribution function, a new texture basis function that distributes procedurally generated objects over a procedurally generated texture. The objects are distributed uniformly over the texture, and are guaranteed not to overlap. The scale, size, and orientation of the objects can be easily manipulated. The texture basis function is efficient to evaluate, and is suited for real-time applications. The new texturing primitive we present extends the range of textures that can be generated procedurally.The procedural object distribution function we propose is based on Poisson disk tiles and a direct stochastic tiling algorithm for Wang tiles. Poisson disk tiles are square tiles filled with a precomputed set of Poisson disk distributed points, inspired by Wang tiles. A single set of Poisson disk tiles enables the real-time generation of an infinite amount of Poisson disk distributions of arbitrary size. With the direct stochastic tiling algorithm, these Poisson disk distributions can be evaluated locally, at any position in the Euclidean plane.Poisson disk tiles and the direct stochastic tiling algorithm have many other applications in computer graphics. We briefly explore applications in object distribution, primitive distribution for illustration, and environment map sampling.
Sampling with polyominoes	We present a new general-purpose method for fast hierarchical importance sampling with blue-noise properties. Our approach is based on self-similar tiling of the plane or the surface of a sphere with rectifiable polyominoes. Sampling points are associated with polyominoes, one point per polyomino. Each polyomino is recursively subdivided until the desired local density of samples is reached. A numerical code generated during the subdivision process is used for thresholding to accept or reject the sample. The exact position of the sampling point within the polyomino is determined according to a structural index, which indicates the polyomino's local neighborhood. The variety of structural indices and associated sampling point positions are computed during the offline optimization process, and tabulated. Consequently, the sampling itself is extremely fast. The method allows both deterministic and pseudo-non-deterministic sampling. It can be successfully applied in a large variety of graphical applications, where fast sampling with good spectral and visual properties is required. The prime application is rendering.
A spatial data structure for fast Poisson-disk sample generation	Sampling distributions with blue noise characteristics are widely used in computer graphics. Although Poisson-disk distributions are known to have excellent blue noise characteristics, they are generally regarded as too computationally expensive to generate in real time. We present a new method for sampling by dart-throwing in O(N log N) time and introduce a novel and efficient variation for generating Poisson-disk distributions in O(N) time and space.
Adaptive numerical cumulative distribution functions for efficient importance sampling	As image-based surface reflectance and illumination gain wider use in physically-based rendering systems, it is becoming more critical to provide representations that allow sampling light paths according to the distribution of energy in these high-dimensional measured functions. In this paper, we apply algorithms traditionally used for curve approximation to reduce the size of a multidimensional tabulated Cumulative Distribution Function (CDF) by one to three orders of magnitude without compromising its fidelity. These adaptive representations enable new algorithms for sampling environment maps according to the local orientation of the surface and for multiple importance sampling of image-based lighting and measured BRDFs.
Streaming multigrid for gradient-domain operations on large images	We introduce a new tool to solve the large linear systems arising from gradient-domain image processing. Specifically, we develop a streaming multigrid solver, which needs just two sequential passes over out-of-core data. This fast solution is enabled by a combination of three techniques: (1) use of second-order finite elements (rather than traditional finite differences) to reach sufficient accuracy in a single V-cycle, (2) temporally blocked relaxation, and (3) multi-level streaming to pipeline the restriction and prolongation phases into single streaming passes. A key contribution is the extension of the B-spline finite-element method to be compatible with the forward-difference gradient representation commonly used with images. Our streaming solver is also efficient for in-memory images, due to its fast convergence and excellent cache behavior. Remarkably, it can outperform spatially adaptive solvers that exploit application-specific knowledge. We demonstrate seamless stitching and tone-mapping of gigapixel images in about an hour on a notebook PC.
Interactive digital photomontage	We describe an interactive, computer-assisted framework for combining parts of a set of photographs into a single composite picture, a process we call "digital photomontage." Our framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite. Also central to the framework is a suite of interactive tools that allow the user to specify a variety of high-level image objectives, either globally across the image, or locally through a painting-style interface. Image objectives are applied independently at each pixel location and generally involve a function of the pixel values (such as "maximum contrast") drawn from that same location in the set of source images. Typically, a user applies a series of image objectives iteratively in order to create a finished composite. The power of this framework lies in its generality; we show how it can be used for a wide variety of applications, including "selective composites" (for instance, group photos in which everyone looks their best), relighting, extended depth of field, panoramic stitching, clean-plate production, stroboscopic visualization of movement, and time-lapse mosaics.
Efficient gradient-domain compositing using quadtrees	We describe a hierarchical approach to improving the efficiency of gradient-domain compositing, a technique that constructs seamless composites by combining the gradients of images into a vector field that is then integrated to form a composite. While gradient-domain compositing is powerful and widely used, it suffers from poor scalability. Computing an n pixel composite requires solving a linear system with n variables; solving such a large system quickly overwhelms the main memory of a standard computer when performed for multi-megapixel composites, which are common in practice. In this paper we show how to perform gradient-domain compositing approximately by solving an O(p) linear system, where p is the total length of the seams between image regions in the composite; for typical cases, p is O(√n). We achieve this reduction by transforming the problem into a space where much of the solution is smooth, and then utilize the pattern of this smoothness to adaptively subdivide the problem domain using quadtrees. We demonstrate the merits of our approach by performing panoramic stitching and image region copy-and-paste in significantly reduced time and memory while achieving visually identical results.
Removing photography artifacts using gradient projection and flash-exposure sampling	Flash images are known to suffer from several problems: saturation of nearby objects, poor illumination of distant objects, reflections of objects strongly lit by the flash and strong highlights due to the reflection of flash itself by glossy surfaces. We propose to use a flash and no-flash (ambient) image pair to produce better flash images. We present a novel gradient projection scheme based on a gradient coherence model that allows removal of reflections and highlights from flash images. We also present a brightness-ratio based algorithm that allows us to compensate for the falloff in the flash image brightness due to depth. In several practical scenarios, the quality of flash/no-flash images may be limited in terms of dynamic range. In such cases, we advocate using several images taken under different flash intensities and exposures. We analyze the flash intensity-exposure space and propose a method for adaptively sampling this space so as to minimize the number of captured images for any given scene. We present several experimental results that demonstrate the ability of our algorithms to produce improved flash images.
A multigrid solver for boundary value problems using programmable graphics hardware	We present a case study in the application of graphics hardware to general-purpose numeric computing. Specifically, we describe a system, built on programmable graphics hardware, able to solve a variety of partial differential equations with complex boundary conditions. Many areas of graphics, simulation, and computational science require efficient techniques for solving such equations. Our system implements the multigrid method, a fast and popular approach to solving large boundary value problems. We demonstrate the viability of this technique by using it to accelerate three applications: simulation of heat transfer, modeling of fluid mechanics, and tone mapping of high dynamic range images. We analyze the performance of our solver and discuss several issues, including techniques for improving the computational efficiency of iterative grid-based computations for the GPU.
Two-scale tone management for photographic look	We introduce a new approach to tone management for photographs. Whereas traditional tone-mapping operators target a neutral and faithful rendition of the input image, we explore pictorial looks by controlling visual qualities such as the tonal balance and the amount of detail. Our method is based on a two-scale non-linear decomposition of an image. We modify the different layers based on their histograms and introduce a technique that controls the spatial variation of detail. We introduce a Poisson correction that prevents potential gradient reversal and preserves detail. In addition to directly controlling the parameters, the user can transfer the look of a model photograph to the picture being edited.
Using GPUs to improve multigrid solver performance on a cluster	This paper explores the coupling of coarse and fine-grained parallelism for Finite Element (FE) simulations based on efficient parallel multigrid solvers. The focus lies on both system performance and a minimally invasive integration of hardware acceleration into an existing software package, requiring no changes to application code. Because of their excellent price performance ratio, we demonstrate the viability of our approach by using commodity Graphics Processing Units (GPUs), addressing the issue of limited precision on GPUs by applying a mixed precision, iterative refinement technique. Our results show that we do not compromise any software functionality and gain speedups of two and more for large problems.
Multilevel streaming for out-of-core surface reconstruction	Reconstruction of surfaces from huge collections of scanned points often requires out-of-core techniques, and most such techniques involve local computations that are not resilient to data errors. We show that a Poisson-based reconstruction scheme, which considers all points in a global analysis, can be performed efficiently in limited memory using a streaming framework. Specifically, we introduce a multilevel streaming representation, which enables efficient traversal of a sparse octree by concurrently advancing through multiple streams, one per octree level. Remarkably, for our reconstruction application, a sufficiently accurate solution to the global linear system is obtained using a single iteration of cascadic multigrid, which can be evaluated within a single multi-stream pass. We demonstrate scalable performance on several large datasets.
Fast modal sounds with scalable frequency-domain synthesis	Audio rendering of impact sounds, such as those caused by falling objects or explosion debris, adds realism to interactive 3D audiovisual applications, and can be convincingly achieved using modal sound synthesis. Unfortunately, mode-based computations can become prohibitively expensive when many objects, each with many modes, are impacted simultaneously. We introduce a fast sound synthesis approach, based on short-time Fourier Tranforms, that exploits the inherent sparsity of modal sounds in the frequency domain. For our test scenes, this "fast mode summation" can give speedups of 5--8 times compared to a time-domain solution, with slight degradation in quality. We discuss different reconstruction windows, affecting the quality of impact sound "attacks". Our Fourier-domain processing method allows us to introduce a scalable, real-time, audio processing pipeline for both recorded and modal sounds, with auditory masking and sound source clustering. To avoid abrupt computation peaks, such as during the simultaneous impacts of an explosion, we use crossmodal perception results on audiovisual synchrony to effect temporal scheduling. We also conducted a pilot perceptual user evaluation of our method. Our implementation results show that we can treat complex audiovisual scenes in real time with high quality.
A practical model for subsurface light transport	This paper introduces a simple model for subsurface light transport in translucent materials. The model enables efficient simulation of effects that BRDF models cannot capture, such as color bleeding within materials and diffusion of light across shadow boundaries. The technique is efficient even for anisotropic, highly scattering media that are expensive to simulate using existing methods. The model combines an exact solution for single scattering with a dipole point source diffusion approximation for multiple scattering. We also have designed a new, rapid image-based measurement technique for determining the optical properties of translucent materials. We validate the model by comparing predicted and measured values and show how the technique can be used to recover the optical properties of a variety of materials, including milk, marble, and skin. Finally, we describe sampling techniques that allow the model to be used within a conventional ray tracer.
Scanning physical interaction behavior of 3D objects	We describe a system for constructing computer models of several aspects of physical interaction behavior, by scanning the response of real objects. The behaviors we can successfully scan and model include deformation response, contact textures for interaction with force-feedback, and contact sounds. The system we describe uses a highly automated robotic facility that can scan behavior models of whole objects. We provide a comprehensive view of the modeling process, including selection of model structure, measurement, estimation, and rendering at interactive rates. The results are demonstrated with two examples: a soft stuffed toy which has significant deformation behavior, and a hard clay pot which has significant contact textures and sounds. The results described here make it possible to quickly construct physical interaction models of objects for applications in games, animation, and e-commerce.
Interactive simulation of complex audiovisual scenes	We demonstrate a method for efficiently rendering the audio generated by graphical scenes with a large number of sounding objects. This is achieved by using modal synthesis for rigid bodies and rendering only those modes that we judge to be audible to a user observing the scene. We show how excitations of modes can be estimated and inaudible modes eliminated based on the masking characteristics of the human ear. We describe a novel technique for generating contact events by performing closed-form particle simulation and collision detection with the aid of programmable graphics hardware The effectiveness of our system is shown in the context of suitably complex simulations.
Synthesizing sounds from rigid-body simulations	This paper describes a real-time technique for generating realistic and compelling sounds that correspond to the motions of rigid objects. By numerically precomputing the shape and frequencies of an object's deformation modes, audio can be synthesized interactively directly from the force data generated by a standard rigid-body simulation. Using sparse-matrix eigen-decomposition methods, the deformation modes can be computed efficiently even for large meshes. This approach allows us to accurately model the sounds generated by arbitrarily shaped objects based only on a geometric description of the objects and a handful of material parameters. We validate our method by comparing results from a simulated set of wind chimes to audio measurements taken from a real set.
Backward steps in rigid body simulation	Physically based simulation of rigid body dynamics is commonly done by time-stepping systems forward in time. In this paper, we propose methods to allow time-stepping rigid body systems back-ward in time. Unfortunately, reverse-time integration of rigid bodies involving frictional contact is mathematically ill-posed, and can lack unique solutions. We instead propose time-reversed rigid body integrators that can sample possible solutions when unique ones do not exist. We also discuss challenges related to dissipation-related energy gain, sensitivity to initial conditions, stacking, constraints and articulation, rolling, sliding, skidding, bouncing, high angular velocities, rapid velocity growth from micro-collisions, and other problems encountered when going against the usual flow of time.
Keyframe control of complex particle systems using the adjoint method	Control of physical simulation has become a popular topic in the field of computer graphics. Keyframe control has been applied to simulations of rigid bodies, smoke, liquid, flocks, and finite element-based elastic bodies. In this paper, we create a framework for controlling systems of interacting particles -- paying special attention to simulations of cloth and flocking behavior. We introduce a novel integrator-swapping approximation in order to apply the adjoint method to linearized implicit schemes appropriate for cloth simulation. This allows the control of cloth while avoiding computationally infeasible derivative calculations. Meanwhile, flocking control using the adjoint method is significantly more efficient than currently-used methods for constraining group behaviors, allowing the controlled simulation of greater numbers of agents in fewer optimization iterations.
Many-worlds browsing for control of multibody dynamics	Animation techniques for controlling passive simulation are commonly based on an optimization paradigm: the user provides goals a priori, and sophisticated numerical methods minimize a cost function that represents these goals. Unfortunately, for multibody systems with discontinuous contact events these optimization problems can be highly nontrivial to solve, and many-hour offline optimizations, unintuitive parameters, and convergence failures can frustrate end-users and limit usage. On the other hand, users are quite adaptable, and systems which provide interactive feedback via an intuitive interface can leverage the user's own abilities to quickly produce interesting animations. However, the online computation necessary for interactivity limits scene complexity in practice. We introduce Many-Worlds Browsing, a method which circumvents these limits by exploiting the speed of multibody simulators to compute numerous example simulations in parallel (offline and online), and allow the user to browse and modify them interactively. We demonstrate intuitive interfaces through which the user can select among the examples and interactively adjust those parts of the scene that do not match his requirements. We show that using a combination of our techniques, unusual and interesting results can be generated for moderately sized scenes with under an hour of user time. Scalability is demonstrated by sampling much larger scenes using modest offline computations.
Animating oscillatory motion with overlap: wiggly splines	Oscillatory motion is ubiquitous in computer graphics, yet existing animation techniques are ill-suited to its authoring. We introduce a new type of spline for this purpose, known as a "Wiggly Spline." The spline generalizes traditional piecewise cubics when its resonance and damping are set to zero, but creates oscillatory animation when its resonance and damping are changed. The spline provides a combination of direct manipulation and physical realism. To create overlapped and propagating motion, we generate phase shifts of the Wiggly Spline, and use these to control appropriate degrees of freedom in a model. The phase shifts can be created directly by procedural techniques or through a paint-like interface. A further option is to derive the phase shifts statistically by analyzing a time-series of a simulation. In this case, the Wiggly Spline makes it possible to canonicalize a simulation, generalize it by providing frequency and damping controls and control it through direct manipulation.
The cartoon animation filter	We present the "Cartoon Animation Filter", a simple filter that takes an arbitrary input motion signal and modulates it in such a way that the output motion is more "alive" or "animated". The filter adds a smoothed, inverted, and (sometimes) time shifted version of the second derivative (the acceleration) of the signal back into the original signal. Almost all parameters of the filter are automated. The user only needs to set the desired strength of the filter. The beauty of the animation filter lies in its simplicity and generality. We apply the filter to motions ranging from hand drawn trajectories, to simple animations within PowerPoint presentations, to motion captured DOF curves, to video segmentation results. Experimental results show that the filtered motion exhibits anticipation, follow-through, exaggeration and squash-and-stretch effects which are not present in the original input motion data.
DyRT: dynamic response textures for real time deformation simulation with graphics hardware	In this paper we describe how to simulate geometrically complex, interactive, physically-based, volumetric, dynamic deformation models with negligible main CPU costs. This is achieved using a Dynamic Response Texture, or DyRT, that can be mapped onto any conventional animation as an optional rendering stage using commodity graphics hardware. The DyRT simulation process employs precomputed modal vibration models excited by rigid body motions. We present several examples, with an emphasis on bone-based character animation for interactive applications.
Invertible finite elements for robust simulation of large deformation	We present an algorithm for the finite element simulation of elastoplastic solids which is capable of robustly and efficiently handling arbitrarily large deformation. In fact, our model remains valid even when large parts of the mesh are inverted. The algorithm is straightforward to implement and can be used with any material constitutive model, and for both volumetric solids and thin shells such as cloth. We also provide a mechanism for controlling plastic deformation, which allows a deformable object to be guided towards a desired final shape without sacrificing realistic behavior. Finally, we present an improved method for rigid body collision handling in the context of mixed explicit/implicit time-stepping.
Efficient synthesis of physically valid human motion	Optimization is a promising way to generate new animations from a minimal amount of input data. Physically based optimization techniques, however, are difficult to scale to complex animated characters, in part because evaluating and differentiating physical quantities becomes prohibitively slow. Traditional approaches often require optimizing or constraining parameters involving joint torques; obtaining first derivatives for these parameters is generally an O(D2) process, where D is the number of degrees of freedom of the character. In this paper, we describe a set of objective functions and constraints that lead to linear time analytical first derivatives. The surprising finding is that this set includes constraints on physical validity, such as ground contact constraints. Considering only constraints and objective functions that lead to linear time first derivatives results in fast per-iteration computation times and an optimization problem that appears to scale well to more complex characters. We show that qualities such as squash-and-stretch that are expected from physically based optimization result from our approach. Our animation system is particularly useful for synthesizing highly dynamic motions, and we show examples of swinging and leaping motions for characters having from 7 to 22 degrees of freedom.
Hair photobooth: geometric and photometric acquisition of real hairstyles	We accurately capture the shape and appearance of a person's hairstyle. We use triangulation and a sweep with planes of light for the geometry. Multiple projectors and cameras address the challenges raised by the reflectance and intricate geometry of hair. We introduce the use of structure tensors to infer the hidden geometry between the hair surface and the scalp. Our triangulation approach affords substantial accuracy improvement and we are able to measure elaborate hair geometry including complex curls and concavities. To reproduce the hair appearance, we capture a six-dimensional reflectance field. We introduce a new reflectance interpolation technique that leverages an analytical reflectance model to alleviate cross-fading artifacts caused by linear methods. Our results closely match the real hairstyles and can be used for animation.
Capture of hair geometry from multiple images	Hair is a major feature of digital characters. Unfortunately, it has a complex geometry which challenges standard modeling tools. Some dedicated techniques exist, but creating a realistic hairstyle still takes hours. Complementary to user-driven methods, we here propose an image-based approach to capture the geometry of hair.The novelty of this work is that we draw information from the scattering properties of the hair that are normally considered a hindrance. To do so, we analyze image sequences from a fixed camera with a moving light source. We first introduce a novel method to compute the image orientation of the hairs from their anisotropic behavior. This method is proven to subsume and extend existing work while improving accuracy. This image orientation is then raised into a 3D orientation by analyzing the light reflected by the hair fibers. This part relies on minimal assumptions that have been proven correct in previous work.Finally, we show how to use several such image sequences to reconstruct the complete hair geometry of a real person. Results are shown to illustrate the fidelity of the captured geometry to the original hair. This technique paves the way for a new approach to digital hair generation.
Volumetric reconstruction and interactive rendering of trees from photographs	Reconstructing and rendering trees is a challenging problem due to the geometric complexity involved, and the inherent difficulties of capture. In this paper we propose a volumetric approach to capture and render trees with relatively sparse foliage. Photographs of such trees typically have single pixels containing the blended projection of numerous leaves/branches and background. We show how we estimate opacity values on a recursive grid, based on alphamattes extracted from a small number of calibrated photographs of a tree. This data structure is then used to render billboards attached to the centers of the grid cells. Each billboard is assigned a set of view-dependent textures corresponding to each input view. These textures are generated by approximating coverage masks based on opacity and depth from the camera. Rendering is performed using a view-dependent texturing algorithm. The resulting volumetric tree structure has low polygon count, permitting interactive rendering of realistic 3D trees. We illustrate the implementation of our system on several different real trees, and show that we can insert the resulting model in virtual scenes.
Light scattering from human hair fibers	Light scattering from hair is normally simulated in computer graphics using Kajiya and Kay's classic phenomenological model. We have made new measurements of scattering from individual hair fibers that exhibit visually significant effects not predicted by Kajiya and Kay's model. Our measurements go beyond previous hair measurements by examining out-of-plane scattering, and together with this previous work they show a multiple specular highlight and variation in scattering with rotation about the fiber axis. We explain the sources of these effects using a model of a hair fiber as a transparent elliptical cylinder with an absorbing interior and a surface covered with tilted scales. Based on an analytical scattering function for a circular cylinder, we propose a practical shading model for hair that qualitatively matches the scattering behavior shown in the measurements. In a comparison between a photograph and rendered images, we demonstrate the new model's ability to match the appearance of real hair.
Confocal stereo	We present confocal stereo, a new method for computing 3D shape by controlling the focus and aperture of a lens. The method is specifically designed for reconstructing scenes with high geometric complexity or fine-scale texture. To achieve this, we introduce the confocal constancy property, which states that as the lens aperture varies, the pixel intensity of a visible in-focus scene point will vary in a scene-independent way, that can be predicted by prior radiometric lens calibration. The only requirement is that incoming radiance within the cone subtended by the largest aperture is nearly constant. First, we develop a detailed lens model that factors out the distortions in high resolution SLR cameras (12MP or more) with large-aperture lenses (e.g., f1.2). This allows us to assemble an A × F aperture-focus image (AFI) for each pixel, that collects the undistorted measurements over all A apertures and F focus settings. In the AFI representation, confocal constancy reduces to color comparisons within regions of the AFI, and leads to focus metrics that can be evaluated separately for each pixel. We propose two such metrics and present initial reconstruction results for complex scenes.
Decoupling BRDFs from surface mesostructures	We present a technique for the easy acquisition of realistic materials and mesostructures, without acquiring the actual BRDF. The method uses the observation that under certain circumstances the mesostructure of a surface can be acquired independently of the underlying BRDF.The acquired data can be used directly for rendering with little preprocessing. Rendering is possible using an offline renderer but also using graphics hardware, where it achieves real-time frame rates. Compelling results are achieved for a wide variety of materials.
The digital Michelangelo project: 3D scanning of large statues	We describe a hardware and software system for digitizing the shape and color of large fragile objects under non-laboratory conditions. Our system employs laser triangulation rangefinders, laser time-of-flight rangefinders, digital still cameras, and a suite of software for acquiring, aligning, merging, and viewing scanned data. As a demonstration of this system, we digitized 10 statues by Michelangelo, including the well-known figure of David, two building interiors, and all 1,163 extant fragments of the Forma Urbis Romae, a giant marble map of ancient Rome. Our largest single dataset is of the David - 2 billion polygons and 7,000 color images. In this paper, we discuss the challenges we faced in building this system, the solutions we employed, and the lessons we learned. We focus in particular on the unusual design of our laser triangulation scanner and on the algorithms and software we developed for handling very large scanned models.
A Survey on Hair Modeling: Styling, Simulation, and Rendering	Realistic hair modeling is a fundamental part of creating virtual humans in computer graphics. This paper surveys the state of the art in the major topics of hair modeling: hairstyling, hair simulation, and hair rendering. Because of the difficult, often unsolved problems that arise in all these areas, a broad diversity of approaches are used, each with strengths that make it appropriate for particular applications. We discuss each of these major topics in turn, presenting the unique challenges facing each area and describing solutions that have been presented over the years to handle these complex issues. Finally, we outline some of the remaining computational challenges in hair modeling.
A meshless hierarchical representation for light transport	We introduce a meshless hierarchical representation for solving light transport problems. Precomputed radiance transfer (PRT) and finite elements require a discrete representation of illumination over the scene. Non-hierarchical approaches such as per-vertex values are simple to implement, but lead to long precomputation. Hierarchical bases like wavelets lead to dramatic acceleration, but in their basic form they work well only on flat or smooth surfaces. We introduce a hierarchical function basis induced by scattered data approximation. It is decoupled from the geometric representation, allowing the hierarchical representation of illumination on complex objects. We present simple data structures and algorithms for constructing and evaluating the basis functions. Due to its hierarchical nature, our representation adapts to the complexity of the illumination, and can be queried at different scales. We demonstrate the power of the new basis in a novel precomputed direct-to-indirect light transport algorithm that greatly increases the complexity of scenes that can be handled by PRT approaches.
Direct-to-indirect transfer for cinematic relighting	This paper presents an interactive GPU-based system for cinematic relighting with multiple-bounce indirect illumination from a fixed view-point. We use a deep frame-buffer containing a set of view samples, whose indirect illumination is recomputed from the direct illumination on a large set of gather samples, distributed around the scene. This direct-to-indirect transfer is a linear transform which is particularly large, given the size of the view and gather sets. This makes it hard to precompute, store and multiply with. We address this problem by representing the transform as a set of sparse matrices encoded in wavelet space. A hierarchical construction is used to impose a wavelet basis on the unstructured gather cloud, and an image-based approach is used to map the sparse matrix computations to the GPU. We precompute the transfer matrices using a hierarchical algorithm and a variation of photon mapping in less than three hours on one processor. We achieve high-quality indirect illumination at 10-20 frames per second for complex scenes with over 2 million polygons, with diffuse and glossy materials, and arbitrary direct lighting models (expressed using shaders). We compute per-pixel indirect illumination without the need of irradiance caching or other subsampling techniques.
Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments	We present a new, real-time method for rendering diffuse and glossy objects in low-frequency lighting environments that captures soft shadows, interreflections, and caustics. As a preprocess, a novel global transport simulator creates functions over the object's surface representing transfer of arbitrary, low-frequency incident lighting into transferred radiance which includes global effects like shadows and interreflections from the object onto itself. At run-time, these transfer functions are applied to actual incident lighting. Dynamic, local lighting is handled by sampling it close to the object every frame; the object can also be rigidly rotated with respect to the lighting and vice versa. Lighting and transfer functions are represented using low-order spherical harmonics. This avoids aliasing and evaluates efficiently on graphics hardware by reducing the shading integral to a dot product of 9 to 25 element vectors for diffuse receivers. Glossy objects are handled using matrices rather than vectors. We further introduce functions for radiance transfer from a dynamic lighting environment through a preprocessed object to neighboring points in space. These allow soft shadows and caustics from rigidly moving objects to be cast onto arbitrary, dynamic receivers. We demonstrate real-time global lighting effects with this approach.
A framework for the analysis of error in global illumination algorithms	In this paper we identify sources of error in global illumination algorithms and derive bounds for each distinct category. Errors arise from three sources: inaccuracies in the boundary data, discretization, and computation. Boundary data consists of surface geometry, reflectance functions, and emission functions, all of which may be perturbed by errors in measurement or simulation, or by simplifications made for computational efficiency. Discretization error is introduced by replacing the continuous radiative transfer equation with a finite-dimensional linear system, usually by means of boundary elements and a corresponding projection method. Finally, computational errors perturb the finite-dimensional linear system through imprecise form factors, inner products, visibility, etc., as well as by halting iterative solvers after a finite number of steps. Using the error taxonomy introduced in the paper we examine existing global illumination algorithms and suggest new avenues of research.
A rapid hierarchical radiosity algorithm	This paper presents a rapid hierarchical radiosity algorithm for illuminating scenes containing large polygonal patches. The algorithm constructs a hierarchical representation of the form factor matrix by adaptively subdividing patches into subpatches according to a user-supplied error bound. The algorithm guarantees that all form factors are calculated to the same precision, removing many common image artifacts due to inaccurate form factors. More importantly, the algorithm decomposes the form factor matrix into at most O(n) blocks (where n is the number of elements). Previous radiosity algorithms represented the element-to-element transport interactions with n2 form factors. Visibility algorithms are given that work well with this approach. Standard techniques for shooting and gathering can be used with the hierarchical representation to solve for equilibrium radiosities, but we also discuss using a brightness-weighted error criteria, in conjunction with multigridding, to even more rapidly progressively refine the image.
Radiance cache splatting: a GPU-friendly global illumination algorithm	Fast global illumination computation is a challenge in several fields such as lighting simulation and computergenerated visual effects for movies. To this end, the irradiance caching algorithm is commonly used since it provides high-quality rendering in a reasonable time. However this algorithm relies on a spatial data structure in which nearest-neighbors queries and data insertions are performed alternately within a single rendering step. Due to this central and permanently modified data structure, the irradiance caching algorithm cannot be easily implemented on graphics hardware. This paper proposes a novel approach to global illumination using irradiance and radiance cache: the radiance cache splatting. This method directly meets the processing constraints of graphics hardware since it avoids the need of complex data structure and algorithms. Moreover, the rendering quality remains identical to classical irradiance and radiance caching. Our renderer shows an implementation of our algorithm which provides a significant speedup compared to classical irradiance caching.
All-frequency precomputed radiance transfer using spherical radial basis functions and clustered tensor approximation	This paper introduces a new data representation and compression technique for precomputed radiance transfer (PRT). The light transfer functions and light sources are modeled with spherical radial basis functions (SRBFs). A SRBF is a rotation-invariant function that depends on the geodesic distance between two points on the unit sphere. Rotating functions in SRBF representation is as straightforward as rotating the centers of SRBFs. Moreover, high-frequency signals are handled by adjusting the bandwidth parameters of SRBFs. To exploit inter-vertex coherence, the light transfer functions are further classified iteratively into disjoint clusters, and tensor approximation is applied within each cluster. Compared with previous methods, the proposed approach enables real-time rendering with comparable quality under high-frequency lighting environments. The data storage is also more compact than previous all-frequency PRT algorithms.
All-frequency shadows using non-linear wavelet lighting approximation	We present a method, based on pre-computed light transport, for real-time rendering of objects under all-frequency, time-varying illumination represented as a high-resolution environment map. Current techniques are limited to small area lights, with sharp shadows, or large low-frequency lights, with very soft shadows. Our main contribution is to approximate the environment map in a wavelet basis, keeping only the largest terms (this is known as a non-linear approximation). We obtain further compression by encoding the light transport matrix sparsely but accurately in the same basis. Rendering is performed by multiplying a sparse light vector by a sparse transport matrix, which is very fast. For accurate rendering, using non-linear wavelets is an order of magnitude faster than using linear spherical harmonics, the current best technique.
Precomputed local radiance transfer for real-time lighting design	This paper introduces a new method for real-time relighting of scenes illuminated by local light sources. We extend previous work on precomputed radiance transfer for distant lighting to local lighting by introducing the concept of unstructured light clouds. The unstructured light cloud enables a compact representation of local lights in the model and real-time rendering of complex models with full global illumination due to local light sources. We use simplification of lights, and clustered PCA to obtain a compressed representation. When storing only the indirect component of the illumination, we are able to get high quality with only 8-16 lighting coefficients per vertex. Our results demonstrate real-time rendering of scenes with moving lights, dynamic cameras, glossy materials and global illumination.
Local, deformable precomputed radiance transfer	Precomputed radiance transfer (PRT) captures realistic lighting effects from distant, low-frequency environmental lighting but has been limited to static models or precomputed sequences. We focus on PRT for local effects such as bumps, wrinkles, or other detailed features, but extend it to arbitrarily deformable models. Our approach applies zonal harmonics (ZH) which approximate spherical functions as sums of circularly symmetric Legendre polynomials around different axes. By spatially varying both the axes and coefficients of these basis functions, we can fit to spatially varying transfer signals. Compared to the spherical harmonic (SH) basis, the ZH basis yields a more compact approximation. More important, it can be trivially rotated whereas SH rotation is expensive and unsuited for dense per-vertex or per-pixel evaluation. This property allows, for the first time, PRT to be mapped onto deforming models which re-orient the local coordinate frame. We generate ZH transfer models by fitting to PRT signals simulated on meshes or simple parametric models for thin membranes and wrinkles. We show how shading with ZH transfer can be significantly accelerated by specializing to a given lighting environment. Finally, we demonstrate real-time rendering results with soft shadows, inter-reflections, and subsurface scatter on deforming models.
Wavelet radiance transport for interactive indirect lighting	Global illumination is a complex all-frequency phenomenon including subtle effects caused by indirect lighting. Computing global illumination interactively for dynamic lighting conditions has many potential applications, notably in architecture, motion pictures and computer games. It remains a challenging issue, despite the considerable amount of research work devoted to finding efficient methods. This paper presents a novel method for fast computation of indirect lighting; combined with a separate calculation of direct lighting, we provide interactive global illumination for scenes with diffuse and glossy materials, and arbitrarily distributed point light sources. To achieve this goal, we introduce three new tools: a 4D wavelet basis for concise radiance expression, an efficient hierarchical pre-computation of the Global Transport Operator representing the entire propagation of radiance in the scene in a single operation, and a run-time projection of direct lighting on to our wavelet basis. The resulting technique allows unprecedented freedom in the interactive manipulation of lighting for static scenes.
Point based animation of elastic, plastic and melting objects	We present a method for modeling and animating a wide spectrum of volumetric objects, with material properties anywhere in the range from stiff elastic to highly plastic. Both the volume and the surface representation are point based, which allows arbitrarily large deviations form the original shape. In contrast to previous point based elasticity in computer graphics, our physical model is derived from continuum mechanics, which allows the specification of common material properties such as Young's Modulus and Poisson's Ratio. In each step, we compute the spatial derivatives of the discrete displacement field using a Moving Least Squares (MLS) procedure. From these derivatives we obtain strains, stresses and elastic forces at each simulated point. We demonstrate how to solve the equations of motion based on these forces, with both explicit and implicit integration schemes. In addition, we propose techniques for modeling and animating a point-sampled surface that dynamically adapts to deformations of the underlying volumetric model.
Fast and detailed approximate global illumination by irradiance decomposition	In this paper we present an approximate method for accelerated computation of the final gathering step in a global illumination algorithm. Our method operates by decomposing the radiance field close to surfaces into separate far- and near-field components that can be approximated individually. By computing surface shading using these approximations, instead of directly querying the global illumination solution, we have been able to obtain rendering time speed ups on the order of 10x compared to previous acceleration methods. Our approximation schemes rely mainly on the assumptions that radiance due to distant objects will exhibit low spatial and angular variation, and that the visibility between a surface and nearby surfaces can be reasonably predicted by simple location and orientation-based heuristics. Motivated by these assumptions, our far-field scheme uses scattered-data interpolation with spherical harmonics to represent spatial and angular variation, and our near-field scheme employs an aggressively simple visibility heuristic. For our test scenes, the errors introduced when our assumptions fail do not result in visually objectionable artifacts or easily noticeable deviation from a ground-truth solution. We also discuss how our near-field approximation can be used with standard local illumination algorithms to produce significantly improved images at only negligible additional cost.
Multidimensional lightcuts	Multidimensional lightcuts is a new scalable method for efficiently rendering rich visual effects such as motion blur, participating media, depth of field, and spatial anti-aliasing in complex scenes. It introduces a flexible, general rendering framework that unifies the handling of such effects by discretizing the integrals into large sets of gather and light points and adaptively approximating the sum of all possible gather-light pair interactions.We create an implicit hierarchy, the product graph, over the gather-light pairs to rapidly and accurately approximate the contribution from hundreds of millions of pairs per pixel while only evaluating a tiny fraction (e.g., 200--1,000). We build upon the techniques of the prior Lightcuts method for complex illumination at a point, however, by considering the complete pixel integrals, we achieve much greater efficiency and scalability.Our example results demonstrate efficient handling of volume scattering, camera focus, and motion of lights, cameras, and geometry. For example, enabling high quality motion blur with 256x temporal sampling requires only a 6.7x increase in shading cost in a scene with complex moving geometry, materials, and illumination.
Face swapping: automatically replacing faces in photographs	In this paper, we present a complete system for automatic face replacement in images. Our system uses a large library of face images created automatically by downloading images from the internet, extracting faces using face detection software, and aligning each extracted face to a common coordinate system. This library is constructed off-line, once, and can be efficiently accessed during face replacement. Our replacement algorithm has three main stages. First, given an input image, we detect all faces that are present, align them to the coordinate system used by our face library, and select candidate face images from our face library that are similar to the input face in appearance and pose. Second, we adjust the pose, lighting, and color of the candidate face images to match the appearance of those in the input image, and seamlessly blend in the results. Third, we rank the blended candidate replacements by computing a match distance over the overlap region. Our approach requires no 3D model, is fully automatic, and generates highly plausible results across a wide range of skin tones, lighting conditions, and viewpoints. We show how our approach can be used for a variety of applications including face de-identification and the creation of appealing group photographs from a set of images. We conclude with a user study that validates the high quality of our replacement results, and a discussion on the current limitations of our system.
Lambertian Reflectance and Linear Subspaces	We prove that the set of all Lambertian reflectance functions (the mapping from surface normals to intensities) obtained with arbitrary distant light sources lies close to a 9D linear subspace. This implies that, in general, the set of images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately by a low-dimensional linear subspace, explaining prior empirical results. We also provide a simple analytic characterization of this linear space. We obtain these results by representing lighting using spherical harmonics and describing the effects of Lambertian materials as the analog of a convolution. These results allow us to construct algorithms for object recognition based on linear methods as well as algorithms that use convex optimization to enforce nonnegative lighting functions. We also show a simple way to enforce nonnegative lighting when the images of an object lie near a 4D linear space. We apply these algorithms to perform face recognition by finding the 3D model that best matches a 2D query image.
Learning Gender with Support Faces	Nonlinear Support Vector Machines (SVMs) are investigated for appearance-based gender classification with low-resolution thumbnail faces processed from 1,755 images from the FERET face database. The performance of SVMs (3.4 percent error) is shown to be superior to traditional pattern classifiers (linear, quadratic, Fisher linear discriminant, nearest-neighbor) as well as more modern techniques such as Radial Basis Function (RBF) classifiers and large ensemble-RBF networks. Furthermore, the difference in classification performance with low-resolution thumbnails (21-by-12 pixels) and the corresponding higher resolution images (84-by-48 pixels) was found to be only 1 percent, thus demonstrating robustness and stability with respect to scale and degree of facial detail.
Active Appearance Models	We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.
Interactive digital photomontage	We describe an interactive, computer-assisted framework for combining parts of a set of photographs into a single composite picture, a process we call "digital photomontage." Our framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite. Also central to the framework is a suite of interactive tools that allow the user to specify a variety of high-level image objectives, either globally across the image, or locally through a painting-style interface. Image objectives are applied independently at each pixel location and generally involve a function of the pixel values (such as "maximum contrast") drawn from that same location in the set of source images. Typically, a user applies a series of image objectives iteratively in order to create a finished composite. The power of this framework lies in its generality; we show how it can be used for a wide variety of applications, including "selective composites" (for instance, group photos in which everyone looks their best), relighting, extended depth of field, panoramic stitching, clean-plate production, stroboscopic visualization of movement, and time-lapse mosaics.
Computing geometry-aware handle and tunnel loops in 3D models	Many applications such as topology repair, model editing, surface parameterization, and feature recognition benefit from computing loops on surfaces that wrap around their 'handles' and 'tunnels'. Computing such loops while optimizing their geometric lengths is difficult. On the other hand, computing such loops without considering geometry is easy but may not be very useful. In this paper we strike a balance by computing topologically correct loops that are also geometrically relevant. Our algorithm is a novel application of the concepts from topological persistence introduced recently in computational topology. The usability of the computed loops is demonstrated with some examples in feature identification and topology simplification.
The digital Michelangelo project: 3D scanning of large statues	We describe a hardware and software system for digitizing the shape and color of large fragile objects under non-laboratory conditions. Our system employs laser triangulation rangefinders, laser time-of-flight rangefinders, digital still cameras, and a suite of software for acquiring, aligning, merging, and viewing scanned data. As a demonstration of this system, we digitized 10 statues by Michelangelo, including the well-known figure of David, two building interiors, and all 1,163 extant fragments of the Forma Urbis Romae, a giant marble map of ancient Rome. Our largest single dataset is of the David - 2 billion polygons and 7,000 color images. In this paper, we discuss the challenges we faced in building this system, the solutions we employed, and the lessons we learned. We focus in particular on the unusual design of our laser triangulation scanner and on the algorithms and software we developed for handling very large scanned models.
Robust on-line computation of Reeb graphs: simplicity and speed	Reeb graphs are a fundamental data structure for understanding and representing the topology of shapes. They are used in computer graphics, solid modeling, and visualization for applications ranging from the computation of similarities and finding defects in complex models to the automatic selection of visualization parameters. We introduce an on-line algorithm that reads a stream of elements (vertices, triangles, tetrahedra, etc.) and continuously maintains the Reeb graph of all elements already reed. The algorithm is robust in handling non-manifold meshes and general in its applicability to input models of any dimension. Optionally, we construct a skeleton-like embedding of the Reeb graph, and/or remove topological noise to reduce the output size. For interactive multi-resolution navigation we also build a hierarchical data structure which allows real-time extraction of approximated Reeb graphs containing all topological features above a given error threshold. Our extensive experiments show both high performance and practical linear scalability for meshes ranging from thousands to hundreds of millions of triangles. We apply our algorithm to the largest, most general, triangulated surfaces available to us, including 3D, 4D and 5D simplicial meshes. To demonstrate one important application we use Reeb graphs to find and highlight topological defects in meshes, including some widely believed to be "clean."
Topological noise removal	Meshes obtained from laser scanner data often contain topological noise due to inaccuracies in the scanning and merging process. This topological noise complicates subsequent operations such as remeshing, parameterization and smoothing. We introduce an approach that removes unnecessary nontrivial topology from meshes. Using a local wave front traversal, we discover the local topologies of the mesh and identify features such as small tunnels. We then identify non-separating cuts along which we cut and seal the mesh, reducing the genus and thus the topological complexity of the mesh.
Two-way coupling of fluids to rigid and deformable solids and shells	We propose a novel solid/fluid coupling method that treats the coupled system in a fully implicit manner making it stable for arbitrary time steps, large density ratios, etc. In contrast to previous work in computer graphics, we derive our method using a simple back-of-the-envelope approach which lumps the solid and fluid momenta together, and which we show exactly conserves the momentum of the coupled system. Notably, our method uses the standard Cartesian fluid discretization and does not require (moving) conforming tetrahedral meshes or ALE frameworks. Furthermore, we use a standard Lagrangian framework for the solid, thus supporting arbitrary solid constitutive models, both implicit and explicit time integration, etc. The method is quite general, working for smoke, water, and multiphase fluids as well as both rigid and deformable solids, and both volumes and thin shells. Rigid shells and cloth are handled automatically without special treatment, and we support fully one-sided discretizations without leaking. Our equations are fully symmetric, allowing for the use of fast solvers, which is a natural result of properly conserving momentum. Finally, for simple explicit time integration of rigid bodies, we show that our equations reduce to form similar to previous work via a single block Gaussian elimination operation, but that this approach scales poorly, i.e. as though four spatial dimensions rather than three.
A finite element method for animating large viscoplastic flow	We present an extension to Lagrangian finite element methods to allow for large plastic deformations of solid materials. These behaviors are seen in such everyday materials as shampoo, dough, and clay as well as in fantastic gooey and blobby creatures in special effects scenes. To account for plastic deformation, we explicitly update the linear basis functions defined over the finite elements during each simulation step. When these updates cause the basis functions to become ill-conditioned, we remesh the simulation domain to produce a new high-quality finite-element mesh, taking care to preserve the original boundary. We also introduce an enhanced plasticity model that preserves volume and includes creep and work hardening/softening. We demonstrate our approach with simulations of synthetic objects that squish, dent, and flow. To validate our methods, we compare simulation results to videos of real materials.
Simultaneous coupling of fluids and deformable bodies	This paper presents a method for simulating the two-way interaction between fluids and deformable solids. The fluids are simulated using an incompressible Eulerian formulation where a linear pressure projection on the fluid velocities enforces mass conservation. Similarly, elastic solids are simulated using a semi-implicit integrator implemented as a linear operator applied to the forces acting on the nodes in Lagrangian formulation. The proposed method enforces coupling constraints between the fluid and the elastic systems by combining both the pressure projection and implicit integration steps into one set of simultaneous equations. Because these equations are solved simultaneously the resulting combined system treats closed regions in a physically correct fashion, and has good stability characteristics allowing for relatively large time steps. This general approach is not tied to any particular volume discretization of fluid or solid, and we present results implemented using both regular-grid and tetrahedral simulations.
Melting and flowing	We present a fast and stable system for animating materials that melt, flow, and solidify. Examples of real-world materials that exhibit these phenomena include melting candles, lava flow, the hardening of cement, icicle formation, and limestone deposition. We animate such phenomena by physical simulation of fluids --- in particular the incompressible viscous Navier-Stokes equations with free surfaces, treating solid and nearly-solid materials as very high viscosity fluids. The computational method is a modification of the Marker-and-Cell (MAC) algorithm in order to rapidly simulate fluids with variable and arbitrarily high viscosity. This allows the viscosity of the material to change in space and time according to variation in temperature, water content, or any other spatial variable, allowing different locations in the same continuous material to exhibit states ranging from the absolute rigidity or slight bending of hardened wax to the splashing and sloshing of water. We create detailed polygonal models of the fluid by splatting particles into a volumetric grid and we render these models using ray tracing with sub-surface scattering. We demonstrate the method with examples of several viscous materials including melting wax and sand drip castles.
Point based animation of elastic, plastic and melting objects	We present a method for modeling and animating a wide spectrum of volumetric objects, with material properties anywhere in the range from stiff elastic to highly plastic. Both the volume and the surface representation are point based, which allows arbitrarily large deviations form the original shape. In contrast to previous point based elasticity in computer graphics, our physical model is derived from continuum mechanics, which allows the specification of common material properties such as Young's Modulus and Poisson's Ratio. In each step, we compute the spatial derivatives of the discrete displacement field using a Moving Least Squares (MLS) procedure. From these derivatives we obtain strains, stresses and elastic forces at each simulated point. We demonstrate how to solve the equations of motion based on these forces, with both explicit and implicit integration schemes. In addition, we propose techniques for modeling and animating a point-sampled surface that dynamically adapts to deformations of the underlying volumetric model.
Rigid fluid: animating the interplay between rigid bodies and fluid	We present the Rigid Fluid method, a technique for animating the interplay between rigid bodies and viscous incompressible fluid with free surfaces. We use distributed Lagrange multipliers to ensure two-way coupling that generates realistic motion for both the solid objects and the fluid as they interact with one another. We call our method the rigid fluid method because the simulator treats the rigid objects as if they were made of fluid. The rigidity of such an object is maintained by identifying the region of the velocity field that is inside the object and constraining those velocities to be rigid body motion. The rigid fluid method is straightforward to implement, incurs very little computational overhead, and can be added as a bridge between current fluid simulators and rigid body solvers. Many solid objects of different densities (e.g., wood or lead) can be combined in the same animation.
Hybrid simulation of deformable solids	Although mesh-based methods are efficient for simulating simple hyperelasticity, maintaining and adapting a mesh-based representation is less appealing in more complex scenarios, e.g. collision, plasticity and fracture. Thus, meshless or point-based methods have enjoyed recent popularity due to their added flexibility in dealing with these situations. Our approach begins with an initial mesh that is either conforming (as generated by one's favorite meshing algorithm) or non-conforming (e.g. a BCC background lattice). We then propose a framework for embedding arbitrary sample points into this initial mesh allowing for the straightforward handling of collisions, plasticity and fracture without the need for complex remeshing. A straightforward consequence of this new framework is the ability to naturally handle T-junctions alleviating the requirement for a manifold initial mesh. The arbitrarily added embedded points are endowed with full simulation capability allowing them to collide, interact with each other, and interact with the parent geometry in the fashion of a particle-centric simulation system. We demonstrate how this formulation facilitates tasks such as arbitrary refinement or resampling for collision processing, the handling of multiple and possibly conflicting constraints (e.g. when cloth is nonphysically pinched between two objects), the straightforward treatment of fracture, and sub-element resolution of elasticity and plasticity.
Directable photorealistic liquids	We present a method for the directable animation of photorealistic liquids using the particle level set method to obtain smooth, visually pleasing complex liquid surfaces. We also provide for a degree of control common to particle-only based simulation techniques. A variety of directable liquid primitive variables, including the isosurface value, velocity, and viscosity, can be set throughout the liquid. Interaction of thin liquid sheets with immersed rigid bodies is improved with newly proposed object-liquid boundary conditions. Efficient calculation of large-scale animations is supported via a multiple grid pipelined flow method and a novel moving grid windowing technique. In addition, we propose a few significant algorithmic enhancements to the basic liquid simulation algorithm to provide for the smooth merging of liquid drops, allow for the efficient calculation of high viscosity liquids, and ensure the proper treatment of isolated free liquid pockets surrounded by controlled liquid regions.
Fast viscoelastic behavior with thin features	We introduce a method for efficiently animating a wide range of deformable materials. We combine a high resolution surface mesh with a tetrahedral finite element simulator that makes use of frequent re-meshing. This combination allows for fast and detailed simulations of complex elastic and plastic behavior. We significantly expand the range of physical parameters that can be simulated with a single technique, and the results are free from common artifacts such as volume-loss, smoothing, popping, and the absence of thin features like strands and sheets. Our decision to couple a high resolution surface with low-resolution physics leads to efficient simulation and detailed surface features, and our approach to creating the tetrahedral mesh leads to an order-of-magnitude speedup over previous techniques in the time spent re-meshing. We compute masses, collisions, and surface tension forces on the scale of the fine mesh, which helps avoid visual artifacts due to the differing mesh resolutions. The result is a method that can simulate a large array of different material behaviors with high resolution features in a short amount of time.
A finite element method for animating large viscoplastic flow	We present an extension to Lagrangian finite element methods to allow for large plastic deformations of solid materials. These behaviors are seen in such everyday materials as shampoo, dough, and clay as well as in fantastic gooey and blobby creatures in special effects scenes. To account for plastic deformation, we explicitly update the linear basis functions defined over the finite elements during each simulation step. When these updates cause the basis functions to become ill-conditioned, we remesh the simulation domain to produce a new high-quality finite-element mesh, taking care to preserve the original boundary. We also introduce an enhanced plasticity model that preserves volume and includes creep and work hardening/softening. We demonstrate our approach with simulations of synthetic objects that squish, dent, and flow. To validate our methods, we compare simulation results to videos of real materials.
FastLSM: fast lattice shape matching for robust real-time deformation	We introduce a simple technique that enables robust approximation of volumetric, large-deformation dynamics for real-time or large-scale offline simulations. We propose Lattice Shape Matching, an extension of deformable shape matching to regular lattices with embedded geometry; lattice vertices are smoothed by convolution of rigid shape matching operators on local lattice regions, with the effective mechanical stiffness specified by the amount of smoothing via region width. Since the naïve method can be very slow for stiff models - per-vertex costs scale cubically with region width - we provide a fast summation algorithm, Fast Lattice Shape Matching (FastLSM), that exploits the inherent summation redundancy of shape matching and can provide large-region matching at constant per-vertex cost. With this approach, large lattices can be simulated in linear time. We present several examples and benchmarks of an efficient CPU implementation, including many dozens of soft bodies simulated at real-time rates on a typical desktop machine.
A multiresolution framework for dynamic deformations	We present a novel framework for the dynamic simulation of elastic deformable solids. Our approach combines classical finite element methodology with a multiresolution subdivision framework in order to produce fast, easy to use, and realistic animations. We represent deformations using a hierarchical basis constructed using volumetric subdivision. The subdivision framework provides topological flexibility and the hierarchical basis allows the simulation to add detail where it is needed. Since volumetric parameterization is difficult for complex models, we support the embedding of objects in domains that are easier to parameterize.
Interactive skeleton-driven dynamic deformations	This paper presents a framework for the skeleton-driven animation of elastically deformable characters. A character is embedded in a coarse volumetric control lattice, which provides the structure needed to apply the finite element method. To incorporate skeletal controls, we introduce line constraints along the bones of simple skeletons. The bones are made to coincide with edges of the control lattice, which enables us to apply the constraints efficiently using algebraic methods. To accelerate computation, we associate regions of the volumetric mesh with particular bones and perform locally linearized simulations, which are blended at each time step. We define a hierarchical basis on the control lattice, so for detailed interactions the simulation can adapt the level of detail. We demonstrate the ability to animate complex models using simple skeletons and coarse volumetric meshes in a manner that simulates secondary motions at interactive rates.
Isosurface stuffing: fast tetrahedral meshes with good dihedral angles	The isosurface stuffing algorithm fills an isosurface with a uniformly sized tetrahedral mesh whose dihedral angles are bounded between 10.7° and 164.8°, or (with a change in parameters) between 8.9° and 158.8°. The algorithm is whip fast, numerically robust, and easy to implement because, like Marching Cubes, it generates tetrahedra from a small set of precomputed stencils. A variant of the algorithm creates a mesh with internal grading: on the boundary, where high resolution is generally desired, the elements are fine and uniformly sized, and in the interior they may be coarser and vary in size. This combination of features makes isosurface stuffing a powerful tool for dynamic fluid simulation, large-deformation mechanics, and applications that require interactive remeshing or use objects defined by smooth implicit surfaces. It is the first algorithm that rigorously guarantees the suitability of tetrahedra for finite element methods in domains whose shapes are substantially more challenging than boxes. Our angle bounds are guaranteed by a computer-assisted proof. If the isosurface is a smooth 2-manifold with bounded curvature, and the tetrahedra are sufficiently small, then the boundary of the mesh is guaranteed to be a geometrically and topologically accurate approximation of the isosurface.
Dynamic Free-Form Deformations for Animation Synthesis	Free-form deformations (FFDs) are a popular tool for modeling and keyframe animation. This paper extends the use of FFDs to a dynamic setting. Our goal is to enable normally inanimate graphics objects, such as teapots and tables, to become animated, and learn to move about in a charming, cartoon-like manner. To achieve this goal, we implement a system that can transform a wide class of objects into dynamic characters. Our formulation is based on parameterized hierarchical FFDs augmented with Lagrangian dynamics, and provides an efficient way to animate and control the simulated characters. Objects are assigned mass distributions and elastic deformation properties, which allow them to translate, rotate, and deform according to internal and external forces. In addition, we implement an automated optimization process that searches for suitable control strategies. The primary contributions of the work are threefold. First, we formulate a dynamic generalization of conventional, geometric FFDs. The formulation employs deformation modes which are tailored by the user and are expressed in terms of FFDs. Second, the formulation accommodates a hierarchy of dynamic FFDs that can be used to model local as well as global deformations. Third, the deformation modes can be active, thereby producing locomotion.
Meshless deformations based on shape matching	We present a new approach for simulating deformable objects. The underlying model is geometrically motivated. It handles pointbased objects and does not need connectivity information. The approach does not require any pre-processing, is simple to compute, and provides unconditionally stable dynamic simulations.The main idea of our deformable model is to replace energies by geometric constraints and forces by distances of current positions to goal positions. These goal positions are determined via a generalized shape matching of an undeformed rest state with the current deformed state of the point cloud. Since points are always drawn towards well-defined locations, the overshooting problem of explicit integration schemes is eliminated. The versatility of the approach in terms of object representations that can be handled, the efficiency in terms of memory and computational complexity, and the unconditional stability of the dynamic simulation make the approach particularly interesting for games.
Invertible finite elements for robust simulation of large deformation	We present an algorithm for the finite element simulation of elastoplastic solids which is capable of robustly and efficiently handling arbitrarily large deformation. In fact, our model remains valid even when large parts of the mesh are inverted. The algorithm is straightforward to implement and can be used with any material constitutive model, and for both volumetric solids and thin shells such as cloth. We also provide a mechanism for controlling plastic deformation, which allows a deformable object to be guided towards a desired final shape without sacrificing realistic behavior. Finally, we present an improved method for rigid body collision handling in the context of mixed explicit/implicit time-stepping.
Point based animation of elastic, plastic and melting objects	We present a method for modeling and animating a wide spectrum of volumetric objects, with material properties anywhere in the range from stiff elastic to highly plastic. Both the volume and the surface representation are point based, which allows arbitrarily large deviations form the original shape. In contrast to previous point based elasticity in computer graphics, our physical model is derived from continuum mechanics, which allows the specification of common material properties such as Young's Modulus and Poisson's Ratio. In each step, we compute the spatial derivatives of the discrete displacement field using a Moving Least Squares (MLS) procedure. From these derivatives we obtain strains, stresses and elastic forces at each simulated point. We demonstrate how to solve the equations of motion based on these forces, with both explicit and implicit integration schemes. In addition, we propose techniques for modeling and animating a point-sampled surface that dynamically adapts to deformations of the underlying volumetric model.
Hybrid simulation of deformable solids	Although mesh-based methods are efficient for simulating simple hyperelasticity, maintaining and adapting a mesh-based representation is less appealing in more complex scenarios, e.g. collision, plasticity and fracture. Thus, meshless or point-based methods have enjoyed recent popularity due to their added flexibility in dealing with these situations. Our approach begins with an initial mesh that is either conforming (as generated by one's favorite meshing algorithm) or non-conforming (e.g. a BCC background lattice). We then propose a framework for embedding arbitrary sample points into this initial mesh allowing for the straightforward handling of collisions, plasticity and fracture without the need for complex remeshing. A straightforward consequence of this new framework is the ability to naturally handle T-junctions alleviating the requirement for a manifold initial mesh. The arbitrarily added embedded points are endowed with full simulation capability allowing them to collide, interact with each other, and interact with the parent geometry in the fashion of a particle-centric simulation system. We demonstrate how this formulation facilitates tasks such as arbitrary refinement or resampling for collision processing, the handling of multiple and possibly conflicting constraints (e.g. when cloth is nonphysically pinched between two objects), the straightforward treatment of fracture, and sub-element resolution of elasticity and plasticity.
Volume conserving finite element simulations of deformable models	We propose a numerical method for modeling highly deformable nonlinear incompressible solids that conserves the volume locally near each node in a finite element mesh. Our method works with arbitrary constitutive models, is applicable to both passive and active materials (e.g. muscles), and works with simple tetrahedra without the need for multiple quadrature points or stabilization techniques. Although simple linear tetrahedra typically suffer from locking when modeling incompressible materials, our method enforces incompressibility per node (in a one-ring), and we demonstrate that it is free from locking. We correct errors in volume without introducing oscillations by treating position and velocity in separate implicit solves. Finally, we propose a novel method for treating both object contact and self-contact as linear constraints during the incompressible solve, alleviating issues in enforcing multiple possibly conflicting constraints.
Inverse texture synthesis	The quality and speed of most texture synthesis algorithms depend on a 2D input sample that is small and contains enough texture variations. However, little research exists on how to acquire such sample. For homogeneous patterns this can be achieved via manual cropping, but no adequate solution exists for inhomogeneous or globally varying textures, i.e. patterns that are local but not stationary, such as rusting over an iron statue with appearance conditioned on varying moisture levels. We present inverse texture synthesis to address this issue. Our inverse synthesis runs in the opposite direction with respect to traditional forward synthesis: given a large globally varying texture, our algorithm automatically produces a small texture compaction that best summarizes the original. This small compaction can be used to reconstruct the original texture or to re-synthesize new textures under user-supplied controls. More important, our technique allows real-time synthesis of globally varying textures on a GPU, where the texture memory is usually too small for large textures. We propose an optimization framework for inverse texture synthesis, ensuring that each input region is properly encoded in the output compaction. Our optimization process also automatically computes orientation fields for anisotropic textures containing both low- and high-frequency regions, a situation difficult to handle via existing techniques.
Image analogies	This paper describes a new framework for processing images by example, called “image analogies.” The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a “filtered” version of the other, is presented as “training data”; and an application phase, in which the learned filter is applied to some new target image in order to create an “analogous” filtered result. Image analogies are based on a simple multi-scale autoregression, inspired primarily by recent results in texture synthesis. By choosing different types of source image pairs as input, the framework supports a wide variety of “image filter” effects, including traditional image filters, such as blurring or embossing; improved texture synthesis, in which some textures are synthesized with higher quality than by previous approaches; super-resolution, in which a higher-resolution image is inferred from a low-resolution source; texture transfer, in which images are “texturized” with some arbitrary source texture; artistic filters, in which various drawing and painting styles are synthesized based on scanned real-world examples; and texture-by-numbers, in which realistic scenes, composed of a variety of textures, are created using a simple painting interface.
Appearance-space texture synthesis	The traditional approach in texture synthesis is to compare color neighborhoods with those of an exemplar. We show that quality is greatly improved if pointwise colors are replaced by appearance vectors that incorporate nonlocal information such as feature and radiance-transfer data. We perform dimensionality reduction on these vectors prior to synthesis, to create a new appearance-space exemplar. Unlike a texton space, our appearance space is low-dimensional and Euclidean. Synthesis in this information-rich space lets us reduce runtime neighborhood vectors from 5x5 grids to just 4 locations. Building on this unifying framework, we introduce novel techniques for coherent anisometric synthesis, surface texture synthesis directly in an ordinary atlas, and texture advection. Remarkably, we achieve all these functionalities in real-time, or 3 to 4 orders of magnitude faster than prior work.
Texture synthesis over arbitrary manifold surfaces	Algorithms exist for synthesizing a wide variety of textures over rectangular domains. However, it remains difficult to synthesize general textures over arbitrary manifold surfaces. In this paper, we present a solution to this problem for surfaces defined by dense polygon meshes. Our solution extends Wei and Levoy's texture synthesis method [25] by generalizing their definition of search neighborhoods. For each mesh vertex, we establish a local parameterization surrounding the vertex, use this parameterization to create a small rectangular neighborhood with the vertex at its center, and search a sample texture for similar neighborhoods. Our algorithm requires as input only a sample texture and a target model. Notably, it does not require specification of a global tangent vector field; it computes one as it goes - either randomly or via a relaxation process. Despite this, the synthesized texture contains no discontinuities, exhibits low distortion, and is perceived to be similar to the sample texture. We demonstrate that our solution is robust and is applicable to a wide range of textures.
Capture of hair geometry from multiple images	Hair is a major feature of digital characters. Unfortunately, it has a complex geometry which challenges standard modeling tools. Some dedicated techniques exist, but creating a realistic hairstyle still takes hours. Complementary to user-driven methods, we here propose an image-based approach to capture the geometry of hair.The novelty of this work is that we draw information from the scattering properties of the hair that are normally considered a hindrance. To do so, we analyze image sequences from a fixed camera with a moving light source. We first introduce a novel method to compute the image orientation of the hairs from their anisotropic behavior. This method is proven to subsume and extend existing work while improving accuracy. This image orientation is then raised into a 3D orientation by analyzing the light reflected by the hair fibers. This part relies on minimal assumptions that have been proven correct in previous work.Finally, we show how to use several such image sequences to reconstruct the complete hair geometry of a real person. Results are shown to illustrate the fidelity of the captured geometry to the original hair. This technique paves the way for a new approach to digital hair generation.
Synthesis of progressively-variant textures on arbitrary surfaces	We present an approach for decorating surfaces with progressively-variant textures. Unlike a homogeneous texture, a progressively-variant texture can model local texture variations, including the scale, orientation, color, and shape variations of texture elements. We describe techniques for modeling progressively-variant textures in 2D as well as for synthesizing them over surfaces. For 2D texture modeling, our feature-based warping technique allows the user to control the shape variations of texture elements, making it possible to capture complex texture variations such as those seen in animal coat patterns. In addition, our feature-based blending technique can create a smooth transition between two given homogeneous textures, with progressive changes of both shapes and colors of texture elements. For synthesizing textures over surfaces, the biggest challenge is that the synthesized texture elements tend to break apart as they progressively vary. To address this issue, we propose an algorithm based on texton masks, which mark most prominent texture elements in the 2D texture sample. By leveraging the power of texton masks, our algorithm can maintain the integrity of the synthesized texture elements on the target surface.
Painting with texture	We present an interactive texture painting system that allows the user to author digital images by painting with a palette of input textures. At the core of our system is an interactive texture synthesis algorithm that generates textures with natural-looking boundary effects and alpha information as the user paints. Furthermore, we describe an intuitive layered painting model that allows strokes of texture to be merged, intersected and overlapped while maintaining the appropriate boundaries between texture regions. We demonstrate the utility and expressiveness of our system by painting several images using textures that exhibit a range of different boundary effects.
Self-similarity based texture editing	We present a simple method of interactive texture editing that utilizes self-similarity to replicate intended operations globally over an image. Inspired by the recent successes of hierarchical approaches to texture synthesis, this method also uses multi-scale neighborhoods to assess the similarity of pixels within a texture. However, neighborhood matching is not employed to generate new instances of a texture. We instead locate similar neighborhoods for the purpose of replicating editing operations on the original texture itself, thereby creating a fundamentally new texture. This general approach is applied to texture painting, cloning and warping. These global operations are performed interactively, most often directed with just a single mouse movement.
Textureshop: texture synthesis as a photograph editing tool	We combine existing techniques for shape-from-shading and texture synthesis to create a new tool for texturing objects in photographs. Our approach clusters pixels with similar recovered normals into patches on which texture is synthesized. Distorting the texture based on the recovered normals creates the illusion that the texture adheres to the undulations of the photographed surface. Inconsistencies in the recovered surface are disguised by the graphcut blending of the individually textured patches. Further applications include the generation of detail on manually-shaded painting, extracting and synthesizing a displacement map from a texture swatch, and the embossed transfer of normals from one image to another, which would be difficult to create with current image processing packages.
A texture synthesis method for liquid animations	In this paper we present a method for synthesizing textures on animated liquid surfaces generated by a physically based fluid simulation system. Rather than advecting texture coordinates on the surface, our algorithm synthesizes a new texture for every frame using an optimization procedure which attempts to match the surface texture to an input sample texture. By synthesizing a new texture for every frame, our method is able to overcome the discontinuities and distortions of an advected parameterization. We achieve temporal coherence by initializing the surface texture with color values advected from the surface at the previous frame and including these colors in the energy function used during optimization.
Time-varying surface appearance: acquisition, modeling and rendering	For computer graphics rendering, we generally assume that the appearance of surfaces remains static over time. Yet, there are a number of natural processes that cause surface appearance to vary dramatically, such as burning of wood, wetting and drying of rock and fabric, decay of fruit skins, and corrosion and rusting of steel and copper. In this paper, we take a significant step towards measuring, modeling, and rendering time-varying surface appearance. We describe the acquisition of the first time-varying database of 26 samples, encompassing a variety of natural processes including burning, drying, decay, and corrosion. Our main technical contribution is a Space-Time Appearance Factorization (STAF). This model factors space and time-varying effects. We derive an overall temporal appearance variation characteristic curve of the specific process, as well as space-dependent textures, rates, and offsets. This overall temporal curve controls different spatial locations evolve at the different rates, causing spatial patterns on the surface over time. We show that the model accurately represents a variety of phenomena. Moreover, it enables a number of novel rendering applications, such as transfer of the time-varying effect to a new static surface, control to accelerate time evolution in certain areas, extrapolation beyond the acquired sequence, and texture synthesis of time-varying appearance.
Appearance manifolds for modeling time-variant appearance of materials	We present a visual simulation technique called appearance manifolds for modeling the time-variant surface appearance of a material from data captured at a single instant in time. In modeling time-variant appearance, our method takes advantage of the key observation that concurrent variations in appearance over a surface represent different degrees of weathering. By reorganizing these various appearances in a manner that reveals their relative order with respect to weathering degree, our method infers spatial and temporal appearance properties of the material's weathering process that can be used to convincingly generate its weathered appearance at different points in time. Results with natural non-linear reflectance variations are demonstrated in applications such as visual simulation of weathering on 3D models, increasing and decreasing the weathering of real objects, and material transfer with weathering effects.
Texture optimization for example-based synthesis	We present a novel technique for texture synthesis using optimization. We define a Markov Random Field (MRF)-based similarity metric for measuring the quality of synthesized texture with respect to a given input sample. This allows us to formulate the synthesis problem as minimization of an energy function, which is optimized using an Expectation Maximization (EM)-like algorithm. In contrast to most example-based techniques that do region-growing, ours is a joint optimization approach that progressively refines the entire texture. Additionally, our approach is ideally suited to allow for controllable synthesis of textures. Specifically, we demonstrate controllability by animating image textures using flow fields. We allow for general two-dimensional flow fields that may dynamically change over time. Applications of this technique include dynamic texturing of fluid animations and texture-based flow visualization.
Lapped solid textures: filling a model with anisotropic textures	We present a method for representing solid objects with spatially-varying oriented textures by repeatedly pasting solid texture exemplars. The underlying concept is to extend the 2D texture patch-pasting approach of lapped textures to 3D solids using a tetrahedral mesh and 3D texture patches. The system places texture patches according to the user-defined volumetric tensor fields over the mesh to represent oriented textures. We have also extended the original technique to handle nonhomogeneous textures for creating solid models whose textural patterns change gradually along the depth fields. We identify several texture types considering the amount of anisotropy and spatial variation and provide a tailored user interface for each. With our simple framework, large-scale realistic solid models can be created easily with little memory and computational cost. We demonstrate the effectiveness of our approach with several examples including trees, fruits, and vegetables.
Sketching hairstyles	This paper presents an intuitive sketching interface for interactive hairstyle design, made possible by an efficient numerical updating scheme. The user portrays the global shape of a desired hairstyle through a few 3D style curves which are manipulated by interactively sketching freeform strokes. Our approach is based on a vector field representation which is obtained by solving a sparse linear system with the style curves acting as boundary constraints. The key observation is that the specific sparseness pattern of the linear system enables an efficient incremental numerical updating scheme. This gives rise to a sketching interface that provides interactive visual feedback to the user. Interesting hairstyles can be easily created in minutes.
Hierarchical pattern mapping	We present a multi-scale algorithm for mapping a texture defined by an input image onto an arbitrary surface. It avoids the generation and storage of a new, specific texture. The idea is to progressively cover the surface by texture patches of various sizes and shapes, selected from a single input image. The process starts with large patches. A mapping that minimizes the texture fitting error with already textured neighbouring patches is selected. When this error is above a threshold, the patch is split into smaller ones, and the algorithm recursively looks for good fits at a smaller scale. The process ends when the surface is entirely covered. Our results show that the method correctly handles a wide set of texture patterns, which can be used at different mapping scales. Hierarchical texture mapping only outputs texture coordinates in the original texture for each triangle of the initial mesh. Rendering is therefore easy and memory cost minimal. Moreover the initial geometry is preserved.
Aura 3D Textures	This paper presents a new technique, called aura 3D textures, for generating solid textures based on input examples. Our method is fully automatic and requires no user interactions in the process. Given an input texture sample, our method first creates its aura matrix representations and then generates a solid texture by sampling the aura matrices of the input sample constrained in multiple view directions. Once the solid texture is generated, any given object can be textured by the solid texture. We evaluate the results of our method based on extensive user studies. Based on the evaluation results using human subjects, we conclude that our algorithm can generate faithful results of both stochastic and structural textures with an average successful rate of 76.4 percent. Our experimental results also show that the new method outperforms Wei and Levoy's method and is comparable to that proposed by Jagnow et al. [CHECK END OF SENTENCE].
Texture optimization for example-based synthesis	We present a novel technique for texture synthesis using optimization. We define a Markov Random Field (MRF)-based similarity metric for measuring the quality of synthesized texture with respect to a given input sample. This allows us to formulate the synthesis problem as minimization of an energy function, which is optimized using an Expectation Maximization (EM)-like algorithm. In contrast to most example-based techniques that do region-growing, ours is a joint optimization approach that progressively refines the entire texture. Additionally, our approach is ideally suited to allow for controllable synthesis of textures. Specifically, we demonstrate controllability by animating image textures using flow fields. We allow for general two-dimensional flow fields that may dynamically change over time. Applications of this technique include dynamic texturing of fluid animations and texture-based flow visualization.
Light field transfer: global illumination between real and synthetic objects	We present a novel image-based method for compositing real and synthetic objects in the same scene with a high degree of visual realism. Ours is the first technique to allow global illumination and near-field lighting effects between both real and synthetic objects at interactive rates, without needing a geometric and material model of the real scene. We achieve this by using a light field interface between real and synthetic components---thus, indirect illumination can be simulated using only two 4D light fields, one captured from and one projected onto the real scene. Multiple bounces of interreflections are obtained simply by iterating this approach. The interactivity of our technique enables its use with time-varying scenes, including dynamic objects. This is in sharp contrast to the alternative approach of using 6D or 8D light transport functions of real objects, which are very expensive in terms of acquisition and storage and hence not suitable for real-time applications. In our method, 4D radiance fields are simultaneously captured and projected by using a lens array, video camera, and digital projector. The method supports full global illumination with restricted object placement, and accommodates moderately specular materials. We implement a complete system and show several example scene compositions that demonstrate global illumination effects between dynamic real and synthetic objects. Our implementation requires a single point light source and dark background.
Fast separation of direct and global components of a scene using high frequency illumination	We present fast methods for separating the direct and global illumination components of a scene measured by a camera and illuminated by a light source. In theory, the separation can be done with just two images taken with a high frequency binary illumination pattern and its complement. In practice, a larger number of images are used to overcome the optical and resolution limitations of the camera and the source. The approach does not require the material properties of objects and media in the scene to be known. However, we require that the illumination frequency is high enough to adequately sample the global components received by scene points. We present separation results for scenes that include complex interreflections, subsurface scattering and volumetric scattering. Several variants of the separation approach are also described. When a sinusoidal illumination pattern is used with different phase shifts, the separation can be done using just three images. When the computed images are of lower resolution than the source and the camera, smoothness constraints are used to perform the separation using a single image. Finally, in the case of a static scene that is lit by a simple point source, such as the sun, a moving occluder and a video camera can be used to do the separation. We also show several simple examples of how novel images of a scene can be computed from the separation results.
Symmetric photography: exploiting data-sparseness in reflectance fields	We present a novel technique called symmetric photography to capture real world reflectance fields. The technique models the 8D reflectance field as a transport matrix between the 4D incident light field and the 4D exitant light field. It is a challenging task to acquire this transport matrix due to its large size. Fortunately, the transport matrix is symmetric and often data-sparse. Symmetry enables us to measure the light transport from two sides simultaneously, from the illumination directions and the view directions. Data-sparseness refers to the fact that sub-blocks of the matrix can be well approximated using low-rank representations. We introduce the use of hierarchical tensors as the underlying data structure to capture this data-sparseness, specifically through local rank-1 factorizations of the transport matrix. Besides providing an efficient representation for storage, it enables fast acquisition of the approximated transport matrix and fast rendering of images from the captured matrix. Our prototype acquisition system consists of an array of mirrors and a pair of coaxial projector and camera. We demonstrate the effectiveness of our system with scenes rendered from reflectance fields that were captured by our system. In these renderings we can change the viewpoint as well as relight using arbitrary incident light fields.
Capturing and rendering with incident light fields	This paper presents a process for capturing spatially and directionally varying illumination from a real-world scene and using this lighting to illuminate computer-generated objects. We use two devices for capturing such illumination. In the first we photograph an array of mirrored spheres in high dynamic range to capture the spatially varying illumination. In the second, we obtain higher resolution data by capturing images with an high dynamic range omnidirectional camera as it traverses across a plane. For both methods we apply the light field technique to extrapolate the incident illumination to a volume. We render computer-generated objects as illuminated by this captured illumination using a custom shader within an existing global illumination rendering system. To demonstrate our technique we capture several spatially-varying lighting environments with spotlights, shadows, and dappled lighting and use them to illuminate synthetic scenes. We also show comparisons to real objects under the same illumination.
Perception of complex aggregates	Aggregates of individual objects, such as forests, crowds, and piles of fruit, are a common source of complexity in computer graphics scenes. When viewing an aggregate, observers attend less to individual objects and focus more on overall properties such as numerosity, variety, and arrangement. Paradoxically, rendering and modeling costs increase with aggregate complexity, exactly when observers are attending less to individual objects. In this paper we take some first steps to characterize the limits of visual coding of aggregates to efficiently represent their appearance in scenes. We describe psychophysical experiments that explore the roles played by the geometric and material properties of individual objects in observers' abilities to discriminate different aggregate collections. Based on these experiments we derive metrics to predict when two aggregates have the same appearance, even when composed of different objects. In a follow-up experiment we confirm that these metrics can be used to predict the appearance of a range of realistic aggregates. Finally, as a proof-of-concept we show how these new aggregate perception metrics can be applied to simplify scenes by allowing substitution of geometrically simpler aggregates for more complex ones without changing appearance.
Stochastic simplification of aggregate detail	Many renderers perform poorly on scenes that contain a lot of detailed geometry. The load on the renderer can be alleviated by simplification techniques, which create less expensive representations of geometry that is small on the screen. Current simplification techniques for high-quality surface-based rendering tend to work best with element detail (i.e., detail due to the complexity of individual elements) but not as well with aggregate detail (i.e., detail due to the large number of elements). To address this latter type of detail, we introduce a stochastic technique related to some approaches used for point-based renderers. Scenes are rendered by randomly selecting a subset of the geometric elements and altering those elements statistically to preserve the overall appearance of the scene. The amount of simplification can depend on a number of factors, including screen size, motion blur, and depth of field.
Sketchpad: a man-machine graphical communication system	The Sketchpad system makes it possible for a man and a computer to converse rapidly through the medium of line drawings. Heretofore, most interaction between man and computers has been slowed down by the need to reduce all communication to written statements that can be typed; in the past, we have been writing letters to rather than conferring with our computers. For many types of communication, such as describing the shape of a mechanical part or the connections of an electrical circuit, typed statements can prove cumbersome. The Sketchpad system, by eliminating typed statements (except for legends) in favor of line drawings, opens up a new area of man-machine communication.
Mesh saliency	Research over the last decade has built a solid mathematical foundation for representation and analysis of 3D meshes in graphics and geometric modeling. Much of this work however does not explicitly incorporate models of low-level human visual attention. In this paper we introduce the idea of mesh saliency as a measure of regional importance for graphics meshes. Our notion of saliency is inspired by low-level human visual system cues. We define mesh saliency in a scale-dependent manner using a center-surround operator on Gaussian-weighted mean curvatures. We observe that such a definition of mesh saliency is able to capture what most would classify as visually interesting regions on a mesh. The human-perception-inspired importance measure computed by our mesh saliency operator results in more visually pleasing results in processing and viewing of 3D meshes. compared to using a purely geometric measure of shape. such as curvature. We discuss how mesh saliency can be incorporated in graphics applications such as mesh simplification and viewpoint selection and present examples that show visually appealing results from using mesh saliency.
Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope	In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.
A spatial data structure for fast Poisson-disk sample generation	Sampling distributions with blue noise characteristics are widely used in computer graphics. Although Poisson-disk distributions are known to have excellent blue noise characteristics, they are generally regarded as too computationally expensive to generate in real time. We present a new method for sampling by dart-throwing in O(N log N) time and introduce a novel and efficient variation for generating Poisson-disk distributions in O(N) time and space.
Multidimensional lightcuts	Multidimensional lightcuts is a new scalable method for efficiently rendering rich visual effects such as motion blur, participating media, depth of field, and spatial anti-aliasing in complex scenes. It introduces a flexible, general rendering framework that unifies the handling of such effects by discretizing the integrals into large sets of gather and light points and adaptively approximating the sum of all possible gather-light pair interactions.We create an implicit hierarchy, the product graph, over the gather-light pairs to rapidly and accurately approximate the contribution from hundreds of millions of pairs per pixel while only evaluating a tiny fraction (e.g., 200--1,000). We build upon the techniques of the prior Lightcuts method for complex illumination at a point, however, by considering the complete pixel integrals, we achieve much greater efficiency and scalability.Our example results demonstrate efficient handling of volume scattering, camera focus, and motion of lights, cameras, and geometry. For example, enabling high quality motion blur with 256x temporal sampling requires only a 6.7x increase in shading cost in a scene with complex moving geometry, materials, and illumination.
Discrete elastic rods	We present a discrete treatment of adapted framed curves, parallel transport, and holonomy, thus establishing the language for a discrete geometric model of thin flexible rods with arbitrary cross section and undeformed configuration. Our approach differs from existing simulation techniques in the graphics and mechanics literature both in the kinematic description---we represent the material frame by its angular deviation from the natural Bishop frame---as well as in the dynamical treatment---we treat the centerline as dynamic and the material frame as quasistatic. Additionally, we describe a manifold projection method for coupling rods to rigid-bodies and simultaneously enforcing rod inextensibility. The use of quasistatics and constraints provides an efficient treatment for stiff twisting and stretching modes; at the same time, we retain the dynamic bending of the centerline and accurately reproduce the coupling between bending and twisting modes. We validate the discrete rod model via quantitative buckling, stability, and coupled-mode experiments, and via qualitative knot-tying comparisons.
Efficient simulation of inextensible cloth	Many textiles do not noticeably stretch under their own weight. Unfortunately, for better performance many cloth solvers disregard this fact. We propose a method to obtain very low strain along the warp and weft direction using Constrained Lagrangian Mechanics and a novel fast projection method. The resulting algorithm acts as a velocity filter that easily integrates into existing simulation code.
Simulating complex hair with robust collision handling	We present a new framework for simulating dynamic movements of complex hairstyles. The proposed framework, which treats hair as a collection of wisps, includes new approaches to simulating dynamic wisp movements and handling wisp-body collisions and wisp-wisp interactions. For the simulation of wisps, we introduce a new hair dynamics model, a hybrid of the rigid multi-body, serial chain and mass-spring models, to formulate the simulation system using an implicit integration method. Consequently, the simulator can impose collision/contact constraints systematically, allowing it to handle wisp-body collisions efficiently without the need for backtracking or sub-timestepping. In addition, the simulator handles wisp-wisp collisions based on impulses while taking into account viscous damping and cohesive forces. Experimental results show that the proposed technique can stably simulate hair with intricate geometries while robustly handling wisp-body collisions and wisp-wisp interactions.
Strands and hair: modeling, animation, and rendering	The last six years has seen a renaissance in hair modeling, rendering and animation. This course covers the gamut of hair simulation problems and present working solutions, from recent and novel research ideas to time tested industrial practices that resulted in spectacular imagery.
Oriented strands: dynamics of stiff multi-body system	The simulation of strand like primitives modeled as dynamics of serial branched multi-body chain, albeit a potential reduced coordinate formulation, gives rise to stiff and highly non-linear differential equations. We introduce a recursive, linear time and fully implicit method to solve the stiff dynamical problem arising from such a multi-body system. We augment the merits of the proposed scheme by means of analytical constraints and an elaborate collision response model. We finally discuss a versatile simulation system based on the strand primitive for character dynamics and visual effects. We demonstrate dynamics of ears, braid, long/curly hair and foliage.
A Survey on Hair Modeling: Styling, Simulation, and Rendering	Realistic hair modeling is a fundamental part of creating virtual humans in computer graphics. This paper surveys the state of the art in the major topics of hair modeling: hairstyling, hair simulation, and hair rendering. Because of the difficult, often unsolved problems that arise in all these areas, a broad diversity of approaches are used, each with strengths that make it appropriate for particular applications. We discuss each of these major topics in turn, presenting the unique challenges facing each area and describing solutions that have been presented over the years to handle these complex issues. Finally, we outline some of the remaining computational challenges in hair modeling.
CoRdE: Cosserat rod elements for the dynamic simulation of one-dimensional elastic objects	Simulating one-dimensional elastic objects such as threads, ropes or hair strands is a difficult problem, especially if material torsion is considered. In this paper, we present CoRdE(french 'rope'), a novel deformation model for the dynamic interactive simulation of elastic rods with torsion. We derive continuous energies for a dynamically deforming rod based on the Cosserat theory of elastic rods. We then discretize the rod and compute energies per element by employing finite element methods. Thus, the global dynamic behavior is independent of the discretization. The dynamic evolution of the rod is obtained by numerical integration of the resulting Lagrange equations of motion. We further show how this system of equations can be decoupled and efficiently solved. Since the centerline of the rod is explicitly represented, the deformation model allows for accurate contact and self-contact handling. Thus, we can reproduce many important looping phenomena. Further, a broad variety of different materials can be simulated at interactive rates. Experiments underline the physical plausibility of our deformation model.
Animating developable surfaces using nonconforming elements	We present a new discretization for the physics-based animation of developable surfaces. Constrained to not deform at all in-plane but free to bend out-of-plane, these are an excellent approximation for many materials, including most cloth, paper, and stiffer materials. Unfortunately the conforming (geometrically continuous) discretizations used in graphics break down in this limit. Our nonconforming approach solves this problem, allowing us to simulate surfaces with zero in-plane deformation as a hard constraint. However, it produces discontinuous meshes, so we further couple this with a "ghost" conforming mesh for collision processing and rendering. We also propose a new second order accurate constrained mechanics time integration method that greatly reduces the numerical damping present in the usual first order methods used in graphics, for virtually no extra cost and sometimes significant speed-up.
Efficient simulation of inextensible cloth	Many textiles do not noticeably stretch under their own weight. Unfortunately, for better performance many cloth solvers disregard this fact. We propose a method to obtain very low strain along the warp and weft direction using Constrained Lagrangian Mechanics and a novel fast projection method. The resulting algorithm acts as a velocity filter that easily integrates into existing simulation code.
Volume conserving finite element simulations of deformable models	We propose a numerical method for modeling highly deformable nonlinear incompressible solids that conserves the volume locally near each node in a finite element mesh. Our method works with arbitrary constitutive models, is applicable to both passive and active materials (e.g. muscles), and works with simple tetrahedra without the need for multiple quadrature points or stabilization techniques. Although simple linear tetrahedra typically suffer from locking when modeling incompressible materials, our method enforces incompressibility per node (in a one-ring), and we demonstrate that it is free from locking. We correct errors in volume without introducing oscillations by treating position and velocity in separate implicit solves. Finally, we propose a novel method for treating both object contact and self-contact as linear constraints during the incompressible solve, alleviating issues in enforcing multiple possibly conflicting constraints.
Dynamic range independent image quality assessment	The diversity of display technologies and introduction of high dynamic range imagery introduces the necessity of comparing images of radically different dynamic ranges. Current quality assessment metrics are not suitable for this task, as they assume that both reference and test images have the same dynamic range. Image fidelity measures employed by a majority of current metrics, based on the difference of pixel intensity or contrast values between test and reference images, result in meaningless predictions if this assumption does not hold. We present a novel image quality metric capable of operating on an image pair where both images have arbitrary dynamic ranges. Our metric utilizes a model of the human visual system, and its central idea is a new definition of visible distortion based on the detection and classification of visible changes in the image structure. Our metric is carefully calibrated and its performance is validated through perceptual experiments. We demonstrate possible applications of our metric to the evaluation of direct and inverse tone mapping operators as well as the analysis of the image appearance on displays with various characteristics.
Ldr2Hdr: on-the-fly reverse tone mapping of legacy video and photographs	New generations of display devices promise to provide significantly improved dynamic range over conventional display technology. In the long run, evolving camera technology and file formats will provide high fidelity content for these display devices. In the near term, however, the vast majority of images and video will only be available in low dynamic range formats. In this paper we describe a method for boosting the dynamic range of legacy video and photographs for viewing on high dynamic range displays. Our emphasis is on real-time processing of video streams, such as web streams or the signal from a DVD player. We place particular emphasis on robustness of the method, and its ability to deal with a wide range of content without user adjusted parameters or visible artifacts. The method can be implemented on both graphics hardware and on signal processors that are directly integrated in the HDR displays.
Do HDR displays support LDR content?: a psychophysical evaluation	The development of high dynamic range (HDR) imagery has brought us to the verge of arguably the largest change in image display technologies since the transition from black-and-white to color television. Novel capture and display hardware will soon enable consumers to enjoy the HDR experience in their own homes. The question remains, however, of what to do with existing images and movies, which are intrinsically low dynamic range (LDR). Can this enormous volume of legacy content also be displayed effectively on HDR displays? We have carried out a series of rigorous psychophysical investigations to determine how LDR images are best displayed on a state-of-the-art HDR monitor, and to identify which stages of the HDR imaging pipeline are perceptually most critical. Our main findings are: (1) As expected, HDR displays outperform LDR ones. (2) Surprisingly, HDR images that are tone-mapped for display on standard monitors are often no better than the best single LDR exposure from a bracketed sequence. (3) Most importantly of all, LDR data does not necessarily require sophisticated treatment to produce a compelling HDR experience. Simply boosting the range of an LDR image linearly to fit the HDR display can equal or even surpass the appearance of a true HDR image. Thus the potentially tricky process of inverse tone mapping can be largely circumvented.
Single image dehazing	In this paper we present a new method for estimating the optical transmission in hazy scenes given a single input image. Based on this estimation, the scattered light is eliminated to increase scene visibility and recover haze-free scene contrasts. In this new approach we formulate a refined image formation model that accounts for surface shading in addition to the transmission function. This allows us to resolve ambiguities in the data by searching for a solution in which the resulting shading and transmission functions are locally statistically uncorrelated. A similar principle is used to estimate the color of the haze. Results demonstrate the new method abilities to remove the haze layer as well as provide a reliable transmission estimate which can be used for additional applications such as image refocusing and novel view synthesis.
Image and depth from a conventional camera with a coded aperture	A conventional camera captures blurred versions of scene information away from the plane of focus. Camera systems have been proposed that allow for recording all-focus images, or for extracting depth, but to record both simultaneously has required more extensive hardware and reduced spatial resolution. We propose a simple modification to a conventional camera that allows for the simultaneous recovery of both (a) high resolution image information and (b) depth information adequate for semi-automatic extraction of a layered depth representation of the image. Our modification is to insert a patterned occluder within the aperture of the camera lens, creating a coded aperture. We introduce a criterion for depth discriminability which we use to design the preferred aperture pattern. Using a statistical model of images, we can recover both depth information and an all-focus image from single photographs taken with the modified camera. A layered depth map is then extracted, requiring user-drawn strokes to clarify layer assignments in some cases. The resulting sharp image and layered depth map can be combined for various photographic applications, including automatic scene segmentation, post-exposure refocusing, or re-rendering of the scene from an alternate viewpoint.
Green Coordinates	We introduce Green Coordinates for closed polyhedral cages. The coordinates are motivated by Green's third integral identity and respect both the vertices position and faces orientation of the cage. We show that Green Coordinates lead to space deformations with a shape-preserving property. In particular, in 2D they induce conformal mappings, and extend naturally to quasi-conformal mappings in 3D. In both cases we derive closed-form expressions for the coordinates, yielding a simple and fast algorithm for cage-based space deformation. We compare the performance of Green Coordinates with those of Mean Value Coordinates and Harmonic Coordinates and show that the advantage of the shape-preserving property is not achieved at the expense of speed or simplicity. We also show that the new coordinates extend the mapping in a natural analytic manner to the exterior of the cage, allowing the employment of partial cages.
Spherical barycentric coordinates	We develop spherical barycentric coordinates. Analogous to classical, planar barycentric coordinates that describe the positions of points in a plane with respect to the vertices of a given planar polygon, spherical barycentric coordinates describe the positions of points on a sphere with respect to the vertices of a given spherical polygon. In particular, we introduce spherical mean value coordinates that inherit many good properties of their planar counterparts. Furthermore, we present a construction that gives a simple and intuitive geometric interpretation for classical barycentric coordinates, like Wachspress coordinates, mean value coordinates, and discrete harmonic coordinates. One of the most interesting consequences is the possibility to construct mean value coordinates for arbitrary polygonal meshes. So far, this was only possible for triangular meshes. Furthermore, spherical barycentric coordinates can be used for all applications where only planar barycentric coordinates were available up to now. They include Bézier surfaces, parameterization, free-form deformations, and interpolation of rotations.
PriMo: coupled prisms for intuitive surface modeling	We present a new method for 3D shape modeling that achieves intuitive and robust deformations by emulating physically plausible surface behavior inspired by thin shells and plates. The surface mesh is embedded in a layer of volumetric prisms, which are coupled through non-linear, elastic forces. To deform the mesh, prisms are rigidly transformed to satisfy user constraints while minimizing the elastic energy. The rigidity of the prisms prevents degenerations even under extreme deformations, making the method numerically stable. For the underlying geometric optimization we employ both local and global shape matching techniques. Our modeling framework allows for the specification of various geometrically intuitive parameters that provide control over the physical surface behavior. While computationally more involved than previous methods, our approach significantly improves robustness and simplifies user interaction for large, complex deformations.
Subspace gradient domain mesh deformation	In this paper we present a general framework for performing constrained mesh deformation tasks with gradient domain techniques. We present a gradient domain technique that works well with a wide variety of linear and nonlinear constraints. The constraints we introduce include the nonlinear volume constraint for volume preservation, the nonlinear skeleton constraint for maintaining the rigidity of limb segments of articulated figures, and the projection constraint for easy manipulation of the mesh without having to frequently switch between multiple viewpoints. To handle nonlinear constraints, we cast mesh deformation as a nonlinear energy minimization problem and solve the problem using an iterative algorithm. The main challenges in solving this nonlinear problem are the slow convergence and numerical instability of the iterative solver. To address these issues, we develop a subspace technique that builds a coarse control mesh around the original mesh and projects the deformation energy and constraints onto the control mesh vertices using the mean value interpolation. The energy minimization is then carried out in the subspace formed by the control mesh vertices. Running in this subspace, our energy minimization solver is both fast and stable and it provides interactive responses. We demonstrate our deformation constraints and subspace deformation technique with a variety of constrained deformation examples.
Mesh editing with poisson-based gradient field manipulation	In this paper, we introduce a novel approach to mesh editing with the Poisson equation as the theoretical foundation. The most distinctive feature of this approach is that it modifies the original mesh geometry implicitly through gradient field manipulation. Our approach can produce desirable and pleasing results for both global and local editing operations, such as deformation, object merging, and smoothing. With the help from a few novel interactive tools, these operations can be performed conveniently with a small amount of user interaction. Our technique has three key components, a basic mesh solver based on the Poisson equation, a gradient field manipulation scheme using local transforms, and a generalized boundary condition representation based on local frames. Experimental results indicate that our framework can outperform previous related mesh editing techniques.
A system for high-volume acquisition and matching of fresco fragments: reassembling Theran wall paintings	Although mature technologies exist for acquiring images, geometry, and normals of small objects, they remain cumbersome and time-consuming for non-experts to employ on a large scale. In an archaeological setting, a practical acquisition system for routine use on every artifact and fragment would open new possibilities for archiving, analysis, and dissemination. We present an inexpensive system for acquiring all three types of information, and associated metadata, for small objects such as fragments of wall paintings. The acquisition system requires minimal supervision, so that a single, non-expert user can scan at least 10 fragments per hour. To achieve this performance, we introduce new algorithms to robustly and automatically align range scans, register 2-D scans to 3-D geometry, and compute normals from 2-D scans. As an illustrative application, we present a novel 3-D matching algorithm that efficiently searches for matching fragments using the scanned geometry.
The digital Michelangelo project: 3D scanning of large statues	We describe a hardware and software system for digitizing the shape and color of large fragile objects under non-laboratory conditions. Our system employs laser triangulation rangefinders, laser time-of-flight rangefinders, digital still cameras, and a suite of software for acquiring, aligning, merging, and viewing scanned data. As a demonstration of this system, we digitized 10 statues by Michelangelo, including the well-known figure of David, two building interiors, and all 1,163 extant fragments of the Forma Urbis Romae, a giant marble map of ancient Rome. Our largest single dataset is of the David - 2 billion polygons and 7,000 color images. In this paper, we discuss the challenges we faced in building this system, the solutions we employed, and the lessons we learned. We focus in particular on the unusual design of our laser triangulation scanner and on the algorithms and software we developed for handling very large scanned models.
Distinctive Image Features from Scale-Invariant Keypoints	This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.
Fluorescent immersion range scanning	The quality of a 3D range scan should not depend on the surface properties of the object. Most active range scanning techniques, however, assume a diffuse reflector to allow for a robust detection of incident light patterns. In our approach we embed the object into a fluorescent liquid. By analyzing the light rays that become visible due to fluorescence rather than analyzing their reflections off the surface, we can detect the intersection points between the projected laser sheet and the object surface for a wide range of different materials. For transparent objects we can even directly depict a slice through the object in just one image by matching its refractive index to the one of the embedding liquid. This enables a direct sampling of the object geometry without the need for computational reconstruction. This way, a high-resolution 3D volume can be assembled simply by sweeping a laser plane through the object. We demonstrate the effectiveness of our light sheet range scanning approach on a set of objects manufactured from a variety of materials and material mixes, including dark, translucent and transparent objects.
Tomographic reconstruction of transparent objects	The scanning of 3D geometry has become a popular way of capturing the shape of real-world objects. Transparent objects, however, pose problems for traditional scanning methods. We present a visible light tomographic reconstruction method for recovering the shape of transparent objects, such as glass. Our setup is relatively simple to implement, and accounts for refraction, which can be a significant problem in visible light tomography.
Fast separation of direct and global components of a scene using high frequency illumination	We present fast methods for separating the direct and global illumination components of a scene measured by a camera and illuminated by a light source. In theory, the separation can be done with just two images taken with a high frequency binary illumination pattern and its complement. In practice, a larger number of images are used to overcome the optical and resolution limitations of the camera and the source. The approach does not require the material properties of objects and media in the scene to be known. However, we require that the illumination frequency is high enough to adequately sample the global components received by scene points. We present separation results for scenes that include complex interreflections, subsurface scattering and volumetric scattering. Several variants of the separation approach are also described. When a sinusoidal illumination pattern is used with different phase shifts, the separation can be done using just three images. When the computed images are of lower resolution than the source and the camera, smoothness constraints are used to perform the separation using a single image. Finally, in the case of a static scene that is lit by a simple point source, such as the sun, a moving occluder and a video camera can be used to do the separation. We also show several simple examples of how novel images of a scene can be computed from the separation results.
Interactive visual editing of grammars for procedural architecture	We introduce a real-time interactive visual editing paradigm for shape grammars, allowing the creation of rulebases from scratch without text file editing. In previous work, shape-grammar based procedural techniques were successfully applied to the creation of architectural models. However, those methods are text based, and may therefore be difficult to use for artists with little computer science background. Therefore the goal was to enable a visual work-flow combining the power of shape grammars with traditional modeling techniques. We extend previous shape grammar approaches by providing direct and persistent local control over the generated instances, avoiding the combinatorial explosion of grammar rules for modifications that should not affect all instances. The resulting visual editor is flexible: All elements of a complex state-of-the-art grammar can be created and modified visually.
Persistent realtime building interior generation	A novel approach to generate virtual building interiors in real-time is presented. The interiors are generated in a top-down fashion using architectural guidelines. Although a building interior in its entirety may be quite large, only the portions that are needed immediately are generated. This lazy generation scheme allows the use of only a fraction of the memory that a model of the entire interior would otherwise require. Our method provides real-time frame rates, making it attractive for realtime interactive applications.Memory is controlled by deleting regions of the interior that are no longer needed. That said, any changes made in these regions will not be lost. We provide a simple and efficient method to allow changes made to the interior to persist past the life time of the regions that contain them. This allows a dynamic, consistent environment and increases control over the content by allowing developers to make changes.
Parallel multilevel algorithms for hypergraph partitioning	In this paper, we present parallel multilevel algorithms for the hypergraph partitioning problem. In particular, we describe for parallel coarsening, parallel greedy k-way refinement and parallel multi-phase refinement. Using an asymptotic theoretical performance model, we derive the isoefficiency function for our algorithms and hence show that they are technically scalable when the maximum vertex and hyperedge degrees are small. We conduct experiments on hypergraphs from six different application domains to investigate the empirical scalability of our algorithms both in terms of runtime and partition quality. Our findings confirm that the quality of partition produced by our algorithms is stable as the number of processors is increased while being competitive with those produced by a state-of-the-art serial multilevel partitioning tool. We also validate our theoretical performance model through an isoefficiency study. Finally, we evaluate the impact of introducing parallel multi-phase refinement into our parallel multilevel algorithm in terms of the trade off between improved partition quality and higher runtime cost.
Uniformization and hypergraph partitioning for the distributed computation of response time densities in very large Markov models	Fast response times and the satisfaction of response time quantile targets are important performance criteria for almost all transaction processing and computer-communication systems. We present a distributed uniformization-based technique for obtaining response time densities from very large unstructured Markov models. Our method utilizes hypergraph partitioning to minimize inter-processor communication while maintaining a good load balance. The resulting algorithm scales well on a distributed-memory parallel computer and, unusually for a problem of this nature, also produces near-linear speed-ups on a network of commodity PCs linked by 100 Mbps ethernet. We demonstrate our approach by calculating passage time densities in a 1.6 million state Markov chain derived from a Generalized Stochastic Petri net model and a 10.8 million state Markov chain derived from a closed tree-like queueing network. We compare the accuracy of our results with simulation and known analytical solutions and contrast the run-time performance of our technique with an approach based on numerical Laplace transform inversion.
An Improved Min-Cut Algonthm for Partitioning VLSI Networks	Recently, a fast (linear) heuristic for improving min-cut partitions of VLSI networks was suggested by Fiduccia and Mattheyses [6]. In this-paper we generalize their ideas and suggest a class of increasingly sophisticated heuristics. We then show, by exploiting the data structures originally suggested by them, that the computational complexity of any specific heuristic in the suggested class remains linear in the size of the network.
Partitioning around roadblocks: tackling constraints with intermediate relaxations	Constraint satisfaction during partitioning and placement of VLSI circuits is an important problem, and effective techniques to address it lead to high-quality physical design solutions. This problem has, however, been cursorily treated in previous partitioning and placement research. Our work presented here addresses the balance-ratio constraint, and is a crucial first step to an effective solution to the general constraint-satisfaction problem. In current iterative-improvement mincut partitioners, the balance-ratio constraint is tackled by disallowing moves that violate it. These methods can lead to sub-optimal solutions since the process is biased against the movement of large cells and clusters of cells. We present techniques for an informed relaxation process that attempts to estimate whether relaxing the constraint temporarily will ultimately benefit the mincut objective. If so, then a violating move is allowed, otherwise it is disallowed. The violations are corrected in future moves so that the final solution satisfies the given constraint. On a set of ACM/SIGDA PROUD benchmark circuits with actual cell sizes, we obtained up to 38% and an average of 14.5% better cutsizes with as little as 13% time overhead using our techniques compared to the standard method of not allowing any relaxation.
Parallel hypergraph partitioning for scientific computing	Graph partitioning is often used for load balancing in parallel computing, but it is known that hypergraph partitioning has several advantages. First, hypergraphs more accurately model communication volume, and second, they are more expressive and can better represent nonsymmetric problems. Hypergraph partitioning is particularly suited to parallel sparse matrix-vector multiplication, a common kernel in scientific computing. We present a parallel software package for hypergraph (and sparse matrix) partitioning developed at Sandia National Labs. The algorithm is a variation on multilevel partitioning. Our parallel implementation is novel in that it uses a two-dimensional data distribution among processors. We present empirical results that show our parallel implementation achieves good speedup on several large problems (up to 33 million nonzeros) with up to 64 processors on a Linux cluster.
GALILEO: a strongly-typed, interactive conceptual language	Galileo, a programming language for database applications, is presented. Galileo is a strongly-typed, interactive programming language designed specifically to support semantic data model features (classification, aggregation, and specialization), as well as the abstraction mechanisms of modern programming languages (types, abstract types, and modularization). The main contributions of Galileo are (a) a flexible type system to model database structure and semantic integrity constraints; (b) the inclusion of type hierarchies to support the specialization abstraction mechanisms of semantic data models; (c) a modularization mechanism to structure data and operations into interrelated units (d) the integration of abstraction mechanisms into an expression-based language that allows interactive use of the database without resorting to a new stand-alone query language.Galileo will be used in the immediate future as a tool for database design and, in the long term, as a high-level interface for DBMSs.
Limitations of record-based information models	Record structures are generally efficient, familiar, and easy to use for most current data processing applications. But they are not complete in their ability to represent information, nor are they fully self-describing.
Type hierarchies and Semantic Data Models	The basic abstraction mechanisms of Semantic Data Models - aggregation, classification and generalization - are considered the essential features to overcome the limitations of traditional data models in terms of semantic expressiveness. An important issue in database programming language design is which features should a programming language have to support the abstraction mechanisms of Semantic Data Models. This paper shows that when using a strongly typed programming language, that language should support the notion of type hierarchies to achieve a full integration of Semantic Data Models abstraction mechanisms within the language's type system. The solution is presented using the language Galileo, a strongly typed, interactive programming language specifically designed for database applications.
DIAL: a programming language for data intensive applications	DIAL is a problem-oriented and high-level programming language oriented towards database applications. It integrates into a unified framework database primitives and computational facilities, so that an application programmer will deal with a single coherent language. The design of DIAL is based on the premise that in order to have a meaningful impact on the construction of application software, a database programming language should eschew generality and focus on what is unique about the application domain in question. To that end, DIAL seeks to embody features that naturally express the most common and frequently recurring patterns encountered in database applications programs.A number of its features distinguish DIAL from other contemporary related efforts. Data description plays a primary role in DIAL, in that a substantial amount of application semantics is expressed in the database schema rather than in procedure definitions; to achieve this end, DIAL employs a higher-level data model (the SDM) as its data description mechanism. Facilities for conducting user-system dialogues are also embedded in the language. Specialized control structures are provided to allow for succinct and direct expression of the algorithmic structure of procedures that utilize the database. High-level mechanisms (called controllers) are used to specify an application system's required behavior in the face of multiple concurrent users and aborted transactions.DIAL has been applied to two realistic and substantial systems, a purchasing application and a job-shop scheduler. Continued validation studies of the language are underway, as are an implementation effort and the design of an allied program development system.
Query optimization on local area networks	Local area networks are becoming widely used as the database communication framework for sophisticated information systems. Databases can be distributed among stations on a network to achieve the advantages of performance, reliability, availability, and modularity. Efficient distributed query optimization algorithms are presented here for two types of local area networks: address ring networks and broadcast networks. Optimal algorithms are designed for simple queries. Optimization principles from these algorithms guide the development of effective heuristic algorithms for general queries on both types of networks. Several examples illustrate distributed query processing on local area networks.
Query processing in a system for distributed databases (SDD-1)	This paper describes the techniques used to optimize relational queries in the SDD-1 distributed database system. Queries are submitted to SDD-1 in a high-level procedural language called Datalanguage. Optimization begins by translating each Datalanguage query into a relational calculus form called an envelope, which is essentially an aggregate-free QUEL query. This paper is primarily concerned with the optimization of envelopes.Envelopes are processed in two phases. The first phase executes relational operations at various sites of the distributed database in order to delimit a subset of the database that contains all data relevant to the envelope. This subset is called a reduction of the database. The second phase transmits the reduction to one designated site, and the query is executed locally at that site.The critical optimization problem is to perform the reduction phase efficiently. Success depends on designing a good repertoire of operators to use during this phase, and an effective algorithm for deciding which of these operators to use in processing a given envelope against a given database. The principal reduction operator that we employ is called a semijoin. In this paper we define the semijoin operator, explain why semijoin is an effective reduction operator, and present an algorithm that constructs a cost-effective program of semijoins, given an envelope and a database.
An ant colony optimization algorithm for DNA sequencing by hybridization	The reconstruction of DNA sequences from DNA fragments is one of the most challenging problems in computational biology. In recent years the specific problem of DNA sequencing by hybridization has attracted quite a lot of interest in the optimization community. Several metaheuristics such as tabu search and evolutionary algorithms have been applied to this problem. However, the performance of existing metaheuristics is often inferior to the performance of recently proposed constructive heuristics. On the basis of these new heuristics we develop an ant colony optimization algorithm for DNA sequencing by hybridization. An important feature of this algorithm is the implementation in a so-called multi-level framework. The computational results show that our algorithm is currently a state-of-the-art method for the tackled problem.
Metaheuristics in combinatorial optimization: Overview and conceptual comparison	The field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. This is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. We give a survey of the nowadays most important metaheuristics from a conceptual point of view. We outline the different components and concepts that are used in the different metaheuristics in order to analyze their similarities and differences. Two very important concepts in metaheuristics are intensification and diversification. These are the two forces that largely determine the behavior of a metaheuristic. They are in some way contrary but also complementary to each other. We introduce a framework, that we call the I&D frame, in order to put different intensification and diversification components into relation with each other. Outlining the advantages and disadvantages of different metaheuristic approaches we conclude by pointing out the importance of hybridization of metaheuristics as well as the integration of metaheuristics and other methods for optimization.
The price of anarchy in cooperative network creation games	A fundamental family of problems at the intersection between computer science and operations research is network design. This area of research has become increasingly important given the continued growth of computer networks such as the Internet. Traditionally, we want to find a minimum-cost (sub)network that satisfies some specified property such as k-connectivity or connectivity on terminals (as in the classic Steiner tree problem). This goal captures the (possibly incremental) creation cost of the network, but does not incorporate the cost of actually using the network. In contrast, network routing has the goal of optimizing the usage cost of the network, but assumes that the network has already been created.
A tight bound on approximating arbitrary metrics by tree metrics	In this paper, we show that any n point metric space can be embedded into a distribution over dominating tree metrics such that the expected stretch of any edge is O(log n). This improves upon the result of Bartal who gave a bound of O(log n log log n). Moreover, our result is existentially tight; there exist metric spaces where any tree embedding must have distortion Ω(log n)-distortion. This problem lies at the heart of numerous approximation and online algorithms including ones for group Steiner tree, metric labeling, buy-at-bulk network design and metrical task system. Our result improves the performance guarantees for all of these problems.
Strong price of anarchy	A strong equilibrium (Aumann 1959) is a pure Nash equilibrium which is resilient to deviations by coalitions. We define the strong price of anarchy to be the ratio of the worst case strong equilibrium to the social optimum. In contrast to the traditional price of anarchy, which quantifies the loss incurred due to both selfishness and lack of coordination, the strong price of anarchy isolates the loss originated from selfishness from that obtained due to lack of coordination. We study the strong price of anarchy in two settings, one of job scheduling and the other of network creation. In the job scheduling game we show that for unrelated machines the strong price of anarchy can be bounded as a function of the number of machines and the size of the coalition. For the network creation game we show that the strong price of anarchy is at most 2. In both cases we show that a strong equilibrium always exists, except for a well defined subset of network creation games.
On nash equilibria for a network creation game	We study a network creation game recently proposed by Fabrikant, Luthra, Maneva, Papadimitriou and Shenker. In this game, each player (vertex) can create links (edges) to other players at a cost of α per edge. The goal of every player is to minimize the sum consisting of (a) the cost of the links he has created and (b) the sum of the distances to all other players.Fabrikant et al. conjectured that there exists a constant A such that, for any α A, all non-transient Nash equilibria graphs are trees. They showed that if a Nash equilibrium is a tree, the price of anarchy is constant. In this paper we disprove the tree conjecture. More precisely, we show that for any positive integer n0, there exists a graph built by n ≥ n0 players which contains cycles and forms a non-transient Nash equilibrium, for any α with 1 n/2. Our construction makes use of some interesting results on finite affine planes. On the other hand we show that, for α ≥ 12n[log n], every Nash equilibrium forms a tree.Without relying on the tree conjecture, Fabrikant et al. proved an upper bound on the price of anarchy of O(√α), where α ∈ [2, n2]. We improve this bound. Specifically, we derive a constant upper bound for α ∈ O(√n) and for α ≥ 12n[log n]. For the intermediate values we derive an improved bound of O(1 + (min{α2/n, n2/α})1/3).Additionally, we develop characterizations of Nash equilibria and extend our results to a weighted network creation game as well as to scenarios with cost sharing.
The price of selfish behavior in bilateral network formation	Given a collection of selfish agents who wish to establish links to route traffic among themselves, the set of equilibrium network topologies may appear quite different from the centrally enforced optimum. We study the quality (price of anarchy) of equilibrium networks in a game where links require the consent of both participants and are negotiated bilaterally and compare these networks to those generated by an earlier model due to Fabrikant et al. [6] in which links are formed unilaterally. We provide a characterization of stable and efficient networks in the bilateral network formation game, show that the set of stable networks is richer than those in the unilateral game, and that all stable networks of the unilateral game are also stable in the bilateral game. We also provide an upper and lower bound on the price of anarchy (tight in the size of the network n but not the link cost α) of the bilateral game and show that the worst-case price of anarchy of the bilateral model is worse than for the unilateral model. A careful empirical analysis demonstrates that the average price of anarchy is better in the bilateral connection game than in the unilateral game for small link costs but worse as links become more expensive. In the process, a powerful equivalence between link-based graph stability and two game-theoretic equilibrium notions is also discussed. The equivalence establishes necessary and sufficient conditions for an equilibrium in the bilateral game that helps provide a partial geometric characterization of equilibrium graphs.
The price of anarchy in network creation games	We study Nash equilibria in the setting of network creation games introduced recently by Fabrikant, Luthra, Maneva, Papadimitriou and Shenker. In this game we have a set of selfish node players, each creating some incident links, and the goal is to minimize α times the cost of the created links plus sum of the distances to all other players. Fabrikant et al. proved an upper bound O(√α) on the price of anarchy, i.e., the relative cost of the lack of coordination. Albers, Eilts, Even-Dar, Mansour, and Roditty show that the price of anarchy is constant for α = O(√n) and for α ≥ 12n[lg n], and that the price of anarchy is 15(1+min {α2n, n2α})1/3) for any α. The latter bound shows the first sublinear worst-case bound, O(n1/3), for all α. But no better bound is known for α between ω(√n) and o(n lg n). Yet α ≈ n is perhaps the most interesting range, for it corresponds to considering the average distance (instead ofthe sum of distances) to other nodes to be roughly on par with link creation (effectively dividing α by n). In this paper, we prove the first o(nε) upper bound for general α, namely 2O(√lg n). We also prove aconstant upper bound for α = O(n1-ε) for any fixed ε 0, substantially reducing the range of α for which constant bounds have not been obtained. Along the way, we also improve the constant upper bound by Albers et al. (with the leadconstant of 15 ) to 6 for α n/2)1/2 and to 4 for α n/2)1/3}. Next we consider the bilateral network variant of Corbo and Parkesin which links can be created only with the consent of both end points and the link price is shared equally by the two. Corbo and Parkes show an upper bound of O(√α) and a lower bound of Ω(lg α) for α ≤ n. In this paper, we show that in fact the upper bound O(√α) is tight for α ≤, by proving a matching lower bound of Ω(√α). For α n, we prove that the price of anarchy is Θ(n/√ α). Finally we introduce a variant of both network creation games, in which each player desires to minimize α times the cost of its created links plus the maximum distance (instead of the sum of distances) to the other players. This variant of the problem is naturally motivated by considering the worst case instead of the average case. Interestingly, for the original (unilateral) game, we show that the price of anarchy is at most 2 for α ≥ n, O(min{4√lg n, (n/α)1/3}) for 2√lgn ≤ α ≤ n, and O(n2/α) for α n. For the bilateral game, we prove matching upper and lower bounds of Θ(nα+1) for α ≤ n, and an upper bound of 2 for α n.
Near-optimal network design with selfish agents	We introduce a simple network design game that models how independent selfish agents can build or maintain a large network. In our game every agent has a specific connectivity requirement, i.e. each agent has a set of terminals and wants to build a network in which his terminals are connected. Possible edges in the network have costs and each agent's goal is to pay as little as possible. Determining whether or not a Nash equilibrium exists in this game is NP-complete. However, when the goal of each player is to connect a terminal to a common source, we prove that there is a Nash equilibrium as cheap as the optimal network, and give a polynomial time algorithm to find a (1+ε)-approximate Nash equilibrium that does not cost much more. For the general connection game we prove that there is a 3-approximate Nash equilibrium that is as cheap as the optimal network, and give an algorithm to find a (4.65+ε)-approximate Nash equilibrium that does not cost much more.
Lower-stretch spanning trees	We show that every weighted connected graph G contains as a subgraph a spanning tree into which the edges of G can be embedded with average stretch O (log2 n log log n). Moreover, we show that this tree can be constructed in time O (m log2n) in general, and in time O (mlog n) if the input graph is unweighted. The main ingredient in our construction is a novel graph decomposition technique.Our new algorithm can be immediately used to improve the running time of the recent solver for symmetric diagonally dominant linear systems of Spielman and Teng from m2(O√lognlog log n) to m log O(1)n and to O (n log2n log log n) when the system is planar. Our result can also be used to improve several earlier approximation algorithms that use low-stretch spanning trees.
Relating feature models to other models of a software product line: a comparative study of featuremapper and VML	Software product lines using feature models often require the relation between feature models in problem space and the models used to describe the details of the product line to be expressed explicitly. This is particularly important, where automatic product derivation is required. Different approaches for modelling this mapping have been proposed in the literature. However, a discussion of their relative benefits and drawbacks is currently missing. As a first step towards a better understanding of this field, this paper applies two of these approaches-- FeatureMapper as a representative of declarative approaches and VML* as a representative of operational approaches--to the case study. We show in detail how the case study can be expressed using these approaches and discuss strengths and weaknesses of the two approaches with regard to the case study.
Introducing variability into aspect-oriented modeling approaches	Aspect-Oriented Modeling (AOM) approaches propose to model reusable aspects, or cross-cutting concerns, that can be composed in different systems at a model or code level. Building complex systems with reusable aspects helps managing software complexity. But in general, reusability of an aspect is limited to a particular context. On the one hand, if the target model does not match the template point-to-point, the aspect cannot be applied. On the other hand, even when it is actually applied, it is woven into the target model always in the same way. In this paper, we point out the needs of variability in the AOM approaches and introduce seamless variability mechanisms in an existing AOM approach to improve reusability. Our aspects can fit various contexts and can be composed into the base model in different ways. Introducing variability into AOM approaches will turn standard aspects into highly reusable aspects.
Engineering Languages for Specifying Product-Derivation Processes in Software Product Lines	The goal of a Software Product Line (SPL) is to provide a set of reusable software assets for the rapid production of a software systems family aimed at a specific market segment. The main objective of SPL engineering is to construct, as automatically as possible, specific products after selecting the particular set of features that must be included in them. Unlike traditional engineering of single systems, SPL engineering often requires dealing with three different languages at each stage of the software lifecycle: (1) a language for specifying the variability of the SPL (e.g. a feature model); (2) a language for designing the reusable software assets (e.g. UML 2.0); and (3) a language that specifies how these reusable assets must be composed for constructing specific products. There are currently available enough languages for variability specification and software assets design, but there is a general lack of languages for specifying and automating the composition of these assets. This paper presents as a novel contribution a process to engineer this kind of language. The process produces an editor and a "compiler", which automates the composition of reusable assets, for a particular language. To explain this process a language has been developed to compose reusable assets of architectural models.
Aspect-oriented multi-view modeling	Multi-view modeling allows a developer to describe a software system from multiple points of view, e.g. structural and behavioral, using different modeling notations. Aspect-oriented modeling techniques have been proposed to address the scalability problem within individual modeling notations. This paper presents RAM, an aspect-oriented modeling approach that provides scalable multi-view modeling. RAM allows the modeler to define stand-alone reusable aspect models using 3 modeling notations. The aspect models support the modeling of structure (using UML class diagrams) and behavior (using UML state and sequence diagrams). RAM supports aspect dependency chains, which allows an aspect providing complex functionality to reuse the functionality provided by other aspects. The RAM weaver can create woven views of the composed model for debugging, simulation or code generation purpose, as well as perform consistency checks during the weaving and on the woven model to detect inconsistencies of the composition.
Formal approach to integrating feature and architecture models	If we model a family of software applications with a feature model and an architecture model, we are describing the same subject from different perspectives. Hence, we are running the risk of inconsistencies. For instance, the feature model might allow feature configurations that are not realizable by the architecture. In this paper we tackle this problem by providing a formalization of dependencies between features and components. Further, we demonstrate that this formalization offers a better understanding of the modeled concepts. Moreover, we propose automated techniques that derive additional information and provide feedback to the user. Finally, we discuss how some of these techniques can be implemented.
Verifying feature-based model templates against well-formedness OCL constraints	Feature-based model templates have been recently proposed as a approach for modeling software product lines. Unfortunately, templates are notoriously prone to errors that may go unnoticed for long time. This is because such an error is usually exhibited for some configurations only, and testing all configurations is typically not feasible in practice. In this paper, we present an automated verification procedure for ensuring that no ill-structured template instance will be generated from a correct configuration. We present the formal underpinnings of our proposed approach, analyze its complexity, and demonstrate its practical feasibility through a prototype implementation.
FeatureMapper: mapping features to models	Variability modelling with feature models is one key technique for specifying the problem space of Software Product Lines (SPLs). To allow for the automatic derivation of a concrete product based on a given variant configuration, a mapping between features in the problem space and their realisations in the solution space is required. It is crucial to support the developer in the complex task of defining such mappings. These mappings can also be used to provide visualisations of the variant space that allow to reason over variability in SPLs. In this paper we present FeatureMapper, a tool that allows for defining mappings of features to model elements specifying feature realisations. These feature realisations can be defined in arbitrary Ecore-based languages. Furthermore, the tool supports different visualisation techniques that can help developers understand the complex designs of SPLs.
Derivation and Refinement of Textual Syntax for Models	Textual Syntax (TS) as a form of model representation has made its way to the Model-Driven Software Development community and is considered a viable alternative to graphical representations. To support the design and implementation of text editing facilities many concrete syntax and model mapping tools have emerged. Despite the maturity of these tools, users still spend considerable effort to specify syntaxes and generate editors even for simple metamodels. To reduce this effort, we propose to refine a specification that is automatically derived from a given metamodel. We argue that defaults in a customisable setting enable developers to quickly realise text-based editors for models. In particular in settings where metamodels evolve, such a procedure is beneficial. To evaluate this idea we present EMFText [1], an EMF/Eclipse integrated tool for agile Textual Syntax (TS) development. We show how default syntax can easily be tailored and refined to obtain a custom text editor for EMF models and demonstrate our approach by two examples.
Aspect-oriented design with reusable aspect models	The idea behind Aspect-Oriented Modeling (AOM) is to apply aspect-oriented techniques to (software) models with the aim of modularizing crosscutting concerns. This can be done within different modeling notations, at different levels of abstraction, and at different moments during the software development process. This paper demonstrates the applicability of AOM during the software design phase by presenting parts of an aspect-oriented design of a crisis management system. The design solution proposed in this paper is based on the Reusable Aspect Models (RAM) approach, which allows a modeler to express the structure and behavior of a complex system using class, state and sequence diagrams encapsulated in several aspect models. The paper describes how the model of the "create mission" functionality of the server backend can be decomposed into 23 inter-dependent aspect models. The presentation of the design is followed by a discussion on the lessons learned from the case study. Next, RAM is compared to 8 other AOM approaches according to 6 criteria: language, concern composition, asymmetric and symmetric composition, maturity, and tool support. To conclude the paper, a discussion section points out the features of RAM that specifically support reuse.
A generic weaver for supporting product lines	Aspects have gained attention in the earlier steps of the software life-cycle leading to the creation of numerous ad-hoc Aspect-Oriented Modeling (AOM) approaches. These approaches mainly focus on architecture diagrams, class diagrams, state-charts, scenarios or requirements and generally propose Aspect-Oriented composition mechanisms specific to a given kind of models defined by its own meta-model. Recently, some generic AOM approaches propose to extend the notion of aspect to any domain specific modelling language (DSML). In this trend, this paper presents GeKo. GeKo has the following properties. i) It is a generic AOM approach easily adaptable to any DSML with no need to modify the domain meta-model or to generate domain-specific frameworks. ii) It keeps a graphical representation of the weaving between an aspect model and the base model. iii) It is a tool-supported approach with a clear semantics of the different operators used to define the weaving. GeKo relies on the definition of mappings between the different views of an aspect, based on the concrete (graphical) syntax associated to the DSML. To illustrate GeKo, we derive, from the Arcade Game Maker Pedagogical Product Line, a new product in which new features are woven into the Product Line models.
A UML-based aspect-oriented design notation for AspectJ	AspectJ is a well-established programming language for the implementation of aspect-oriented programs. It supports the aspect-oriented programming paradigm by providing a special unit, called "aspect", which encapsulates crosscutting code. While with AspectJ a suitable aspect-oriented programming language is at hand, no feasible modeling language is available that supports the design of AspectJ programs. In this work, such a design notation for AspectJ programs is presented based on the UML. It provides representations for all language constructs in AspectJ and specifies an UML implementation of AspectJ's weaving mechanism. The design notation eases the perception of aspect-orientation and AspectJ programs. It carries over the advantages of aspect-orientation to the design level.
Directives for composing aspect-oriented design class models	An aspect-oriented design model consists of a set of aspect models and a primary model. Each aspect model describes a feature that crosscuts elements in the primary model. Aspect and primary models are composed to obtain an integrated design view. In this paper we describe a composition approach that utilizes a merging algorithm and composition directives. Composition directives are used when the default merging algorithm is known or expected to yield incorrect models. Our prototype tool supports default class diagram composition.
Aspect-oriented multi-view modeling	Multi-view modeling allows a developer to describe a software system from multiple points of view, e.g. structural and behavioral, using different modeling notations. Aspect-oriented modeling techniques have been proposed to address the scalability problem within individual modeling notations. This paper presents RAM, an aspect-oriented modeling approach that provides scalable multi-view modeling. RAM allows the modeler to define stand-alone reusable aspect models using 3 modeling notations. The aspect models support the modeling of structure (using UML class diagrams) and behavior (using UML state and sequence diagrams). RAM supports aspect dependency chains, which allows an aspect providing complex functionality to reuse the functionality provided by other aspects. The RAM weaver can create woven views of the composed model for debugging, simulation or code generation purpose, as well as perform consistency checks during the weaving and on the woven model to detect inconsistencies of the composition.
From aspect-oriented design to aspect-oriented programs: tool-supported translation of JPDDs into code	Join Point Designation Diagrams (JPDDs) permit developers to design aspect-oriented software on an abstract level. Consequently, JPDDs permit developers to communicate their software design independent of the programming language in use. However, developer face two problems. First, they need to understand the semantics of JPDDs in addition to their programming language. Second, after designing aspects using JPDDs, they need to decide how to map them into their programming language. A tool-supported translation of JPDDs into a known aspect-oriented language obviously would ease both problems. However, in order to achieve this goal, it is necessary to determine what a "good" JPDD translation looks like, i.e. it is necessary to have a number of principles that determine the characteristics of a "good" translation. This paper describes a tool-supported translation of JPDDs to aspect-oriented languages. Principles for translating JPDDs are described and a concrete mapping to the aspect-oriented language AspectJ is explained.
Silver: an Extensible Attribute Grammar System	Attribute grammar specification languages, like many domain specific languages, offer significant advantages to their users, such as high-level declarative constructs and domain-specific analyses. Despite these advantages, attribute grammars are often not adopted to the degree that their proponents envision. One practical obstacle to their adoption is a perceived lack of both domain-specific and general purpose language features needed to address the many different aspects of a problem. Here we describe Silver, an extensible attribute grammar specification language, and show how it can be extended with general purpose features such as pattern matching and domain specific features such as collection attributes and constructs for supporting data-flow analysis of imperative programs. The result is an attribute grammar specification language with a rich set of language features. Silver is implemented in itself by a Silver attribute grammar and utilizes forwarding to implement the extensions in a cost-effective manner.
Flexible and extensible notations for modeling languages	In model-based development, a formal description of the software (the model) is the central artifact that drives other development activities. The availability of a modeling language well-suited for the system under development and appropriate tool support are of utmost importance to practitioners. Considering the diverse needs of different application domains, flexibility in the choice of modeling languages and tools may advance the industrial acceptance of formal methods. We describe a flexible modeling language framework by which language and tool developers may better meet the special needs of various users groups without incurring prohibitive costs. The framework is based on a modular and extensible implementation of languages features using attribute grammars and forwarding. We show a prototype implementation of such a framework by extending the host language Mini-Lustre, an example synchronous data-flow language, with a collection of features such as state transitions, condition tables, and events. We also show how new languages can be created in this framework by feature composition.
A theoretical analysis of feedback flow control	Congestion is a longstanding problem in datagram networks. One congestion avoidance technique is feedback flow control, in which sources adjust their transmission rate in response to congestion signals sent (implicitly or explicitly) by network gateways. The goal is to design flow control algorithms which provide time-scale invariant, fair, stable, and robust performance. In this paper we introduce a simple model of feedback flow control, in which sources make synchronous rate adjustments based on the congestion signals and other local information, and apply it to a network of Poisson sources and exponential servers. We investigate two different styles of feedback, aggregate and individual, and two different gateway service disciplines, FIFO and Fair Share. The purpose of this paper is to identify, in the context of our simple model, which flow control design choices allow us to achieve our performance goals.Aggregate feedback flow control, in which congestion signals reflect only the aggregate congestion at the gateways, can provide time-scale invariant and stable performance, but not fair or robust performance. The properties of individual feedback flow control, in which the congestion signals reflect the congestion caused by the individual source, depend on the service discipline used in the gateways. Individual feedback with FIFO gateways can provide time-scale invariant, fair, and stable performance, but not robust performance. Individual feedback with Fair Share gateways can achieve all four performance goals. Furthermore, its stability properties are superior to those of the other two design choices. By making robust and more stable performance possible, gateway service disciplines play a crucial role in realizing effective flow control.
Analysis and simulation of a fair queueing algorithm	We discuss gateway queueing algorithms and their role in controlling congestion in datagram networks. A fair queueing algorithm, based on an earlier suggestion by Nagle, is proposed. Analysis and simulations are used to compare this algorithm to other congestion control schemes. We find that fair queueing provides several important advantages over the usual first-come-first-serve queueing algorithm: fair allocation of bandwidth, lower delay for sources using less than their full share of bandwidth, and protection from ill-behaved sources.
Virtual time	Virtual time is a new paradigm for organizing and synchronizing distributed systems which can be applied to such problems as distributed discrete event simulation and distributed database concurrency control. Virtual time provides a flexible abstraction of real time in much the same way that virtual memory provides an abstraction of real memory. It is implemented using the Time Warp mechanism, a synchronization protocol distinguished by its reliance on lookahead-rollback, and by its implementation of rollback via antimessages.
An experiment in microprocessor-based distributed digital simulation	This paper discusses the design of a distributed simulation system which will utilize off-the-shelf microprocessors in its implementation. Alternative approaches to the assignment of simulation functions and processes are presented. A project currently underway at Texas A & M University is described which considers the impact of distributed architectures on the design of simulation language support systems. The emphasis in this research project is to produce an operational prototype which can be used to establish the feasibility and utility of distributed simulation.
Modeling concepts for VLSI CAD objects	VLSI CAD applications deal with design objects that have an interface description and an implementation description. Versions of design objects have a common interface but differ in their implementations. A molecular object is a modeling construct which enables a database entity to be represented by two sets of heterogeneous records, one set describes the object's interface and the other describes its implementation. Thus a reasonable starting point for modeling design objects is to begin with the concept of molecular objects.In this paper, we identify modeling concepts that are fundamental to capturing the semantics of VLSI CAD design objects and versions in terms of molecular objects. A provisional set of user operations on design objects, consistent with these modeling concepts, is also defined. The modeling framework that we present has been found useful for investigating physical storage techniques and change notification problems in version control.
A database approach for managing VLSI design data	We describe an approach to managing information about VLSI designs, founded upon database system methods. A database component provides a low-level flat-file interface to stored data. Built on top is a design data management system, supporting the hierarchical construction of a design from primitive cells, and organizing data about alternative design representations and versions. Programs to provide a tailored interface to design data are also provided. The system simplifies the rapid construction of new design tools by taking responsibility for design data management.
The entity-relationship model—toward a unified view of data	A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, information retrieval, and data manipulation are discussed.The entity-relationship model can be used as a basis for unification of different views of data: the network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented.
Deciding expressive description logics in the framework of resolution	We present a decision procedure for the description logic SHIQ based on the basic superposition calculus, and show that it runs in exponential time for unary coding of numbers. To derive our algorithm, we extend basic superposition with a decomposition inference rule, which transforms conclusions of certain inferences into equivalent, but simpler clauses. This rule can be used for general first-order theorem proving with any resolution-based calculus compatible with the standard notion of redundancy.
The entity-relationship model—toward a unified view of data	A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, information retrieval, and data manipulation are discussed.The entity-relationship model can be used as a basis for unification of different views of data: the network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented.
Real-time classification of evoked emotions using facial feature tracking and physiological responses	We present automated, real-time models built with machine learning algorithms which use videotapes of subjects' faces in conjunction with physiological measurements to predict rated emotion (trained coders' second-by-second assessments of sadness or amusement). Input consisted of videotapes of 41 subjects watching emotionally evocative films along with measures of their cardiovascular activity, somatic activity, and electrodermal responding. We built algorithms based on extracted points from the subjects' faces as well as their physiological responses. Strengths of the current approach are (1) we are assessing real behavior of subjects watching emotional videos instead of actors making facial poses, (2) the training data allow us to predict both emotion type (amusement versus sadness) as well as the intensity level of each emotion, (3) we provide a direct comparison between person-specific, gender-specific, and general models. Results demonstrated good fits for the models overall, with better performance for emotion categories than for emotion intensity, for amusement ratings than sadness ratings, for a full model using both physiological measures and facial tracking than for either cue alone, and for person-specific models than for gender-specific or general models.
The Effect of Behavioral Realism and Form Realism of Real-Time Avatar Faces on Verbal Disclosure, Nonverbal Disclosure, Emotion Recognition, and Copresence in Dyadic Interaction	The realism of avatars in terms of behavior and form is critical to the development of collaborative virtual environments. In the study we utilized state of the art, real-time face tracking technology to track and render facial expressions unobtrusively in a desktop CVE. Participants in dyads interacted with each other via either a video-conference (high behavioral realism and high form realism), voice only (low behavioral realism and low form realism), or an “emotibox” that rendered the dimensions of facial expressions abstractly in terms of color, shape, and orientation on a rectangular polygon (high behavioral realism and low form realism). Verbal and non-verbal self-disclosure were lowest in the videoconference condition while self-reported copresence and success of transmission and identification of emotions were lowest in the emotibox condition. Previous work demonstrates that avatar realism increases copresence while decreasing self-disclosure. We discuss the possibility of a hybrid realism solution that maintains high copresence without lowering self-disclosure, and the benefits of such an avatar on applications such as distance learning and therapy.
Designing haptic icons to support collaborative turn-taking	This paper describes research exploring the use of haptics to support users collaborating remotely in a single-user shared application. Mediation of turn-taking during remote collaboration provides a context to explore haptic affordances for background communication as well as control negotiation in remote collaboration: existing turn-taking protocols are rudimentary, lacking many communication cues available in face-to-face collaboration. We therefore designed a custom turn-taking protocol that allows users to express different levels of urgency in their request for control from a collaborator; state of control and requests are communicated by touch, with the intent of offloading visual attention. To support it, we developed a set of haptic icons, tangible stimuli to which specific meanings have been assigned. Because we required an icon set which could be utilized with specified, varying levels of intrusiveness in real attentionally challenged situations, we used a perceptually guided procedure that consisted of four steps: initial icon set design, perceptual refinement, validation of learnability and effectiveness under workload, and deployment in an application simulation. We found that our haptic icons could be learned to a high degree of accuracy in under 3min and remained identifiable even under significant cognitive workload. In an exploratory observational study comparing haptic, visual, and combined haptic and visual support for our protocol, participants overall preferred the combined multi-modal support, and in particular preferred the haptic support for control changes and the visual support for displaying state. In their control negotiation, users clearly utilized the option of requesting with graded urgency. The three major contributions in this paper are: (1) the introduction and first case study using a systematic process for refining and evaluating haptic icons for background communication in a primarily visual application; (2) the usability observed for a particular set of icons designed with that process; and (3) the introduction of an urgency-based turn-taking protocol and a comparison of haptic, visual and multi-modal support of our implementation of that protocol.
A role for haptics in mobile interaction: initial design using a handheld tactile display prototype	Mobile interaction can potentially be enhanced with well-designed haptic control and display. However, advances have been limited by a vicious cycle whereby inadequate haptic technology obstructs inception of vitalizing applications. We present the first stages of a systematic design effort to break that cycle, beginning with specific usage scenarios and a new handheld display platform based on lateral skin stretch. Results of a perceptual device characterization inform mappings between device capabilities and specific roles in mobile interaction, and the next step of hardware re-engineering.
Leveraging single-user applications for multi-user collaboration: the coword approach	Single-user interactive computer applications are pervasive in our daily lives and work. Leveraging single-user applications for multi-user collaboration has the potential to significantly increase the availability and improve the usability of collaborative applications. In this paper, we report an innovative transparent adaptation approach for this purpose. The basic idea is to adapt the single-user application programming interface to the data and operational models of the underlying collaboration supporting technique, namely Operational Transformation. Distinctive features of this approach include: (1) Application transparency: it does not require access to the source code of the single-user application; (2) Unconstrained collaboration: it supports concurrent and free interaction and collaboration among multiple users; and (3) Reusable collaborative software components: collaborative software components developed with this approach can be reused in adapting a wide range of single-user applications. This approach has been applied to transparently convert MS Word into a real-time collaborative word processor, called CoWord, which supports multiple users to view and edit any objects in the same Word document at the same time over the Internet. The generality of this approach has been tested by re-applying it to convert MS PowerPoint into CoPowerPoint.
Collaboration awareness in support of collaboration transparency: requirements for the next generation of shared window systems	Shared window systems enable existing applications to be shared in the context of a real-time teleconference. The development and successful use of several such systems, albeit within limited user communities, testifies to the merits of the basic idea. However, experience to date has suggested a number of areas that have not been adequately addressed, namely: spontaneous interactions, shared workspace management, floor control, and annotation and telepointing. This paper focuses on the ramifications, for the software designer, of various user requirements in these areas. While the recommendations that result are motivated by the desire to enable continued use of collaboration-transparent applications, addressing them involves the development of systems software that is distinctly collaboration-aware.
Perceiving ordinal data haptically under workload	Visual information overload is a threat to the interpretation of displays presenting large data sets or complex application environments. To combat this problem, researchers have begun to explore how haptic feedback can be used as another means for information transmission. In this paper, we show that people can perceive and accurately process haptically rendered ordinal data while under cognitive workload. We evaluate three haptic models for rendering ordinal data with participants who were performing a taxing visual tracking task. The evaluation demonstrates that information rendered by these models is perceptually available even when users are visually busy. This preliminary research has promising implications for haptic augmentation of visual displays for information visualization.
ART: an asymmetric and reliable transport mechanism for wireless sensor networks	Existing solutions for reliable data transmission in wireless networks are aimed to offer reliable message or per transport segment delivery. However, densely deployed sensor nodes can generate many redundant messages that essentially indicate the same event from the area of interest, thus this message-level reliability can pose significantly high and unnecessary communication overhead. In this paper, we address the reliability problem by defining event reliability and query reliability to reduce extra cost of reliable transport services. Unlike other studies on transport protocols for Wireless Sensor Networks (WSN), we consider event delivery in conjunction with query delivery. For the purpose, we propose an energy-aware sensor classification algorithm to construct a network topology that is composed of sensors in providing desired level of event and query reliability. We evaluate the performance of the proposed approach and conclude that significant savings on communication costs are attainable while achieving event and query reliability.
Set k-cover algorithms for energy efficient monitoring in wireless sensor networks	Wireless sensor networks (WSNs) are emerging as an effective means for environment monitoring. This paper investigates a strategy for energy efficient monitoring in WSNs that partitions the sensors into covers, and then activates the covers iteratively in a round-robin fashion. This approach takes advantage of the overlap created when many sensors monitor a single area. Our work builds upon previous work in [13], where the model is first formulated. We have designed three approximation algorithms for a variation of the SET K-COVER problem, where the objective is to partition the sensors into covers such that the number of covers that include an area, summed over all areas, is maximized. The first algorithm is randomized and partitions the sensors, in expectation, within a fraction 1-1e (~.63) of the optimum. We present two other deterministic approximation algorithms. One is a distributed greedy algorithm with a 12 approximation ratio and the other is a centralized greedy algorithm with a 1-1e approximation ratio. We show that it is NP-Complete to guarantee better than 1516 of the optimal coverage, indicating that all three algorithms perform well with respect to the best approximation algorithm possible in polynomial time, assuming P ≠ NP. Simulations indicate that in practice, the deterministic algorithms perform far above their worst case bounds, consistently covering more than 72% of what is covered by an optimum solution. Simulations also indicate that the increase in longevity is proportional to the amount of overlap amongst the sensors. The algorithms are fast, easy to use, and according to simulations, significantly increase the longevity of sensor networks. The randomized algorithm in particular seems quite practical.
Range-free localization schemes for large scale sensor networks	Wireless Sensor Networks have been proposed for a multitude of location-dependent applications. For such systems, the cost and limitations of the hardware on sensing nodes prevent the use of range-based localization schemes that depend on absolute point-to-point distance estimates. Because coarse accuracy is sufficient for most sensor network applications, solutions in range-free localization are being pursued as a cost-effective alternative to more expensive range-based approaches. In this paper, we present APIT, a novel localization algorithm that is range-free. We show that our APIT scheme performs best when an irregular radio pattern and random node placement are considered, and low communication overhead is desired. We compare our work via extensive simulation, with three state-of-the-art range-free localization schemes to identify the preferable system configurations of each. In addition, we study the effect of location error on routing and tracking performance. We show that routing performance and tracking accuracy are not significantly affected by localization error when the error is less than 0.4 times the communication radio radius.
A scalable approach for reliable downstream data delivery in wireless sensor networks	There exist several applications of sensor networks wherere liability of data delivery can be critical. While the redundancy inherent in a sensor network might increase the degree of reliability, it by no means can provide any guaranteed reliability semantics. In this paper, we consider the problem of reliable sink-to-sensors data delivery. We first identify several fundamental challenges that need to be addressed, and are unique to a wireless sensor network environment. We then propose a scalable framework for reliable downstream data delivery that is specifically designed to both address and leverage the characteristics of a wireless sensor network, while achieving the reliability in an efficient manner. Through ns2 based simulations, we evaluate the proposed framework.
Distributed low-cost backbone formation for wireless ad hoc networks	Backbone has been used extensively in various aspects (e.g., routing, route maintenance, broadcast, scheduling) for wireless networks. Previous methods are mostly designed to minimize the backbone size. However, in many applications, it is desirable to construct a backbone with small cost when each wireless node has a cost of being in the backbone. In this paper, we first show that previous methods specifically designed to minimize the backbone size may produce a backbone with a large cost. We then propose an efficient distributed method to construct a weighted sparse backbone with low cost. We prove that the total cost of the constructed backbone is within a small constant factor of the optimum for homogeneous networks when either the nodes' costs are smooth or the network maximum node degree is bounded. We also show that with a small modification the constructed backbone is efficient for unicast: the total cost (or hop) of the least cost (or hop) path connecting any two nodes using backbone is no more than 3 (or 4) times of the least cost (or hop) path in the original communication graph. As a side product, we give an efficient overlay based multicast structure whose total cost is no more than 10 times of the minimum when the network is modeled by UDG. Our theoretical results are corroborated by our simulation studies.
Improving TCP performance over mobile ad-hoc networks with out-of-order detection and response	In a Mobile Ad Hoc Network (MANET), temporary link failures and route changes happen frequently. With the assumption that all packet losses are due to congestion, TCP performs poorly in such environment. While there has been some research on improving TCP performance over MANET, most of them require feedback from the network or the lower layer. In this research, we explore a new approach to improve TCP performance by detecting and responding to out-of-order packet delivery events, which are the results of frequent route changes. In our simulation study, this approach had achieved on average 50% performance improvement, without requiring feedback from the network or the lower layer.
Integrated coverage and connectivity configuration in wireless sensor networks	An effective approach for energy conservation in wireless sensor networks is scheduling sleep intervals for extraneous nodes, while the remaining nodes stay active to provide continuous service. For the sensor network to operate successfully, the active nodes must maintain both sensing coverage and network connectivity. Furthermore, the network must be able to configure itself to any feasible degrees of coverage and connectivity in order to support different applications and environments with diverse requirements. This paper presents the design and analysis of novel protocols that can dynamically configure a network to achieve guaranteed degrees of coverage and connectivity. This work differs from existing connectivity or coverage maintenance protocols in several key ways: 1) We present a Coverage Configuration Protocol (CCP) that can provide different degrees of coverage requested by applications. This flexibility allows the network to self-configure for a wide range of applications and (possibly dynamic) environments. 2) We provide a geometric analysis of the relationship between coverage and connectivity. This analysis yields key insights for treating coverage and connectivity in a unified framework: this is in sharp contrast to several existing approaches that address the two problems in isolation. 3) Finally, we integrate CCP with SPAN to provide both coverage and connectivity guarantees. We demonstrate the capability of our protocols to provide guaranteed coverage and connectivity configurations, through both geometric analysis and extensive simulations.
ESRT: event-to-sink reliable transport in wireless sensor networks	Wireless sensor networks (WSN) are event based systems that rely on the collective effort of several microsensor nodes. Reliable event detection at the sink is based on collective information provided by source nodes and not on any individual report. Hence, conventional end-to-end reliability definitions and solutions are inapplicable in the WSN regime and would only lead to a waste of scarce sensor resources. However, the absence of reliable transport altogether can seriously impair event detection. Hence, the WSN paradigm necessitates a collective phevent-to-sink reliability notion rather than the traditional end-to-end notion. To the best of our knowledge, reliable transport in WSN has not been studied from this perspective before.In order to address this need, a new reliable transport scheme for WSN, the event-to-sink reliable transport (ESRT) protocol, is presented in this paper. ESRT is a novel transport solution developed to achieve reliable event detection in WSN with minimum energy expenditure. It includes a congestion control component that serves the dual purpose of achieving reliability and conserving energy. Importantly, the algorithms of ESRT mainly run on the sink, with minimal functionality required at resource constrained sensor nodes. ESRT protocol operation is determined by the current network state based on the reliability achieved and congestion condition in the network. If the event-to-sink reliability is lower than required, ESRT adjusts the reporting frequency of source nodes aggressively in order to reach the target reliability level as soon as possible. If the reliability is higher than required, then ESRT reduces the reporting frequency conservatively in order to conserve energy while still maintaining reliability. This self-configuring nature of ESRT makes it robust to random, dynamic topology in WSN. Analytical performance evaluation and simulation results show that ESRT converges to the desired reliability with minimum energy expenditure, starting from any initial network state.
SPEED: A Stateless Protocol for Real-Time Communication in Sensor Networks	In this paper, we present a real-time communicationprotocol for sensor networks, called SPEED. The protocolprovides three types of real-time communication services,namely, real-time unicast, real-time area-multicast andreal-time area-anycast. SPEED is specifically tailored to bea stateless, localized algorithm with minimal control overhead.End-to-end soft real-time communication is achievedby maintaining a desired delivery speed across the sensornetwork through a novel combination of feedback controland non-deterministic geographic forwarding. SPEED is ahighly efficient and scalable protocol for sensor networkswhere the resources of each node are scarce. Theoreticalanalysis, simulation experiments and a real implementationon Berkeley motes are provided to validate our claims.
Optimal multiple-objective resource allocation using hybrid particle swarm optimization and adaptive resource bounds technique	The multiple-objective resource allocation problem (MORAP) seeks for an allocation of resource to a number of activities such that a set of objectives are optimized simultaneously and the resource constraints are satisfied. MORAP has many applications, such as resource distribution, project budgeting, software testing, health care resource allocation, etc. This paper addresses the nonlinear MORAP with integer decision variable constraint. To guarantee that all the resource constraints are satisfied, we devise an adaptive-resource-bound technique to construct feasible solutions. The proposed method employs the particle swarm optimization (PSO) paradigm and presents a hybrid execution plan which embeds a hill-climbing heuristic into the PSO for expediting the convergence. To cope with the optimization problem with multiple objectives, we evaluate the candidate solutions based on dominance relationship and a score function. Experimental results manifest that the hybrid PSO derives solution sets which are very close to the exact Pareto sets. The proposed method also outperforms several representatives of the state-of-the-art algorithms on a simulation data set of the MORAP.
Optimal testing-resource allocation with genetic algorithm for modular software systems	In software testing, an important issue is to allocate the limited testing resources to achieve maximum reliability. There are numerous publications on this issue, but the models are usually developed under the assumption of simple series or parallel modules. For complex system configuration, the optimization problem becomes difficult to solve. In this paper, we present a genetic algorithm for testing-resource allocation problems that can be used when the software systems structure is complex, and also when there are multiple objectives. We consider both system reliability and testing cost in the testing-resource allocation problems. The approach is easily implemented. Some numerical examples are shown to illustrate the applicability of the approach.
A hybrid particle swarm optimization algorithm for optimal task assignment in distributed systems	In a distributed system, a number of application tasks may need to be assigned to different processors such that the system cost is minimized and the constraints with limited resource are satisfied. Most of the existing formulations for this problem have been found to be NP-complete, and thus finding the exact solutions is computationally intractable for large-scaled problems. This paper presents a hybrid particle swarm optimization algorithm for finding the near optimal task assignment with reasonable time. The experimental results manifest that the proposed method is more effective and efficient than a genetic algorithm. Also, our method converges at a fast rate and is suited to large-scaled task assignment problems.
Polyhedral Analysis for Synchronous Languages	We define an operational semantics for the Signal language and design an analysis which allows to verify properties pertaining to the relation between values of the numeric and boolean variables of a reactive system. A distinguished feature of the analysis is that it is expressed and proved correct with respect to the source program rather than on an intermediate representation of the program. The analysis calculates a safe approximation to the set of reachable states by a symbolic fixed point computation in the domain of convex polyhedra using a novel widening operator based on the convex hull representation of polyhedra.
Programming and Verifying Real-Time Systems by Means of the Synchronous Data-Flow Language LUSTRE	The benefits of using a synchronous data-flow language for programming critical real-time systems are investigated. These benefits concern ergonomy (since the dataflow approach meets traditional description tools used in this domain) and ability to support formal design and verification methods. It is shown, using a simple example, how the language LUSTRE and its associated verification tool LESAR, can be used to design a program, to specify its critical properties, and to verify these properties. As the language LUSTRE and its uses have already been discussed in several papers, emphasis is put on program verification.
Shallow water model on cubed-sphere by multi-moment finite volume method	A global numerical model for shallow water flows on the cubed-sphere grid is proposed in this paper. The model is constructed by using the constrained interpolation profile/multi-moment finite volume method (CIP/MM FVM). Two kinds of moments, i.e. the point value (PV) and the volume-integrated average (VIA) are defined and independently updated in the present model by different numerical formulations. The Lax-Friedrichs upwind splitting is used to update the PV moment in terms of a derivative Riemann problem, and a finite volume formulation derived by integrating the governing equations over each mesh element is used to predict the VIA moment. The cubed-sphere grid is applied to get around the polar singularity and to obtain uniform grid spacing for a spherical geometry. Highly localized reconstruction in CIP/MM FVM is well suited for the cubed-sphere grid, especially in dealing with the discontinuity in the coordinates between different patches. The mass conservation is completely achieved over the whole globe. The numerical model has been verified by Williamson's standard test set for shallow water equation model on sphere. The results reveal that the present model is competitive to most existing ones.
A wave propagation method for hyperbolic systems on the sphere	Presented in this work is an explicit finite volume method for solving general hyperbolic systems on the surface of a sphere. Applications where such systems arise include passive tracer advection in the atmosphere, shallow water models of the ocean and atmosphere, and shallow water magnetohydrodynamic models of the solar tachocline. The method is based on the curved manifold wave propagation algorithm of Rossmanith, Bale, and LeVeque [A wave propagation algorithm for hyperbolic systems on curved manifolds, J. Comput. Phys. 199 (2004) 631-662], which makes use of parallel transport to approximate geometric source terms and orthonormal Riemann solvers to carry out characteristic decompositions. This approach employs TVD wave limiters, which allows the method to be accurate for both smooth solutions and solutions in which large gradients or discontinuities can occur in the form of material interfaces or shock waves. The numerical grid used in this work is the cubed sphere grid of Ronchi, Iacono, and Paolucci [The 'cubed sphere': a new method for the solution of partial differential equations in spherical geometry, J. Comput. Phys. 124 (1996) 93-114], which covers the sphere with nearly uniform resolution using six identical grid patches with grid lines lying on great circles. Boundary conditions across grid patches are applied either through direct copying from neighboring grid cells in the case of scalar equations or 1D interpolation along great circles in the case of more complicated systems. The resulting numerical method is applied to several test problems for the advection equation, the shallow water equations, and the shallow water magnetohydrodynamic (SMHD) equations. For the SMHD equations, we make use of an unstaggered constrained transport method to maintain a discrete divergence-free magnetic field.
CIP/multi-moment finite volume method for Euler equations: A semi-Lagrangian characteristic formulation	An accurate algorithm for the hyperbolic equations has been proposed by combining the constrained interpolation profile/multi-moment finite volume method (CIP/MM FVM) with the characteristic theory. Two types of moments, i.e. the point value (PV) at cell boundary of each mesh element and the volume-integrated average (VIA) over each mesh cell of a physical field, are treated as the model variables and updated independently in time. The interpolation that uses both PV and VIA is reconstructed for each Riemann invariant of the hyperbolic conservation laws. The PVs are then updated by semi-Lagrangian schemes along the characteristic curves, while the VIAs are computed by formulations of flux form, where the numerical fluxes are evaluated by averaging the physical fields over the characteristic curves. The Runge-Kutta type schemes are used for integrating the trajectory equations based on the characteristic speeds to improve the accuracy in time. The numerical procedure for the one-dimensional Euler conservation laws is described in detail in this paper. Number of benchmark tests are presented. The numerical results show that the present method is accurate and competitive to other existing methods.
Static Analyses for Eliminating Unnecessary Synchronization from Java Programs	This paper presents and evaluates a set of analyses designed to reduce synchronization overhead in Java programs. Monitor-based synchronization in Java often causes significant overhead, accounting for 5-10% of total execution time in our benchmark applications. To reduce this overhead, programmers often try to eliminate unnecessary lock operations by hand. Such manual optimizations are tedious, error-prone, and often result in poorly structured and less reusable programs. Our approach replaces manual optimizations with static analyses that automatically find and remove unnecessary synchronization from Java programs. These analyses optimize cases where a monitor is entered multiple times by a single thread, where one monitor is nested within another, and where a monitor is accessible by only one thread. A partial implementation of our analyses eliminates up to 70% of synchronization overhead and improves running time by up to 5% for several already hand-optimized benchmarks. Thus, our automated analyses have the potential to significantly improve the performance of Java applications while enabling programmers to design simpler and more reusable multithreaded code.
Obtaining sequential efficiency for concurrent object-oriented languages	Concurrent object-oriented programming (COOP) languages focus the abstraction and encapsulation power of abstract data types on the problem of concurrency control. In particular, pure fine-grained concurrent object-oriented languages (as opposed to hybrid or data parallel) provides the programmer with a simple, uniform, and flexible model while exposing maximum concurrency. While such languages promise to greatly reduce the complexity of large-scale concurrent programming, the popularity of these languages has been hampered by efficiency which is often many orders of magnitude less than that of comparable sequential code. We present a sufficiency set of techniques which enables the efficiency of fine-grained concurrent object-oriented languages to equal that of traditional sequential languages (like C) when the required data is available. These techniques are empirically validated by the application to a COOP implementation of the Livermore Loops.
Call graph construction in object-oriented languages	Interprocedural analyses enable optimizing compilers to more precisely model the effects of non-inlined procedure calls, potentially resulting in substantial increases in application performance. Applying interprocedural analysis to programs written in object-oriented or functional languages is complicated by the difficulty of constructing an accurate program call graph. This paper presents a parameterized algorithmic framework for call graph construction in the presence of message sends and/or first class functions. We use this framework to describe and to implement a number of well-known and new algorithms. We then empirically assess these algorithms by applying them to a suite of medium-sized programs written in Cecil and Java, reporting on the relative cost of the analyses, the relative precision of the constructed call graphs, and the impact of this precision on the effectiveness of a number of interprocedural optimizations.
Automatic inline allocation of objects	Object-oriented languages like Java and Smalltalk provide a uniform object model that simplifies programming by providing a consistent, abstract model of object behavior. But direct implementations introduce overhead, removal of which requires aggressive implementation techniques (e.g. type inference, function specialization); in this paper, we introduce object inlining, an optimization that automatically inline allocates objects within containers (as is done by hand in C++) within a uniform model. We present our technique, which includes novel program analyses that track how inlinable objects are used throughout the program. We evaluated object inlining on several object-oriented benchmarks. It produces performance up to three times as fast as a dynamic model without inlining and roughly equal to that of manually-inlined codes.
On the equivalence between Non-negative Matrix Factorization and Probabilistic Latent Semantic Indexing	Non-negative Matrix Factorization (NMF) and Probabilistic Latent Semantic Indexing (PLSI) have been successfully applied to document clustering recently. In this paper, we show that PLSI and NMF (with the I-divergence objective function) optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoretical basis for a new hybrid method that runs PLSI and NMF alternatively, each jumping out of the local minima of the other method successively, thus achieving a better final solution. Extensive experiments on five real-life datasets show relations between NMF and PLSI, and indicate that the hybrid method leads to significant improvements over NMF-only or PLSI-only methods. We also show that at first-order approximation, NMF is identical to the @g^2-statistic.
Nonnegative matrix factorization and probabilistic latent semantic indexing: equivalence, chi-square statistic, and a hybrid method	Non-negative Matrix Factorization (NMF) and Probabilistic Latent Semantic Indexing (PLSI) have been successfully applied to document clustering recently. In this paper, we show that PLSI and NMF optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoretical basis for a new hybrid method that runs PLSI and NMF alternatively, each jumping out of local minima of the other method successively, thus achieving better final solution. Extensive experiments on 5 real-life datasets show relations between NMF and PLSI, and indicate the hybrid method lead to significant improvements over NMF-only or PLSI-only methods. We also show that at first order approximation, NMF is identical to χ2-statistic.
A general model for clustering binary data	Clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity classes. This paper studies the problem of clustering binary data. This is the case for market basket datasets where the transactions contain items and for document datasets where the documents contain "bag of words". The contribution of the paper is three-fold. First a general binary data clustering model is presented. The model treats the data and features equally, based on their symmetric association relations, and explicitly describes the data assignments as well as feature assignments. We characterize several variations with different optimization procedures for the general model. Second, we also establish the connections between our clustering model with other existing clustering methods. Third, we also discuss the problem for determining the number of clusters for binary clustering. Experimental results show the effectiveness of the proposed clustering model.
Empirical and Theoretical Comparisons of Selected Criterion Functions for Document Clustering	This paper evaluates the performance of different criterion functions in the context of partitional clustering algorithms for document datasets. Our study involves a total of seven different criterion functions, three of which are introduced in this paper and four that have been proposed in the past. We present a comprehensive experimental evaluation involving 15 different datasets, as well as an analysis of the characteristics of the various criterion functions and their effect on the clusters they produce. Our experimental results show that there are a set of criterion functions that consistently outperform the rest, and that some of the newly proposed criterion functions lead to the best overall results. Our theoretical analysis shows that the relative performance of the criterion functions depends on (i) the degree to which they can correctly operate when the clusters are of different tightness, and (ii) the degree to which they can lead to reasonably balanced clusters.
Trapezoidal approximations of fuzzy numbers preserving the expected interval--- Algorithms and properties	Fuzzy number approximation by trapezoidal fuzzy numbers which preserves the expected interval is discussed. Algorithms for calculating the proper approximations are proposed and some properties of the approximation operators are discussed. It is shown that an adequate approximation operator might be chosen through the comparisons of some characteristics of the fuzzy number, like its ambiguity, width, its value and weighted expected value.
A note on trapezoidal approximations of fuzzy numbers	In this paper, we propose some generalized and new properties of the trapezoidal approximations of fuzzy numbers. First, we claim that the trapezoidal approximation operator is linear with respect to the operations of fuzzy numbers. Second, we show a new property concerning the centroid points of fuzzy numbers. Furthermore, a comment for computing the trapezoidal approximations is provided. Finally, some properties about the distance between a fuzzy number and its trapezoidal approximation are proved.
Trapezoidal approximations of fuzzy numbers---revisited	Fuzzy number approximation by trapezoidal fuzzy numbers which preserve expected interval is discussed. The previously proposed approximation operator is improved so as to always produce a well formed trapezoidal fuzzy number.
Approximation of fuzzy numbers by trapezoidal fuzzy numbers preserving the expected interval	The problem to find the nearest trapezoidal approximation of a fuzzy number with respect to a well-known metric, which preserves the expected interval of the fuzzy number, is completely solved. The previously proposed approximation operators are improved so as to always obtain a trapezoidal fuzzy number. Properties of this new trapezoidal approximation operator are studied.
Approximation of fuzzy numbers by trapezoidal fuzzy numbers preserving the expected interval	The problem to find the nearest trapezoidal approximation of a fuzzy number with respect to a well-known metric, which preserves the expected interval of the fuzzy number, is completely solved. The previously proposed approximation operators are improved so as to always obtain a trapezoidal fuzzy number. Properties of this new trapezoidal approximation operator are studied.
A note on trapezoidal approximations of fuzzy numbers	In this paper, we propose some generalized and new properties of the trapezoidal approximations of fuzzy numbers. First, we claim that the trapezoidal approximation operator is linear with respect to the operations of fuzzy numbers. Second, we show a new property concerning the centroid points of fuzzy numbers. Furthermore, a comment for computing the trapezoidal approximations is provided. Finally, some properties about the distance between a fuzzy number and its trapezoidal approximation are proved.
Trapezoidal approximations of fuzzy numbers---revisited	Fuzzy number approximation by trapezoidal fuzzy numbers which preserve expected interval is discussed. The previously proposed approximation operator is improved so as to always produce a well formed trapezoidal fuzzy number.
Fast cycle-approximate instruction set simulation	Instruction set simulators are indispensable tools in both ASIP design space exploration and the software development and optimisation process for existing platforms. Despite the recent progress in improving the speed of functional instruction set simulators cycle-accurate simulation is still prohibitively slow for all but the most simple programs. This severely limits the applicability of cycle-accurate simulators in the performance evaluation of complex embedded applications. In this paper we present a novel approach, namely the prediction of cycle counts based on information gathered during fast functional simulation and prior training. We have evaluated our approach against a cycle-accurate ARM v5 architecture simulator and a large set of benchmarks. We demonstrate it is capability of providing highly accurate performance predictions with an average error of less than 5.8% at a fraction of the time for cycle-accurate simulation.
Performance prediction based on inherent program similarity	A key challenge in benchmarking is to predict the performance of an application of interest on a number of platforms in order to determine which platform yields the best performance. This paper proposes an approach for doing this. We measure a number of microarchitecture-independent characteristics from the application of interest, and relate these characteristics to the characteristics of the programs from a previously profiled benchmark suite. Based on the similarity of the application of interest with programs in the benchmark suite, we make a performance prediction of the application of interest. We propose and evaluate three approaches (normalization, principal components analysis and genetic algorithm) to transform the raw data set of microarchitecture-independent characteristics into a benchmark space in which the relative distance is a measure for the relative performance differences. We evaluate our approach using all of the SPEC CPU2000 benchmarks and real hardware performance numbers from the SPEC website. Our framework estimates per-benchmark machine ranks with a 0.89 average and a 0.80 worst case rank correlation coefficient.
Flexible and Formal Modeling of Microprocessors with Application to Retargetable Simulation	Given the growth in application-specific processors, there is a strong need for a retargetable modeling framework that is capable of accurately capturing complex processor behaviors and generating efficient simulators. We propose the operation state machine (OSM) computation model to serve as the foundation of such a modeling framework. The OSM model separates the processor into two interacting layers: the operation layer where operation semantics and timing are modeled, and the hardware layer where disciplined hardware units interact. This declarative model allows for direct synthesis of micro-architecture simulators as it encapsulates precise concurrency semantics of microprocessors. We illustrate the practical benefits of this model through two case studies 驴 the StrongARM core and the PowerPC-750 superscalar processor. The experimental results demonstrate that the OSM model has excellent modeling productivity and model efficiency. Additional applications of this modeling framework include derivation of information required by compilers and formal analysis for processor validation.
Accurate and efficient regression modeling for microarchitectural performance and power prediction	We propose regression modeling as an efficient approach for accurately predicting performance and power for various applications executing on any microprocessor configuration in a large microarchitectural design space. This paper addresses fundamental challenges in microarchitectural simulation cost by reducing the number of required simulations and using simulated results more effectively via statistical modeling and inference.Specifically, we derive and validate regression models for performance and power. Such models enable computationally efficient statistical inference, requiring the simulation of only 1 in 5 million points of a joint microarchitecture-application design space while achieving median error rates as low as 4.1 percent for performance and 4.3 percent for power. Although both models achieve similar accuracy, the sources of accuracy are strikingly different. We present optimizations for a baseline regression model to obtain (1) application-specific models to maximize accuracy in performance prediction and (2) regional power models leveraging only the most relevant samples from the microarchitectural design space to maximize accuracy in power prediction. Assessing sensitivity to the number of samples simulated for model formulation, we find fewer than 4,000 samples from a design space of approximately 22 billion points are sufficient. Collectively, our results suggest significant potential in accurate and efficient statistical inference for microarchitectural design space exploration via regression models.
Instruction set compiled simulation: a technique for fast and flexible instruction set simulation	Instruction set simulators are critical tools for the exploration and validation of new programmable architectures. Due to increasing complexity of the architectures and time-to-market pressure, performance is the most important feature of an instruction-set simulator. Interpretive simulators are flexible but slow, whereas compiled simulators deliver speed at the cost of flexibility. This paper presents a novel technique for generation of fast instruction set simulators that combines the benefit of both compiled and interpretive simulation. We achieve fast instruction accurate simulation through two mechanisms. First, we move the time consuming decoding process from run-time to compile time while maintaining the flexibility of the interpretive simulation. Second, we use a novel instruction abstraction technique to generate aggressively optimized decoded instructions that further improves simulation performance. Our instruction set compiled simulation (IS-CS) technique delivers upto 40% performance improvement over the best known published result that has the flexibility of interpretive simulation. We illustrate the applicability of the IS-CS technique using the ARM7 embedded processor.
A retargetable framework for instruction-set architecture simulation	Instruction-set architecture (ISA) simulators are an integral part of today's processor and software design process. While increasing complexity of the architectures demands high-performance simulation, the increasing variety of available architectures makes retargetability a critical feature of an instruction-set simulator. Retargetability requires generic models while high-performance demands target specific customizations. To address these contradictory requirements, we have developed a generic instruction model and a generic decode algorithm that facilitates easy and efficient retargetability of the ISA-simulator for a wide range of processor architectures, such as RISC, CISC, VLIW, and variable length instruction-set processors. The instruction model is used to generate compact and easy to debug instruction descriptions that are very similar to that of architecture manual. These descriptions are used to generate high-performance simulators. Our retargetable framework combines the flexibility of interpretive simulation with the speed of compiled simulation. The generation of the simulator is completely separate from the simulation engine. Hence, we can incorporate any fast simulation technique in our retargetable framework without introducing any performance penalty. To demonstrate this, we have incorporated fast IS-CS simulation engine in our retargetable framework which has generated 70&percnt; performance improvement over the best known simulators in this category. We illustrate the retargetability of our approach using two popular, yet different, realistic architectures: the SPARC and the ARM.
Fast compiler optimisation evaluation using code-feature based performance prediction	Performance tuning is an important and time consuming task which may have to be repeated for each new application and platform. Although iterative optimisation can automate this process, it still requires many executions of different versions of the program. As execution time is frequently the limiting factor in the number of versions or transformed programs that can be considered, what is needed is a mechanism that can automatically predict the performance of a modified program without actually having to run it. This paper presents a new machine learning based technique to automatically predict the speedup of a modified program using a performance model based on the code features of the tuned programs. Unlike previous approaches it does not require any prior learning over a benchmark suite. Furthermore, it can be used to predict the performance of any tuning and is not restricted to a prior seen trans-formation space. We show that it can deliver predictions with a high correlation coefficient and can be used to dramatically reduce the cost of search.
Conformance checking of service behavior	A service-oriented system is composed of independent software units, namely services, that interact with one another exclusively through message exchanges. The proper functioning of such system depends on whether or not each individual service behaves as the other services expect it to behave. Since services may be developed and operated independently, it is unrealistic to assume that this is always the case. This article addresses the problem of checking and quantifying how much the actual behavior of a service, as recorded in message logs, conforms to the expected behavior as specified in a process model. We consider the case where the expected behavior is defined using the BPEL industry standard (Business Process Execution Language for Web Services). BPEL process definitions are translated into Petri nets and Petri net-based conformance checking techniques are applied to derive two complementary indicators of conformance: fitness and appropriateness. The approach has been implemented in a toolset for business process analysis and mining, namely ProM, and has been tested in an environment comprising multiple Oracle BPEL servers.
Web services: a process algebra approach	It is now well-admitted that formal methods are helpful for many issues raised in the Web service area. In this paper we present a framework for the design and the verification of WSs using process algebras and their tools. We define a two-way mapping between abstract specifications written using these calculi and executable Web services written in BPEL4WS; the translation includes also compensation, event, and fault handlers. The following choices are available: design and verification in BPEL4WS, using process algebra tools, or design and verification in process algebra and automatically obtaining the corresponding BPEL4WS code. The approaches can be combined. Process algebras are not useful only for temporal logic verification: we remark the use of simulation/bisimulation for verification, for the hierarchical refinement design method, for the service redundancy analysis in a community, and for replacing a service with another one in a composition.
Workflow mining: a survey of issues and approaches	Many of today's information systems are driven by explicit process models. Workflow management systems, but also ERP, CRM, SCM, and B2B, are configured on the basis of a workflow model specifying the order in which tasks need to be executed. Creating a workflow design is a complicated time-consuming process and typically there are discrepancies between the actual workflow processes and the processes as perceived by the management. To support the design of workflows, we propose the use of workflow mining. Starting point for workflow mining is a so-called "workflow log" containing information about the workflow process as it is actually being executed. In this paper, we introduce the concept of workflow mining and present a common format for workflow logs. Then we discuss the most challenging problems and present some of the workflow mining approaches available today.
Cremona: an architecture and library for creation and monitoring of WS-agreents	Using services across domain boundaries, be they organizations or self-managing components of large distributed systs, requires the setup of an agreent between the parties involved, defining the terms of the service including interfaces, security and Quality of Service (QoS) properties. In an on-dand environment in which services are contracted on a short notice, the establishment of an agreent as well as the setup of agreement-fulfilling and monitoring systs of the parties involved must be spontaneous and, partially, automated. WS-Agreent is a standardization effort being conducted in the Global Grid Forum defining a simple agreent establishment protocol, an XML-representation of agreements and agreent tplates as well as a runtime agreement monitoring interface, based on the WSRF set of standards. WS-Agreent standardizes the interaction between the organizational domains. In addition, providers require an infrastructure to manage agreent tplates, implent the interfaces, check availability of service capacity and expose agreement states at runtime. Also, agreent requesters need infrastructure to read tplates, fill in tplates to create suitable agreements, and monitor agreent state at runtime. Crona (Creation and Monitoring of Agreents) proposes an architecture for the WS-Agreent-implenting middleware. In addition, the Crona Java Library implents the WS-Agreent interfaces, provides management functionality for agreement tplates and instances, and defines abstractions of service-providing systs that can be implented in a domain-specific environment.
Analysis of Multithreaded Programs	The field of program analysis has focused primarily on sequential programming languages. But multithreading is becoming increasingly important, both as a program structuring mechanism and to support efficient parallel computations. This paper surveys research in analysis for multithreaded programs, focusing on ways to improve the efficiency of analyzing interactions between threads, to detect data races, and to ameliorate the impact of weak memory consistency models. We identify two distinct classes of multithreaded programs, activity management programs and parallel computing programs, and discuss how the structure of these kinds of programs leads to different solutions to these problems. Specifically, we conclude that augmented type systems are the most promising approach for activity management programs, while targeted program analyses are the most promising approach for parallel computing programs.
Escape analysis for object-oriented languages: application to Java	Escape analysis [27, 14, 5] is a static analysis that determines whether the lifetime of data exceeds its static scope.The main originality of our escape analysis is that it determines precisely the effect of assignments, which is necessary to apply it to object oriented languages with promising results, whereas previous work [27, 14, 5] applied it to functional languages and were very imprecise on assignments. Our implementation analyses the full Java™ Language.We have applied our analysis to stack allocation and synchronization elimination. We manage to stack allocate 13% to 95% of data, eliminate more than 20% of synchronizations on most programs (94% and 99% on two examples) and get up to 44% speedup (21% on average). Our detailed experimental study on large programs shows that the improvement comes from the decrease of the garbage collection and allocation times than from improvements on data locality [7], contrary to what happened for ML [5].
Removing unnecessary synchronization in Java	Java programs perform many synchronization operations on data structures. Some of these synchronization are unnecessary; in particular, if an object is reachable only by a single thread, concurrent access is impossible and no synchronization is needed. We describe an interprocedural, flow- and context-insensitive dataflow analysis that finds such situations. A global optimizing transformation then eliminates synchronizations on these objects. For every program in our suite of ten Java benchmarks consisting of SPECjvm98 and others, our system optimizes over 90% of the alias sets containing at least one synchronized object. As a result, the dynamic frequency of synchronizations is reduced by up to 99%. For two benchmarks that perform synchronizations very frequently, this optimization leads to speedups of 36% and 20%.
Basic compiler algorithms for parallel programs	Traditional compiler techniques developed for sequential programs do not guarantee the correctness (sequential consistency) of compiler transformations when applied to parallel programs. This is because traditional compilers for sequential programs do not account for the updates to a shared variable by different threads. We present a concurrent static single assignment (CSSA) form for parallel programs containing cobegin/coend and parallel do constructs and post/wait synchronization primitives. Based on the CSSA form, we present copy propagation and dead code elimination techniques. Also, a global value numbering technique that detects equivalent variables in parallel programs is presented. By using global value numbering and the CSSA form, we extend classical common subexpression elimination, redundant load/store elimination, and loop invariant detection to parallel programs without violating sequential consistency. These optimization techniques are the most commonly used techniques for sequential programs. By extending these techniques to parallel programs, we can guarantee the correctness of the optimized program and maintain single processor performance in a multiprocessor environment.
Static Analyses for Eliminating Unnecessary Synchronization from Java Programs	This paper presents and evaluates a set of analyses designed to reduce synchronization overhead in Java programs. Monitor-based synchronization in Java often causes significant overhead, accounting for 5-10% of total execution time in our benchmark applications. To reduce this overhead, programmers often try to eliminate unnecessary lock operations by hand. Such manual optimizations are tedious, error-prone, and often result in poorly structured and less reusable programs. Our approach replaces manual optimizations with static analyses that automatically find and remove unnecessary synchronization from Java programs. These analyses optimize cases where a monitor is entered multiple times by a single thread, where one monitor is nested within another, and where a monitor is accessible by only one thread. A partial implementation of our analyses eliminates up to 70% of synchronization overhead and improves running time by up to 5% for several already hand-optimized benchmarks. Thus, our automated analyses have the potential to significantly improve the performance of Java applications while enabling programmers to design simpler and more reusable multithreaded code.
Effective fine-grain synchronization for automatically parallelized programs using optimistic synchronization primitives	This article presents our experience using optimistic synchronization to implement fine-grain atomic operations in the context of a parallelizing compiler for irregular, object-based computations. Our experience shows that the synchronization requirements of these programs differ significantly from those of traditional parallel computations, which use loop nests to access dense matrices using affine access functions. In addition to coarse-grain barrier synchronization, our irregular computations require synchronization primitives that support efficient fine-grain atomic operations. The standard implementation mechanism for atomic operations uses mutual exclusion locks. But the overhead of acquiring and releasing locks can reduce the performance. Locks can also consume significant amounts of memory. Optimistic synchronization primitives such as loud-linked/store conditional are an attractive alternative. They require no additional memory and eliminate the use of heavyweight blocking synchronization constructs. We evaluate the effectiveness of optimistic synchronization by comparing experimental results from two versions of a parallelizing compiler for irregular, object-based computations. One version generates code that uses mutual exclusion locks to make operations execute atomically. The other version generates code that uses mutual exclusion locks to make operations execute atomically. The other version uses optimistic synchronization. We used this compiler to automatically parallelize three irregular, object-based benchmark applications of interest to the scientific and engineering computation community. The presented experimental results indicate that the use of optimistic synchronization in this context can significantly reduce the memory consumption and improve the overall performance.
The design, implementation, and evaluation of Jade	Jade is a portable, implicitly parallel language designed for exploiting task-level concurrency.Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to declare how parts of the program access data. The Jade implementation uses this data access information to automatically extract the concurrency and map the application onto the machine at hand. The resulting parallel execution preserves the semantics of the original serial program. We have implemented Jade as an extension to C, and Jade implementations exist for s hared-memory multiprocessors, homogeneous message-passing machines, and heterogeneous networks of workstations. In this atricle we discuss the design goals and decisions that determined the final form of Jade and present an overview of the Jade implementation. We also present our experience using Jade to implement several complete scientific and engineering applications. We use this experience to evaluate how the different Jade language features were used in practice and how well Jade as a whole supports the process of developing parallel applications. We find that the basic idea of preserving the serial semantics simplifies the program development process, and that the concept of using data access specifications to guide the parallelization offers significant advantages over more traditional control-based approaches. We also find that the Jade data model can interact poorly with concurrency patterns that write disjoint pieces of a single aggregate data structure, although this problem arises in only one of the applications.
Symbolic bounds analysis of pointers, array indices, and accessed memory regions	This paper presents a novel framework for the symbolic bounds analysis of pointers, array indices, and accessed memory regions. Ourframework formulates each analysis problem as a system of inequality constraints between symbolic bound polynomials. It then reduces the constraint system to a linear program. The solution to the linear program provides symbolic lower and upper bounds for the values of pointer and array index variables and for the regions of memory that each statement and procedure accesses. This approach eliminates fundamental problems associated with applying standard fixed-point approaches to symbolic analysis problems. Experimental results from our implemented compiler show that the analysis can solve several important problems, including staticrace detection, automatic parallelization, static detection of arraybounds violations, elimination of array bounds checks, and reduction of the number of bits used to store computed values.
Compositional pointer and escape analysis for Java programs	This paper presents a combined pointer and escape analysis algorithm for Java programs. The algorithm is based on the abstraction of points-to escape graphs, which characterize how local variables and fields in objects refer to other objects. Each points-to escape graph also contains escape information, which characterizes how objects allocated in one region of the program can escape to be accessed by another region. The algorithm is designed to analyze arbitrary regions of complete or incomplete programs, obtaining complete information for objects that do not escape the analyzed regions.We have developed an implementation that uses the escape information to eliminate synchronization for objects that are accessed by only one thread and to allocate objects on the stack instead of in the heap. Our experimental results are encouraging. We were able to analyze programs tens of thousands of lines long. For our benchmark programs, our algorithms enable the elimination of between 24% and 67% of the synchronization operations. They also enable the stack allocation of between 22% and 95% of the objects.
Effective synchronization removal for Java	We present a new technique for removing unnecessary synchronization operations from statically compiled Java programs. Our approach improves upon current efforts based on escape analysis, as it can eliminate synchronization operations even on objects that escape their allocating threads. It makes use of a compact, equivalence-class-based representation that eliminates the need for fixed point operations during the analysis. We describe and evaluate the performance of an implementation in theMarmot native Java compiler. For the benchmark programs examined, the optimization removes 100% of the dynamic synchronization operations in single-threaded programs, and 0-99% in multi-threaded programs, at a low cost in additional compilation time and code growth.
Detecting conflicts between structure accesses	Two references to a record structure conflict if they access the same field and at least one modifies the location. Because structures can be connected by pointers, deciding if two statements conflict requires knowledge of the possible aliases for the locations that they access.This paper describes a dataflow computation that produces a conservative description of the aliases visible at any point in a program. The data structure that records aliases is an alias graph. It also labels instances of structures so that the objects referenced at different points in a program can be compared. This paper shows how alias graphs can be used to detect potential conflicts.
Event synchronization analysis for debugging parallel programs	One of the major difficulties of explicit parallel programming for a shared memory machine model is detecting the potential for nondeterminacy and identifying its causes. There will often be shared variables in a parallel program, and the tasks comprising the program may need to be synchronized when accessing these variables.This paper discusses this problem and presents a method for automatically detecting non-determinacy in parallel programs that utilize event style synchronization instructions, using the Post, Wait, and Clear primitives. With event style synchronization, especially when there are many references to the same event, the difficulty lies in computing the execution order that is guaranteed given the synchronization instructions and the sequential components of the program. The main result in this paper is an algorithm that computes such an execution order and yields a Task Graph upon which a nondeterminacy detection algorithm can be applied.We have focused on events because they are a frequently used synchronization mechanism in parallel versions of Fortran, including Cray [Cray87], IBM [IBM88], Cedar [GPHL88], and PCF Fortran [PCF88].
Using symbolic execution for verification of Ada tasking programs	A method is presented for using symbolic execution to generate the verification conditions required for proving correctness of programs written in a tasking subset of Ada. The symbolic execution rules are derived from proof systems that allow tasks to be verified independently in local proofs, which are then checked for cooperation. The isolation nature of this approach to symbolic execution of concurrent programs makes it better suited to formal verification than the more traditional interleaving approach, which suffers from combinatorial problems. The criteria for correct operation of a concurrent program include partial correctness, as well as more general safety properties, such as mutual exclusion and freedom from deadlock.
Data flow equations for explicitly parallel programs	We present a solution to the reaching definitions problem for programs with explicit lexically specified parallel constructs, such as cobegin/coend or parallel_sections, both with and without explicit synchronization operations, such as Post, Wait or Advance. The reaching definitions information for sequential programs is used to solve many standard optimization problems. In parallel programs, this information can also be used to explicitly direct communication and data ownership. Although work has been done on analyzing parallel programs to detect data races, little work has been done on optimizing such programs.We show how the memory consistency model specified by an explicitly parallel programming language can influence the complexity of the reaching definitions problem. By selecting the “weakest” memory consistency semantics, we can efficiently solve the reaching definitions problem for correct programs.
The implementation of the Cilk-5 multithreaded language	The fifth release of the multithreaded language Cilk uses a provably good "work-stealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this "work-first" principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5's compiler and its runtime system. In particular, we present Cilk-5's novel "two-clone" compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.
Commutativity analysis: a new analysis technique for parallelizing compilers	This article presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e., generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis technique. We have used this system to automatically parallelize three complete scientific computations: the Barnes-Hut N-body solver, the Water liquid simulation code, and the String seismic simulation code. This article presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler.
Escape analysis for Java	This paper presents a simple and efficient data flow algorithm for escape analysis of objects in Java programs to determine (i) if an object can be allocated on the stack; (ii) if an object is accessed only by a single thread during its lifetime, so that synchronization operations on that object can be removed. We introduce a new program abstraction for escape analysis, the connection graph, that is used to establish reachability relationships between objects and object references. We show that the connection graph can be summarized for each method such that the same summary information may be used effectively in different calling contexts. We present an interprocedural algorithm that uses the above property to efficiently compute the connection graph and identify the non-escaping objects for methods and threads. The experimental results, from a prototype implementation of our framework in the IBM High Performance Compiler for Java, are very promising. The percentage of objects that may be allocated on the stack exceeds 70% of all dynamically created objects in three out of the ten benchmarks (with a median of 19%), 11% to 92% of all lock operations are eliminated in those ten programs (with a median of 51%), and the overall execution time reduction ranges from 2% to 23% (with a median of 7%) on a 333 MHz PowerPC workstation with 128 MB memory.
Data flow analysis for verifying properties of concurrent programs	In this paper we present an approach, based on data flow analysis, that can provide cost-effective analysis of concurrent programs with respect to explicitly stated correctness properties. Using this approach, a developer specifies a property of a concurrent program as a pattern of selected program events and asks the analysis to verify that all or no program executions satisfy the given property. We have developed a family of polynomial-time, conservative data flow anlysis algorithms that support reasoning about these questions. To overcome the traditional inaccuracies of static analysis, we have also developed a range of techniques for improving the accuracy of the analysis results. One strength of our approach is the flexibility allowed in choosing and combining these techniques so as to increase accuracy without making analysis time impractical.We have implemented a prototype toolset that automates the analysis for programs with explicit tasking and rendezvous style communication. We present preliminary experimental results using this toolset.
Application and experimental evaluation of state space reduction methods for deadlock analysis in Ada	An emerging challenge for software engineering is the development of the methods and tools to aid design and analysis of concurrent and distributed software. Over the past few years, a number of analysis methods that focus on Ada tasking have been developed. Many of these methods are based on some form of reachability analysis, which has the advantage of being conceptually simple, but the disadvantage of being computationally expensive. We explore the effectiveness of various Petri net-based techniques for the automated deadlock analysis of Ada programs. Our experiments consider a variety of state space reduction methods both individually and in various combinations. The experiments are applied to a number of classical concurrent programs as well as a set of “real-world” programs. The results indicate that Petri net reduction and reduced state space generation are mutually beneficial techniques, and that combined approaches based on Petri net models are quite effective, compared to alternative analysis approaches.
Dissections, orientations, and trees with applications to optimal mesh encoding and random sampling	We present a bijection between some quadrangular dissections of an hexagon and unrooted binary trees with interesting consequences for enumeration, mesh compression, and graph sampling. Our bijection yields an efficient uniform random sampler for 3-connected planar graphs, which turns out to be determinant for the quadratic complexity of the current best-known uniform random sampler for labelled planar graphs. It also provides an encoding for the set P(n) of n-edge 3-connected planar graphs that matches the entropy bound 1/n log2 &verbar;P(n)&verbar; &equals; 2 &plus; o(1) bits per edge (bpe). This solves a theoretical problem recently raised in mesh compression as these graphs abstract the combinatorial part of meshes with spherical topology. We also achieve the optimal parametric rate 1/n log2 &verbar;P(n, i, j)&verbar; bpe for graphs of P(n) with i vertices and j faces, matching in particular the optimal rate for triangulations. Our encoding relies on a linear time algorithm to compute an orientation associated with the minimal Schnyder wood of a 3-connected planar map. This algorithm is of independent interest, and it is, for instance, a key ingredient in a recent straight line drawing algorithm for 3-connected planar graphs.
Linear-time compression of bounded-genus graphs into information-theoretically optimal number of bits	This extended abstract summarizes a new result for the graphcompression problem, addressing how to compress a graphG into a binary string Z with the requirement thatZ can be decoded to recover G. Graphcompression finds important applications in 3D model compression ofComputer Graphics [12, 17-20] and compact routing table of ComputerNetworks [7]. For brevity, let a ¦Ð-graph standfor a graph with property ¦Ð. Theinformation-theoretically optimal number of bits required torepresent an n-node ¦Ð-graph is⌈log2N¦Ð(n)⌉, whereN¦Ð(n) is the number ofdistinct n-node ¦Ð-graphs. Although determiningor approximating the close forms ofN¦Ð(n) for nontrivial classesof ¦Ð is challenging, we provide a linear-timemethodology for graph compression schemes that areinformation-theoretically optimal with respect to continuoussuper-additive functions (abbreviated as optimal for therest of the extended abstract). Specifically, if ¦Ðsatisfies certain properties, then we can compress anyn-node m-edge ¦Ð-graph G into abinary string Z such that G and Z can becomputed from each other in O(m + n) time, and thatthe bit count of Z is at most ¦Â(n) +o(¦Â(n)) for any continuoussuper-additive function ¦Â(n) withlog2N¦Ð(n)¡Ü ¦Â(n) +o(¦Â(n)). Our methodology is applicableto general classes of graphs; this extended abstract focuses ongraphs with sublinear genus. For example, if the inputn-node ¦Ð-graph G is equipped with anembedding on its genus surface, which is a reasonable assumptionfor graphs arising from 3D model compression, then our methodologyis applicable to any ¦Ð satisfying the followingstatements:F1. The genus of any n-node ¦Ð-graph iso(n/log2 n);F2. Any subgraph of a ¦Ð-graph remains a¦Ð-graph;F3. log N ¦Ð(n) =¦¸(n); andF4. There is an integer k = O(1) such that ittakes O(n) time to determine whether anO(log(k) n)-node graph satisfiesproperty ¦Ð.For instance, ¦Ð can be the property of being adirected 3-colorable simple graph with genus no more than ten. Theresult is a novel application of planarization algorithm forbounded-genus graphs [5] and separator decomposition tree of planargraphs [9]. Rooted trees were the only known nontrivial class ofgraphs with linear-time optimal coding schemes. He, Kao, and Lu[11] provided O(n log n)-time compressionschemes for planar and plane graphs that are optimal. Our resultssignificantly enlarge the classes of graphs that admit efficientoptimal compression schemes. More results on various versions ofgraph compression problems or succinct graph representations can befound in [1-4, 6, 8, 10, 14, 15] and the references therein.
Cleanness Checking of String Manipulations in C Programs via Integer Analysis	All practical C programs use structures, arrays, and/or strings. At runtime, such objects are mapped into consecutive memory locations, hereafter referred to as buffers. Many software defects are caused by buffer overflow -- unintentional access to memory outside the intended object. Stringma nipulation is a major source of such defects. Accordingto the FUZZ study, they are the cause of most UNIX failures. We present a new algorithm for statically detecting buffer overflow defects caused by string manipulations in C programs. In many programs, our algorithm is capable of precisely handling destructive memory updates, even in the presence of overlapping pointer variables which reference the same buffer at different offsets. Thus, our algorithm can uncover defects which go undetected by previous works. We reduce the problem of checkings tring manipulation to that of analyzing integer variables. A prototype of the algorithm has been implemented and applied to statically uncover defects in real C applications, i.e., errors which occur on some inputs to the program. The applications were selected without a priori knowledge of the number of string manipulation errors. A significant number of string manipulation errors were found in every application, further indicating the extensiveness of such errors. We are encouraged by the fact that our algorithm reports very few false alarms, i.e., warnings on errors that never occur at runtime.
Efficient detection of all pointer and array access errors	We present a pointer and array access checking technique that provides complete error coverage through a simple set of program transformations. Our technique, based on an extended safe pointer representation, has a number of novel aspects. Foremost, it is the first technique that detects all spatial and temporal access errors. Its use is not limited by the expressiveness of the language; that is, it can be applied successfully to compiled or interpreted languages with subscripted and mutable pointers, local references, and explicit and typeless dynamic storage management, e.g., C. Because it is a source level transformation, it is amenable to both compile- and run-time optimization. Finally, its performance, even without compile-time optimization, is quite good. We implemented a prototype translator for the C language and analyzed the checking overheads of six non-trivial, pointer intensive programs. Execution overheads range from 130% to 540%; with text and data size overheads typically below 100%.
On the approximability of some network design problems	Consider the following classical network design problem: a set of terminals T &equals; &lcub;ti&rcub; wishes to send traffic to a root r in an n-node graph G &equals; (V, E). Each terminal ti sends di units of traffic and enough bandwidth has to be allocated on the edges to permit this. However, bandwidth on an edge e can only be allocated in integral multiples of some base capacity ue and hence provisioning k × ue bandwidth on edge e incurs a cost of ⌈k⌉ times the cost of that edge. The objective is a minimum-cost feasible solution. This is one of many network design problems widely studied where the bandwidth allocation is governed by side constraints: edges can only allow a subset of cables to be purchased on them or certain quality-of-service requirements may have to be met. In this work, we show that this problem and, in fact, several basic problems in this general network design framework cannot be approximated better than Ω(log log n) unless NP ⊆ DTIME (nO(log log log n)), where &verbar;V&verbar; &equals; n. In particular, we show that this inapproximability threshold holds for (i) the Priority-Steiner Tree problem, (ii) the (single-sink) Cost-Distance problem, and (iii) the single-sink version of an even more fundamental problem, Fixed Charge Network Flow. Our results provide a further breakthrough in the understanding of the level of complexity of network design problems. These are the first nonconstant hardness results known for all these problems.
Polylogarithmic inapproximability	We provide the first hardness result of a polylogarithmic approximation ratio for a natural NP-hard optimization problem. We show that for every fixed ε0, the GROUP-STEINER-TREE problem admits no efficient log2-ε k approximation, where k denotes the number of groups (or, alternatively, the input size), unless NP has quasi polynomial Las-Vegas algorithms. This hardness result holds even for input graphs which are Hierarchically Well-Separated Trees, introduced by Bartal [FOCS, 1996]. For these trees (and also for general trees), our bound is nearly tight with the log-squared approximation currently known. Our results imply that for every fixed ε0, the DIRECTED-STEINER TREE problem admits no log2-ε n--approximation, where n is the number of vertices in the graph, under the same complexity assumption.
A tight bound on approximating arbitrary metrics by tree metrics	In this paper, we show that any n point metric space can be embedded into a distribution over dominating tree metrics such that the expected stretch of any edge is O(log n). This improves upon the result of Bartal who gave a bound of O(log n log log n). Moreover, our result is existentially tight; there exist metric spaces where any tree embedding must have distortion Ω(log n)-distortion. This problem lies at the heart of numerous approximation and online algorithms including ones for group Steiner tree, metric labeling, buy-at-bulk network design and metrical task system. Our result improves the performance guarantees for all of these problems.
Evaluation of American Sign Language Generation by Native ASL Signers	There are many important factors in the design of evaluation studies for systems that generate animations of American Sign Language (ASL) sentences, and techniques for evaluating natural language generation of written texts are not easily adapted to ASL. When conducting user-based evaluations, several cultural and linguistic characteristics of members of the American Deaf community must be taken into account so as to ensure the accuracy of evaluations involving these users. This article describes an implementation and user-based evaluation (by native ASL signers) of a prototype ASL natural language generation system that produces sentences containing classifier predicates, which are frequent and complex spatial phenomena that previous ASL generators have not produced. Native signers preferred the system's output to Signed English animations -- scoring it higher in grammaticality, understandability, and naturalness of movement. They were also more successful at a comprehension task after viewing the system's classifier predicate animations.
Evaluation metrics for generation	Certain generation applications may profit from the use of stochastic methods. In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models. In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment. This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects. To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment. The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.
Watchpoint Semantics: A Tool for Compositional and Focussed Static Analyses	We abstract a denotational trace semantics for an imperative language into a compositional and focussed watchpoint semantics. Every abstraction of its computational domain induces an abstract, still compositional and focussed watchpoint semantics. We describe its implementation and instantiation with a domain of signs. It shows that its space and time costs are proportional to the number of watchpoints and that abstract compilation reduces those costs significantly.
Trace-Based Abstract Interpretation of Operational Semantics	We present trace-based abstract interpretation, a unification of severallines of research on applying Cousot-Cousot-style abstract interpretation a.i. tooperational semantics definitions (such as flowchart, big-step, and small-step semantics)that express a program‘s semantics as a concrete computation tree of trace paths. Aprogram‘s trace-based a.i. is also a computation tree whose nodes contain abstractions ofstate and whose paths simulate the paths in the program‘s concrete computation tree.Using such computation trees, we provide a simple explanation of the central concept of collecting semantics, and we distinguish concrete from abstract collectingsemantics and state-based from path-based collecting semantics. We also expose therelationship between collecting semantics extraction and results garnered from flow-analytic and model-checking-based analysis techniques. We adapt concepts fromconcurrency theory to formalize “safe” and “live” a.i.‘s for computation trees; in particular, coinduction techniques help extend fundamental results to infinite computation trees.Problems specific to the various operational semantics methodologies are discussed: Big-step semantics cannot express divergence, so we employ a mixture of induction andcoinduction in response; small-step semantics generate sequences of programconfigurations unbounded in size, so we abstractly interpret source language syntax.Applications of trace-based a.i. to data-flow analysis, model checking, closure analysis,and concurrency theory are demonstrated.
Representing and Approximating Transfer Functions in Abstract Interpretation of Hetereogeneous Datatypes	We present a general method to combine different datatypes in Abstract Interpretation, within the framework of verification of reactive system. We focus more precisely on the efficient representation and approximation of the transfer functions involved in the abstract fix-point computations. The solution we propose allows to tune smoothly the necessary tradeoff between accuracy and efficiency in the analysis.
Composite model-checking: verification with type-specific symbolic representations	There has been a surge of progress in automated verification methods based on state exploration. In areas like hardware design, these technologies are rapidly augmenting key phases of testing and validation. To date, one of the most successful of these methods has been symbolic model-checking, in which large finite-state machines are encoded into compact data structures such as Binary Decision Diagrams (BDDs), and are then checked for safety and liveness properties. However, these techniques have not realized the same success on software systems. One limitation is their inability to deal with infinite-state programs, even those with a single unbounded integer. A second problem is that of finding efficient representations for various variable types. We recently proposed a model-checker for integer-based systems that uses arithmetic constraints as the underlying state representation. While this approach easily verified some subtle, infinite-state concurrency problems, it proved inefficient in its treatment of boolean and (unordered) enumerated types—which are not efficiently representable using arithmetic constraints. In this article we present a new technique that combines the strengths of both BDD and arithmetic constraint representations. Our composite model merges multiple type-specific symbolic representations in a single model-checker. A system's transitions and fixpoint computations are encoded using both BDD (for boolean and enumerated types) and arithmetic constraints (for integers) representations, where the choice depends on the variable types. Our composite model-checking strategy can be extended to other symbolic representations provided that they support operations such as intersection, union, complement, equivalence checking, and relational image computation. We also present conservative approximation techniques for composite representations to address the undecidability of model-checking on infinite-state systems. We demonstrate the effectiveness of our approach by analyzing two example software specifications which include a mixture of booleans, integers, and enumerated types. One of them is a requirements specification for the control software of a nuclear reactor's cooling system, and the other one is a protocol specification.
Automated Verification of Concurrent Linked Lists with Counters	We present an automated verification technique for verification of concurrent linked lists with integer variables. We show that using our technique one can automatically verify invariants that relate (unbounded) integer variables and heap variables such as head 驴 null 驴 numItems 0. The presented technique extends our previous work on composite symbolic representations with shape analysis. The main idea is to use different data structures such as BDDs, arithmetic constraints and shape graphs as type specific symbolic representations in automated verification. We show that polyhedra based widening operation can be integrated with summarization operation in shape graphs to conservatively verify properties of concurrent linked lists.
Verifying safety properties of concurrent Java programs using 3-valued logic	We provide a parametric framework for verifying safety properties of concurrent Java programs. The framework combines thread-scheduling information with information about the shape of the heap. This leads to error-detection algorithms that are more precise than existing techniques. The framework also provides the most precise shape-analysis algorithm for concurrent programs. In contrast to existing verification techniques, we do not put a bound on the number of allocated objects. The framework even produces interesting results when analyzing Java programs with an unbounded number of threads. The framework is applied to successfully verify the following properties of a concurrent program: •Concurrent manipulation of linked-list based ADT preserves the ADT datatype invariant [19]. •The program does not perform inconsistent updates due to interference. •The program does not reach a deadlock. •The program does not produce run-time errors due to illegal thread interactions. We also find bugs in erroneous versions of such implementations. A prototype of our framework has been implemented.
Modular Control Flow Analysis for Libraries	One problem in analyzing object oriented languages is that the exact control flow graph is not known statically due to dynamic dispatching. However, this is needed in order to apply the large class of known interprocedural analysis. Control Flow Analysis in the object oriented setting aims at determining run-time types of variables, thus allowing to possibly targeted method implementations.We present a flow sensitive analysis that allows separate handling of libraries and thereby efficient analysis of whole programs.
The Cartesian Product Algorithm: Simple and Precise Type Inference Of Parametric Polymorphism	Concrete types and abstract types are different and serve different purposes. Concrete types, the focus of this paper, are essential to support compilation, application delivery, and debugging in object-oriented environments. Concrete types should not be obtained from explicit type declarations because their presence limits polymorphism unacceptably. This leaves us with type inference. Unfortunately, while polymorphism demands the use of type inference, it has also been the hardest challenge for type inference.We review previous type inference algorithms that analyze code with parametric polymorphism and then present a new one: the cartesian product algorithm. It improves precision and efficiency over previous algorithms and deals directly with inheritance, rather than relying on a preprocessor to expand it away. Last, but not least, it is conceptually simple.The cartesian product algorithm has been used in the Self system since late 1993. We present measurements to document its performance and compare it against several previous algorithms.
Scalable propagation-based call graph construction algorithms	Propagation-based call graph construction algorithms have been studied intensively in the 199Os, and differ primarily in the number of sets that are used to approximate run-time values of expressions. In practice, algorithms such as RTA that use a single set for the whole program scale well. The scalability of algorithms such as 0-CFA that use one set per expression remains doubtful.In this paper, we investigate the design space between RTA and 0-CFA. We have implemented various novel algorithms in the context of Jax, an application extractor for Java, and shown that they all scale to a 325,000-line program. A key property of these algorithms is that they do not analyze values on the run-time stack, which makes them efficient and easy to implement. Surprisingly, for detecting unreachable methods, the inexpensive RTA algorithm does almost as well as the seemingly more powerful algorithms. However, for determining call sites with a single target, one of our new algorithms obtains the current best tradeoff between speed and precision.
Projection merging: reducing redundancies in inclusion constraint graphs	Inclusion-based program analyses are implemented by adding new edges to directed graphs. In most analyses, there are many different ways to add a transitive edge between two nodes, namely through each different path connecting the nodes. This path redundancy limits the scalability of these analyses. We present projection merging, a technique to reduce path redundancy. Combined with cycle elimination [7], projection merging achieves orders of magnitude speedup of analysis time on programs over that of using cycle elimination alone.
Componential set-based analysis	Set-based analysis (SBA) produces good predictions about the behavior of functional and object-oriented programs. The analysis proceeds by inferring constraints that characterize the data flow relationships of the analyzed program. Experiences with MrSpidey, a static debugger based on SBA, indicate that SBA can adequately deal with programs of up to a couple of thousand lines of code. SBA fails, however, to cope with larger programs because it generates systems of constraints that are at least linear, and possibility quadratic, in the size of the analyzed program. This article presents theoretical and practical results concerning methods for reducing the size of constraint systems. The theoretical results include of proof-theoretic characterization of the observable behavior of constraint systems for program components, and a complete algorithm for deciding the observable equivalence of constraint systems. In the course of this development we establish a close connection between the observable equivalence of constraint systems and the equivalence of regular-tree grammars. We then exploit this connection to adapt a variety of algoirthms for simplifying grammars to the problem of simplifying constraint systems. Based on the resulting algorithms, we have developed componential set-based analysis, a modular and polymorphic variant of SBA. Experimental results verify the effectiveness of the simplification algorithms and the componential analysis. The simplified constraint systems are typically an order of magnitude smaller than the original systems. These reductions in size produce significant gains in the speed of the analysis.
How to barter bits for chronons: compression and bandwidth trade offs for database scans	Two trends are converging to make the CPU cost of a table scan a more important component of database performance. First, table scans are becoming a larger fraction of the query processing workload, and second, large memories and compression are making table scans CPU, rather than disk bandwidth, bound. Data warehouse systems have found that they can avoid the unpredictability of joins and indexing and achieve good performance by using massive parallel processing to perform scans over compressed vertical partitions of a denormalized schema. In this paper we present a study of how to make such scans faster by the use of a scan code generator that produces code tuned to the database schema, the compression dictionaries, the queries being evaluated and the target CPU architecture. We investigate a variety of compression formats and propose two novel optimizations: tuple length quantization and a field length lookup table, for efficiently processing variable length fields and tuples. We present a detailed experimental study of the performance of generated scans against these compression formats, and use this to explore the trade off between compression quality and scan speed. We also introduce new strategies for removing instruction-level dependencies and increasing instruction-level parallelism, allowing for greater exploitation of multi-issue processors.
C-store: a column-oriented DBMS	This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.We present preliminary performance data on a subset of TPC-H and show that the system we are building, C-Store, is substantially faster than popular commercial products. Hence, the architecture looks very encouraging.
How to wring a table dry: entropy compression of relations and querying of compressed relations	We present a method to compress relations close to their entropy while still allowing efficient queries. Column values are encoded into variable length codes to exploit skew in their frequencies. The codes in each tuple are concatenated and the resulting tuplecodes are sorted and delta-coded to exploit the lack of ordering in a relation. Correlation is exploited either by co-coding correlated columns, or by using a sort order that leverages the correlation. We prove that this method leads to near-optimal compression (within 4.3 bits/tuple of entropy), and in practice, we obtain up to a 40 fold compression ratio on vertical partitions tuned for TPC-H queries.We also describe initial investigations into efficient querying over compressed data. We present a novel Huffman coding scheme, called segregated coding, that allows range and equality predicates on compressed data, without accessing the full dictionary. We also exploit the delta coding to speed up scans, by reusing computations performed on nearly identical records. Initial results from a prototype suggest that with these optimizations, we can efficiently scan, tokenize and apply predicates on compressed relations.
Discretization from data streams: applications to histograms and data mining	In this paper we propose a new method to perform incremental discretization. The basic idea is to perform the task in two layers. The first layer receives the sequence of input data and keeps some statistics on the data using many more intervals than required. Based on the statistics stored by the first layer, the second layer creates the final discretization. The proposed architecture processes streaming examples in a single scan, in constant time and space even for infinite sequences of examples. We experimentally demonstrate that incremental discretization is able to maintain the performance of learning algorithms in comparison to a batch discretization. The proposed method is much more appropriate in incremental learning, and in problems where data flows continuously, as in most of the recent data mining applications.
Fast incremental maintenance of approximate histograms	Many commercial database systems maintain histograms to summarize the contents of large relations and permit efficient estimation of query result sizes for use in query optimizers. Delaying the propagation of database updates to the histogram often introduces errors into the estimation. This article presents new sampling-based approaches for incremental maintenance of approximate histograms. By scheduling updates to the histogram based on the updates to the database, our techniques are the first to maintain histograms effectively up to date at all times and avoid computing overheads when unnecessary. Our techniques provide highly accurate approximate histograms belonging to the equidepth and Compressed classes. Experimental results show that our new approaches provide orders of magnitude more accurate estimation than previous approaches.An important aspect employed by these new approaches is a backing sample, an up-to-date random sample of the tuples currently in a relation. We provide efficient solutions for maintaining a uniformly random sample of a relation in the presence of updates to the relation. The backing sample techniques can be used for any other application that relies on random samples of data.
A data-oriented survey of context models	Context-aware systems are pervading everyday life, therefore context modeling is becoming a relevant issue and an expanding research field. This survey has the goal to provide a comprehensive evaluation framework, allowing application designers to compare context models with respect to a given target application; in particular we stress the analysis of those features which are relevant for the problem of data tailoring. The contribution of this paper is twofold: a general analysis framework for context models and an up-to-date comparison of the most interesting, data-oriented approaches available in the literature.
Middleware for distributed context-aware systems	Context-aware systems represent extremely complex and heterogeneous distributed systems, composed of sensors, actuators, application components, and a variety of context processing components that manage the flow of context information between the sensors/actuators and applications. The need for middleware to seamlessly bind these components together is well recognised. Numerous attempts to build middleware or infrastructure for context-aware systems have been made, but these have provided only partial solutions; for instance, most have not adequately addressed issues such as mobility, fault tolerance or privacy. One of the goals of this paper is to provide an analysis of the requirements of a middleware for context-aware systems, drawing from both traditional distributed system goals and our experiences with developing context-aware applications. The paper also provides a critical review of several middleware solutions, followed by a comprehensive discussion of our own PACE middleware. Finally, it provides a comparison of our solution with the previous work, highlighting both the advantages of our middleware and important topics for future research.
iCAP: interactive prototyping of context-aware applications	Although numerous context-aware applications have been developed and there have been technological advances for acquiring contextual information, it is still difficult to develop and prototype interesting context-aware applications. This is largely due to the lack of programming support available to both programmers and end-users. This lack of support closes off the context-aware application design space to a larger group of users. We present iCAP, a system that allows end-users to visually design a wide variety of context-aware applications, including those based on if-then rules, temporal and spatial relationships and environment personalization. iCAP allows users to quickly prototype and test their applications without writing any code. We describe the study we conducted to understand end-users' mental models of context-aware applications, how this impacted the design of our system and several applications that demonstrate iCAP's richness and ease of use. We also describe a user study performed with 20 end-users, who were able to use iCAP to specify every application that they envisioned, illustrating iCAP's expressiveness and usability.
A Historical View of Context	This paper examines a number of the approaches, origins and ideals of context-aware systems design, looking particularly at the way that history influences what we do in our ongoing activity. As a number of sociologists and philosophers have pointed out, past social interaction, as well as past use of the heterogeneous mix of media, tools and artifacts that we use in our everyday activity, influence our ongoing interaction with the people and media at hand. We suggest that one's experience and history is thus part of one's current context, with patterns of use temporally and subjectively combining and interconnecting different media as well as different modes of use of those media. One such mode of use is transparent use, put forward by Weiser as ubicomp's design ideal. One theoretical finding is that this design ideal is unachievable or incomplete because transparent and more focused analytical use are interdependent, affecting and feeding into each other through one's experience and history. Using these theoretical points, we discuss a number of context-aware system designs that make good use of history in supporting ongoing user activity.
Computational models for experiences in the arts, and multimedia	In this paper, we develop formal computational models for three aspects of experiential systems for browsing media -- (a) context (b) interactivity through hyper-mediation and (c) context evolution using a memory model. Experiential systems deal with the problem of developing context adaptive mechanisms for knowledge acquisition and insight. Context is modeled as a union of graphs whose nodes represent concepts and where the edges represent the semantic relationships. The system context is the union of the contexts of the user, the environment and the media being accessed. We also develop a novel concept dissimilarity. We then develop algorithms to determine the optimal hyperlink for each media element by determining the relationship between the user context and the media. As the user navigates through the hyper-linked sources, the memory model captures the interaction of the user with the hyper-linked sources and updates the user context. Finally, this results in new hyper-links for the media. Our pilot user studies show excellent results, validating our framework.
A methodology for a Very Small Data Base design	This paper proposes a design methodology for very small databases for the purpose of being hosted by portable devices. Three main differences w.r.t. the traditional design methodologies are introduced: first, the main mobility issues are considered along with data distribution; second, context awareness is included in the data design issues to allow full exploitation of context-sensitive application functionalities; and third, the peculiarities of the storage device(s) are taken into account by introducing a logistic phase after the usual conceptual and logical phases. The three aspects together determine the VSDB ambient which is the set of personal and environmental characteristics determining the portion of data that must be stored on the portable device. This paper details the design methodology in its conceptual, logical and logistic phases.
Automatic mutual exclusion	We propose a new concurrent programming model, Automatic Mutual Exclusion (AME). In contrast to lock-based programming, and to other programming models built over software transactional memory (STM), we arrange that all shared state is implicitly protected unless the programmer explicitly specifies otherwise. An AME program is composed from serializable atomic fragments. We include features allowing the programmer to delimit and manage the fragments to achieve appropriate program structure and performance. We explain how I/O activity and legacy code can be incorporated within an AME program. Finally, we outline ways in which future work might expand on these ideas. The resulting programming model makes it easier to write correct code than incorrect code. It favors correctness over performance for simple programs, while allowing advanced programmers the expressivity they need.
Why events are a bad idea (for high-concurrency servers)	Event-based programming has been highly touted in recent years as the best way to write highly concurrent applications. Having worked on several of these systems, we now believe this approach to be a mistake. Specifically, we believe that threads can achieve all of the strengths of events, including support for high concurrency, low overhead, and a simple concurrency model. Moreover, we argue that threads allow a simpler and more natural programming style. We examine the claimed strengths of events over threads and show that the weaknesses of threads are artifacts of specific threading implementations and not inherent to the threading paradigm. As evidence, we present a user-level thread package that scales to 100,000 threads and achieves excellent performance in a web server. We also refine the duality argument of Lauer and Needham, which implies that good implementations of thread systems and event systems will have similar performance. Finally, we argue that compiler support for thread systems is a fruitful area for future research. It is a mistake to attempt high concurrency without help from the compiler, and we discuss several enhancements that are enabled by relatively simple compiler changes.
Compatibility is not transparency: VMM detection myths and realities	Recent work on applications ranging from realistic honeypots to stealthier rootkits has speculated about building transparent VMMs - VMMs that are indistinguishable from native hardware, even to a dedicated adversary. We survey anomalies between real and virtual hardware and consider methods for detecting such anomalies, as well as possible countermeasures. We conclude that building a transparent VMM is fundamentally infeasible, as well as impractical from a performance and engineering standpoint.
Using VMM-based sensors to monitor honeypots	Virtual Machine Monitors (VMMs) are a common tool for implementing honeypots. In this paper we examine the implementation of a VMM-based intrusion detection and monitoring system for collecting information about attacks on honeypots. We document and evaluate three designs we have implemented on two open-source virtualization platforms: User-Mode Linux and Xen. Our results show that our designs give the monitor good visibility into the system and thus, a small number of monitoring sensors can detect a large number of intrusions. In a three month period, we were able to detect five different attacks, as well as collect and try 46 more exploits on our honeypots. All attacks were detected with only two monitoring sensors. We found that the performance overhead for monitoring such intrusions is independent of which events are being monitored, but depends entirely on the number of monitoring events and the underlying monitoring implementation. The performance overhead can be significantly improved by implementing the monitor directly in the privileged code of the VMM, though at the cost of increasing the size of the trusted computing base of the system.
Memory resource management in VMware ESX server	VMware ESX Server is a thin software layer designed to multiplex hardware resources efficiently among virtual machines running unmodified commodity operating systems. This paper introduces several novel ESX Server mechanisms and policies for managing memory. A ballooning technique reclaims the pages considered least valuable by the operating system running in a virtual machine. An idle memory tax achieves efficient memory utilization while maintaining performance isolation guarantees. Content-based page sharing and hot I/O page remapping exploit transparent page remapping to eliminate redundancy and reduce copying overheads. These techniques are combined to efficiently support virtual machine workloads that overcommit memory.
Scale and performance in the Denali isolation kernel	This paper describes the Denali isolation kernel, an operating system architecture that safely multiplexes a large number of untrusted Internet services on shared hardware. Denali's goal is to allow new Internet services to be "pushed" into third party infrastructure, relieving Internet service authors from the burden of acquiring and maintaining physical infrastructure. Our isolation kernel exposes a virtual machine abstraction, but unlike conventional virtual machine monitors, Denali does not attempt to emulate the underlying physical architecture precisely, and instead modifies the virtual architecture to gain scale, performance, and simplicity of implementation. In this paper, we first discuss design principles of isolation kernels, and then we describe the design and implementation of Denali. Following this, we present a detailed evaluation of Denali, demonstrating that the overhead of virtualization is small, that our architectural choices are warranted, and that we can successfully scale to more than 10,000 virtual machines on commodity hardware.
Don't settle for less than the best: use optimization to make decisions	Optimistic synchronization allows concurrent execution of critical sections while performing dynamic conflict detection and recovery. Optimistic synchronization will increase performance only if critical regions are data independent--concurrent critical ...
Managing energy and server resources in hosting centers	Internet hosting centers serve multiple service sites from a common hardware base. This paper presents the design and implementation of an architecture for resource management in a hosting center operating system, with an emphasis on energy as a driving resource management issue for large server clusters. The goals are to provision server resources for co-hosted services in a way that automatically adapts to offered load, improve the energy efficiency of server clusters by dynamically resizing the active server set, and respond to power supply disruptions or thermal events by degrading service in accordance with negotiated Service Level Agreements (SLAs).Our system is based on an economic approach to managing shared server resources, in which services "bid" for resources as a function of delivered performance. The system continuously monitors load and plans resource allotments by estimating the value of their effects on service performance. A greedy resource allocation algorithm adjusts resource prices to balance supply and demand, allocating resources to their most efficient use. A reconfigurable server switching infrastructure directs request traffic to the servers assigned to each service. Experimental results from a prototype confirm that the system adapts to offered load and resource availability, and can reduce server energy usage by 29% or more for a typical Web workload.
Hibernator: helping disk arrays sleep through the winter	Energy consumption has become an important issue in high-end data centers, and disk arrays are one of the largest energy consumers within them. Although several attempts have been made to improve disk array energy management, the existing solutions either provide little energy savings or significantly degrade performance for data center workloads.Our solution, Hibernator, is a disk array energy management system that provides improved energy savings while meeting performance goals. Hibernator combines a number of techniques to achieve this: the use of disks that can spin at different speeds, a coarse-grained approach for dynamically deciding which disks should spin at which speeds, efficient ways to migrate the right data to an appropriate-speed disk automatically, and automatic performance boosts if there is a risk that performance goals might not be met due to disk energy management.In this paper, we describe the Hibernator design, and present evaluations of it using both trace-driven simulations and a hybrid system comprised of a real database server (IBM DB2) and an emulated storage server with multi-speed disks. Our file-system and on-line transaction processing (OLTP) simulation results show that Hibernator can provide up to 65% energy savings while continuing to satisfy performance goals (6.5--26 times better than previous solutions). Our OLTP emulated system results show that Hibernator can save more energy (29%) than previous solutions, while still providing an OLTP transaction rate comparable to a RAID5 array with no energy management.
Value-maximizing deadline scheduling and its application to animation rendering	We describe a new class of utility-maximization scheduling problem with precedence constraints, the disconnected staged scheduling problem (DSSP). DSSP is a nonpreemptive multiprocessor deadline scheduling problem that arises in several commercially-important applications, including animation rendering, protein analysis, and seismic signal processing. DSSP differs from most previously-studied deadline scheduling problems because the graph of precedence constraints among tasks within jobs is disconnected, with one component per job. Another difference is that in practice we often lack accurate estimates of task execution times, and so purely offline solutions are not possible. However we do know the set of jobs and their precedence constraints up front and therefore some offline planning is possible.Our solution decomposes DSSP into an offline job selection phase followed by an online task dispatching phase. We model the former as a knapsack problem and explore several solutions to it, describe a new dispatching algorithm for the latter, and compare both with existing methods. Our theoretical results show that while DSSP is NP-hard and inapproximable in general, our two-phase scheduling method guarantees a good performance bound for many special cases. Our empirical results include an evaluation of scheduling algorithms on a real animation-rendering workload; we present a characterization of this workload in a companion paper. The workload records eight weeks of activity on a 1,000-CPU cluster used to render portions of the full-length animated feature film Shrek 2 in 2004. We show that our improved scheduling algorithms can substantially increase the aggregate value of completed jobs compared to existing practices. Our new task dispatching algorithm LCPF performs well by several metrics, including job completion times as well as the aggregate value of completed jobs.
Making scheduling "cool": temperature-aware workload placement in data centers	Trends towards consolidation and higher-density computing configurations make the problem of heat management one of the critical challenges in emerging data centers. Conventional approaches to addressing this problem have focused at the facilities level to develop new cooling technologies or optimize the delivery of cooling. In contrast to these approaches, our paper explores an alternate dimension to address this problem, namely a systems-level solution to control the heat generation through temperature-aware workload placement. We first examine a theoretic thermodynamic formulation that uses information about steady state hot spots and cold spots in the data center and develop real-world scheduling algorithms. Based on the insights from these results, we develop an alternate approach. Our new approach leverages the non-intuitive observation that the source of cooling inefficiencies can often be in locations spatially uncorrelated with its manifested consequences; this enables additional energy savings. Overall, our results demonstrate up to a factor of two reduction in annual data center cooling costs over location-agnostic workload distribution, purely through software optimizations without the need for any costly capital investment.
Decentralized, adaptive resource allocation for sensor networks	This paper addresses the problem of resource allocation in sensor networks. We are concerned with how to allocate limited energy, radio bandwidth, and other resources to maximize the value of each node's contribution to the network. Sensor networks present a novel resource allocation challenge: given extremely limited resources, varying node capabilities, and changing network conditions, how can one achieve efficient global behavior? Currently, this is accomplished by carefully tuning the behavior of the low-level sensor program to accomplish some global task, such as distributed event detection or in-network data aggregation. This manual tuning is difficult, error-prone, and typically does not consider network dynamics such as energy depletion caused by bursty communication patterns. We present Self-Organizing Resource Allocation (SORA), a new approach for achieving efficient resource allocation in sensor networks. Rather than manually tuning sensor resource usage, SORA defines a virtual market in which nodes sell goods (such as sensor readings or data aggregates) in response to prices that are established by the programmer. Nodes take actions to maximize their profit, subject to energy budget constraints. Nodes individually adapt their operation over time in response to feedback from payments, using reinforcement learning. The behavior of the network is determined by the price for each good, rather than by directly specifying local node programs. SORA provides a useful set of primitives for controlling the aggregate behavior of sensor networks despite variance of individual nodes. We present the SORA paradigm and a sensor network vehicle tracking application based on this design, as well as an extensive evaluation demonstrating that SORA realizes an efficient allocation of network resources that adapts to changing network conditions.
Syntactic-Based Methods for Measuring Word Similarity	This paper explores different strategies for extracting similarity relations between words from partially parsed text corpora. The strategies we have analysed do not require supervised training nor semantic information available from general lexical resources. They differ in the amount and the quality of the syntactic contexts against which words are compared. The paper presents in details the notion of syntactic context and how syntactic information could be used to extract semantic regularities of word sequences. Finally, experimental tests with Portuguese corpus demonstrate that similarity measures based on fine-grained and elaborate syntactic contexts perform better than those based on poorly defined contexts.
Word-sense disambiguation using statistical models of Roget's categories trained on large corpora	This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories. Roget's categories serve as approximations of conceptual classes. The categories listed for a word in Roget's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework.Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention. Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature.
On learning more appropriate Selectional Restrictions	We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora. It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes. Evaluation measures for the Selectional Restrictions learning task are discussed. Finally, an experimental evaluation of these variations is reported.
Rethinking antivirus: executable analysis in the network cloud	Current phishing attacks focus primarily on stealing user credentials such as passwords. In response, web sites are deploying stronger authentication and back-end analytics systems that make it harder for phishers to extract value from stolen passwords. ...
Automated classification and analysis of internet malware	Numerous attacks, such as worms, phishing, and botnets, threaten the availability of the Internet, the integrity of its hosts, and the privacy of its users. A core element of defense against these attacks is anti-virus (AV) software--a service that detects, removes, and characterizes these threats. The ability of these products to successfully characterize these threats has far-reaching effects--from facilitating sharing across organizations, to detecting the emergence of new threats, and assessing risk in quarantine and cleanup. In this paper, we examine the ability of existing host-based anti-virus products to provide semantically meaningful information about the malicious software and tools (or malware) used by attackers. Using a large, recent collection of malware that spans a variety of attack vectors (e.g., spyware, worms, spam), we show that different AV products characterize malware in ways that are inconsistent across AV products, incomplete across malware, and that fail to be concise in their semantics. To address these limitations, we propose a new classification technique that describes malware behavior in terms of system state changes (e.g., files written, processes created) rather than in sequences or patterns of system calls. To address the sheer volume of malware and diversity of its behavior, we provide a method for automatically categorizing these profiles of malware into groups that reflect similar classes of behaviors and demonstrate how behavior-based clustering provides a more direct and effective way of classifying and analyzing Internet malware.
Self-signed executables: restricting replacement of program binaries by malware	Current phishing attacks focus primarily on stealing user credentials such as passwords. In response, web sites are deploying stronger authentication and back-end analytics systems that make it harder for phishers to extract value from stolen passwords. ...
Storage-based intrusion detection: watching storage activity for suspicious behavior	Storage-based intrusion detection allows storage systems to watch for data modifications characteristic of system intrusions. This enables storage systems to spot several common intruder actions, such as adding backdoors, inserting Trojan horses, and tampering with audit logs. Further, an intrusion detection system (IDS) embedded in a storage device continues to operate even after client systems are compromised. This paper describes a number of specific warning signs visible at the storage interface. Examination of 18 real intrusion tools reveals that most (15) can be detected based on their changes to stored files. We describe and evaluate a prototype storage IDS, embedded in an NFS server, to demonstrate both feasibility and efficiency of storage-based intrusion detection. In particular, both the performance overhead and memory required (152KB for 4730 rules) are minimal.
Do strong web passwords accomplish anything?	We find that traditional password advice given to users is somewhat dated. Strong passwords do nothing to protect online users from password stealing attacks such as phishing and keylogging, and yet they place considerable burden on users. Passwords that are too weak of course invite brute-force attacks. However, we find that relatively weak passwords, about 20 bits or so, are sufficient to make brute-force attacks on a single account unrealistic so long as a "three strikes" type rule is in place. Above that minimum it appears that increasing password strength does little to address any real threat. If a larger credential space is needed it appears better to increase the strength of the userID's rather than the passwords. For large institutions this is just as effective in deterring bulk guessing attacks and is a great deal better for users. For small institutions there appears little reason to require strong passwords for online accounts.
A large-scale study of web password habits	We report the results of a large scale study of password use andpassword re-use habits. The study involved half a million users over athree month period. A client component on users' machines recorded a variety of password strength, usage and frequency metrics. This allows us to measure or estimate such quantities as the average number of passwords and average number of accounts each user has, how many passwords she types per day, how often passwords are shared among sites, and how often they are forgotten. We get extremely detailed data on password strength, the types and lengths of passwords chosen, and how they vary by site. The data is the first large scale study of its kind, and yields numerous other insights into the role the passwords play in users' online experience.
Securing passwords against dictionary attacks	The use of passwords is a major point of vulnerability in computer security, as passwords are often easy to guess by automated programs running dictionary attacks. Passwords remain the most widely used authentication method despite their well-known security weaknesses. User authentication is clearly a practical problem. From the perspective of a service provider this problem needs to be solved within real-world constraints such as the available hardware and software infrastructures. From a user's perspective user-friendliness is a key requirement.In this paper we suggest a novel authentication scheme that preserves the advantages of conventional password authentication, while simultaneously raising the costs of online dictionary attacks by orders of magnitude. The proposed scheme is easy to implement and overcomes some of the difficulties of previously suggested methods of improving the security of user authentication schemes.Our key idea is to efficiently combine traditional password authentication with a challenge that is very easy to answer by human users, but is (almost) infeasible for automated programs attempting to run dictionary attacks. This is done without affecting the usability of the system. The proposed scheme also provides better protection against denial of service attacks against user accounts.
Towards a declarative language and system for secure networking	In this paper, we present a declarative language and system for describing and implementing secure networks. Our proposed language, SeNDlog, is an attempt at unifying Binder, a logic-based language for access control in distributed systems, and Network Datalog (NDlog), a database query language for declarative networks. The contributions of this paper are as follows. First, we highlight the similarities and differences between Binder and NDlog with regards to their notion of location, trust model, and evaluation strategies. Second, we motivate and propose the SeNDlog language that combines features from Binder and NDlog. Third, we demonstrate the use of SeNDlog for specifying secure networks and present directions for future work.
Declarative networking: language, execution and optimization	The networking and distributed systems communities have recently explored a variety of new network architectures, both for application-level overlay networks, and as prototypes for a next-generation Internet architecture. In this context, we have investigated declarative networking: the use of a distributed recursive query engine as a powerful vehicle for accelerating innovation in network architectures [23, 24, 33]. Declarative networking represents a significant new application area for database research on recursive query processing. In this paper, we address fundamental database issues in this domain. First, we motivate and formally define the Network Datalog (NDlog) language for declarative network specifications. Second, we introduce and prove correct relaxed versions of the traditional semi-naïve query evaluation technique, to overcome fundamental problems of the traditional technique in an asynchronous distributed setting. Third, we consider the dynamics of network state, and formalize the iheventual consistencyl. of our programs even when bursts of updates can arrive in the midst of query execution. Fourth, we present a number of query optimization opportunities that arise in the declarative networking context, including applications of traditional techniques as well as new optimizations. Last, we present evaluation results of the above ideas implemented in our P2 declarative networking system, running on 100 machines over the Emulab network testbed.
Declarative routing: extensible routing with declarative queries	The Internet's core routing infrastructure, while arguably robust and efficient, has proven to be difficult to evolve to accommodate the needs of new applications. Prior research on this problem has included new hard-coded routing protocols on the one hand, and fully extensible Active Networks on the other. In this paper, we explore a new point in this design space that aims to strike a better balance between the extensibility and robustness of a routing infrastructure. The basic idea of our solution, which we call declarative routing, is to express routing protocols using a database query language. We show that our query language is a natural fit for routing, and can express a variety of well-known routing protocols in a compact and clean fashion. We discuss the security of our proposal in terms of its computational expressive power and language design. Via simulation, and deployment on PlanetLab, we demonstrate that our system imposes no fundamental limits relative to traditional protocols, is amenable to query optimizations, and can sustain long-lived routes under network churn and congestion.
Metarouting	There is a shortage of routing protocols that meet the needs of network engineers. This has led to BGP being pressed into service as an IGP, despite its lack of convergence guarantees. The development, standardization, and deployment of routing protocols, or even minor changes to existing protocols, are very difficult tasks. We present an approach called Metarouting that defines routing protocols using a high-level and declarative language. Once an interpreter for a metarouting language is implemented on a router, a network operator would have the freedom to implement and use any routing protocol definable in the language. We enforce a clean separation of protocol mechanisms (link-state, path-vector, adjacency maintenance, and so on) from routing policy (how routes are described and compared). The Routing Algebra framework of Sobrinho [25] is used as the theoretical basis for routing policy languages. We define the Routing Algebra Meta-Language (RAML) that allows for the construction of a large family of routing algebras and has the key property that correctness conditions --- guarantees of convergence with respect to the chosen mechanisms --- can be derived automatically for each expression defining a new routing algebra.
Delegation logic: A logic-based approach to distributed authorization	We address the problem of authorization in large-scale, open, distributed systems. Authorization decisions are needed in electronic commerce, mobile-code execution, remote resource sharing, privacy protection, and many other applications. We adopt the trust-management approach, in which "authorization" is viewed as a "proof-of-compliance" problem: Does a set of credentials prove that a request complies with a policy?We develop a logic-based language, called Delegation Logic (DL), to represent policies, credentials, and requests in distributed authorization. In this paper, we describe D1LP, the monotonic version of DL. D1LP extends the logic-programming (LP) language Datalog with expressive delegation constructs that feature delegation depth and a wide variety of complex principals (including, but not limited to, k-out-of-n thresholds). Our approach to defining and implementing D1LP is based on tractably compiling D1LP programs into ordinary logic programs (OLPs). This compilation approach enables D1LP to be implemented modularly on top of existing technologies for OLP, for example, Prolog.As a trust-management language, D1LP provides a concept of proof-of-compliance that is founded on well-understood principles of logic programming and knowledge representation. D1LP also provides a logical framework for studying delegation.
Machine learning approaches to network anomaly detection	Networks of various kinds often experience anomalous behaviour. Examples include attacks or large data transfers in IP networks, presence of intruders in distributed video surveillance systems, and an automobile accident or an untimely congestion in a road network. Machine learning techniques enable the development of anomaly detection algorithms that are non-parametric, adaptive to changes in the characteristics of normal behaviour in the relevant network, and portable across applications. In this paper we use two different datasets, pictures of a highway in Quebec taken by a network of webcams and IP traffic statistics from the Abilene network, as examples in demonstrating the applicability of two machine learning algorithms to network anomaly detection. We investigate the use of the block-based One-Class Neighbour Machine and the recursive Kernel-based Online Anomaly Detection algorithms.
